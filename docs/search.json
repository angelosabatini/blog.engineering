[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Computational methods in engineering",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\nprobability\n\n\n\nAlthough experimentalists are well familiar with the topic, nonetheless I believe it might be helpful to spend a few minutes for a review of basic formulae of combinatorial analysis.\n\n\n\n\n\nApril 24, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\nprobability\n\n\n\nIn this post, I briefly discuss the random incidence phenomenon, using the classical example of a person arriving at a bus stop at a random time, and waiting for the arrival of the next bus.\n\n\n\n\n\nApril 22, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\nsignal processing\n\n\n\nSpectral leakage and length of data record hamper our ability to resolve spectral lines by DFT/FFT analysis. In this post, I briefly discuss this problem, with examples using sinusoidal mixtures.\n\n\n\n\n\nApril 13, 2024\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Pills of combinatorics\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\n\n\nApr 13, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/combinatorics/index.html",
    "href": "posts/combinatorics/index.html",
    "title": "Pills of combinatorics",
    "section": "",
    "text": "At least in the elementary case of finite sample spaces (i.e., a finite number of outcomes can be generated by our experiment), we often find useful to consider the intuitive notion that the outcomes are equally likely; this allows to introduce what is known as the classical definition of probability. For any set \\(A\\) made of some collection of outcomes from the sample space, we define the probability of \\(A\\) as:\n\\[\n\\text{Pr}(A)=\\frac{\\text{number of cases in}\\;A}{\\text{number of cases in the sample space}}\n\\]\nAfter the sample space has been defined and its size determined, we calculate probabilities of sets by counting the number of possible cases.\nThe art of counting is a relevant part of a field in mathematics known as combinatorics. In this post, I present the basic principle of counting and apply it to a number of situations that are often encountered in probabilistic models."
  },
  {
    "objectID": "posts/combinatorics/index.html#the-counting-principle",
    "href": "posts/combinatorics/index.html#the-counting-principle",
    "title": "Pills of combinatorics",
    "section": "The counting principle",
    "text": "The counting principle\nConsider an experiment that consists of two consecutive stages. The possible outcomes of the first stage are \\(a_1,\\cdots,a_n\\) and, for each outcome from the first stage, \\(b_1,\\cdots,b_m\\) outcomes are possible for the second stage. The outcomes of the two-stage experiment are thus the ordered pairs \\((a_i,b_j),i=1,...,m;j=1,\\cdots,m\\), whose number is \\(nm\\). An obvious generalization is for an \\(r\\)-stage experiment, with \\(n_i,i=1,2,\\cdots,r\\) outcomes each. The total number of outcomes is\n\\[\nN=\\prod_{i=1}^rn_i\n\\]\n\nExample 1 (Number of subsets of an \\(n\\)-element set.) Consider an \\(n\\)-element set \\(\\{a_1,a_2,\\cdots,a_n\\}\\). The construction of a subset can be seen as an \\(n\\)-stage process, where, at the \\(i\\)th stage, the \\(i\\)th element is offered the opportunity to be a member of the subset or not (binary choice). Therefore the number of subsets is \\(N=2^n\\), inclusive of either the empty set (no elements are in one subset) or the sample space (all the elements are in another subset).\n\nConsider now the situation where we have \\(n\\) distinct objects in a box and we want to form groups by sequentially selecting \\(k\\) objects from it, without repetitions being allowed (sampling without replacement) or with repetitions being allowed (sampling with replacement). In the former case, of course, we cannot select more objects than they are in the box, namely \\(k\\leq n\\). In the latter case, the restriction \\(k\\leq n\\) does not apply and, although unlikely, a group can even consist of \\(k\\) replicas of the same object.\nMoreover, the selection process can differ in regard to wthether we are interested in the order of selection or not. If the order of selection matters, two groups that are made by the same objects, each one with its own number of occurrences, are considered distinct, whereas, if the order of selection does not matter, they should count as one case only in the probability calculation.\nBased on these properties, four different arrangements of \\(k\\) out of a collection of \\(n\\) objects have to be considered, namely (without/with) repetition-order of selection (does/does not) matter. Each arrangement has its own name and related formula of counting, as outlined in the following."
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-without-replacement",
    "href": "posts/combinatorics/index.html#sampling-without-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling without replacement",
    "text": "Sampling without replacement\n\nOrder of selection matters\nThe number of arrangements, called permutations, can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at the second stage, the choices are reduced by one unit, i.e., \\(n-1\\); when we reach the \\(k\\)-stage, we are left with \\(n-k+1\\) choices. Applying the principle of counting, we have:\n\\[\nD(n,k)=n(n-1)\\cdots (n-k+1)=\\frac{n!}{(n-k)!}\n\\tag{1}\\]\nwhere the factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\), i.e., \\(n!=1\\cdot2\\cdots n\\). Recall that, when \\(k=n\\), \\(D(n,n)=n!\\) (\\(0!=1!=1\\)).\n\n\nOrder of selection does not matter\nThe number of arrangements, called combinations, can be calculated by noting that there exist \\(P(k,k)=k!\\) sequences of length \\(k\\) that differ from one another just in terms of the order of the presentation of their elements. They contribute only one arrangement to the total number of them:\n\\[\nC(n,k)=\\frac{D(n,k)}{D(k,k)}=\\frac{n!}{(n-k)!k!}={n\\choose k}\n\\tag{2}\\]\nwhere the binomial coefficient \\({n\\choose k}\\), indexed by the pair of integers \\(n\\geq k\\geq0\\), is considered.\n\nExample 2 (Newton’s binomial theorem) For any real number \\(n\\) that is not a non-negative integer, the theorem states that:\n\\[\n\\sum_{k=0}^n{n\\choose k}a^kb^{n-k}=(a+b)^n\n\\]\nwhen \\(a,b\\in\\mathbb{R}\\).\nSince \\({n\\choose k}\\) is the number of \\(k\\)-element sequences of a given \\(n\\)-element set, the sum over \\(k\\) of \\({n\\choose k}\\) counts the number of subsets of all possible sizes. Using the result of Example 1:\n\\[\n\\sum_{k=0}^{n}{n\\choose k}=2^n\n\\]\nThis result is also a simple application of the Newton’s binomial theorem for \\(a=b=1\\).\n\nRecall that a combination is a choice of \\(k\\) elements out of an \\(n\\)-element set without regard to order. This is the same as partitioning the set in two: one part contains \\(k\\) elements and the other contains the remaining \\(n−k\\). We can generalize by considering partitions in more than two subsets. We have \\(n\\) distinct objects and we are given non-negative integers \\(n_1,n_2,\\cdots,n_r\\), whose sum is equal to \\(n\\). The \\(n\\) items are to be divided into \\(r\\) disjoint groups, with the \\(i\\)th group containing exactly \\(n_i\\) items. The total number of groups is given by the multinomial coefficient:\n\\[\nN_r={n\\choose n_1n_2\\cdots n_r}=\\frac{n!}{n_1!n_2!\\cdots n_r!}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-with-replacement",
    "href": "posts/combinatorics/index.html#sampling-with-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling with replacement",
    "text": "Sampling with replacement\n\nOrder of selection matters\nThe number of arrangements, called dispositions can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at all the other stages, the choices are still \\(n\\). Applying the principle of counting, we have:\n\\[\nD^*(n,k)=n^k\n\\tag{3}\\]\n\n\nOrder of selection does not matter\nThe number of arrangements, called partitions is given by:\n\\[\nC^*(n,k)=\\frac{(n+k-1)!}{k!(n-1)!}={n+k-1\\choose k}\n\\tag{4}\\]\nTo understand the formula of Equation 4, think of writing \\(C^*(n,k)\\) when, for instance, \\(n=6,k=4\\) as, say, \\(a_1a_2a_3a_2\\) or equivalently \\(a_1*a_2**a_3*a_4\\), where any element \\(a_i,i=1,2,\\cdots,n\\) is followed by a number of asterisks equal to the number of its occurrences in the sequence (the total number of asterisks in the equivalent representation needs to be equal to \\(k\\)). Another example: \\(a_2a_3a_3a_3\\) and \\(a_1a_2*a_3***a_4\\). It is observed that a one-to-one correspondence exists between the original arrangements and all possible permutations in the alignment of elements and asterisks in the equivalent representation. Since each alignment starts with \\(a_1\\), we need to permute \\(n+k-1\\) elements, among which \\(k\\) (the asterisks) and \\(n-1\\) (the elements \\(a_i,i=2,\\cdots,n\\)) are equal."
  },
  {
    "objectID": "posts/combinatorics/index.html#formulae",
    "href": "posts/combinatorics/index.html#formulae",
    "title": "Pills of combinatorics",
    "section": "Formulae",
    "text": "Formulae\nIn conclusion, the four different arrangements of \\(k\\) out of a collection of \\(n\\) objects can be computed as follows:\n\\[\n\\begin{split}\n&\\quad\\quad\\textbf{without replacement}&\\quad\\quad\\textbf{with replacement}\\\\\n\\\\\n\\textbf{order matters}&\\quad\\quad\\text{permutations}&\\quad\\quad\\text{dispositions}\\\\\n&\\quad\\quad D(n,k)=\\dfrac{n!}{(n-k)!}&\\quad\\quad D^*(n,k)=n^k\\\\\n\\textbf{order does not matter}&\\quad\\quad\\text{combinations}&\\quad\\quad\\text{partitions}\\\\\n&\\quad\\quad C(n,k)={n\\choose k}&\\quad\\quad C^*(n,k)={n+k-1\\choose k}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#exercises",
    "href": "posts/combinatorics/index.html#exercises",
    "title": "Pills of combinatorics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Given a group of \\(n\\) individuals, count how many subgroups can be formed having one particular person as the leader, and a number (from 0 to \\(n-1\\)) of additional members.\nAnswer The counting principle can be applied as follows. First, we have \\(n\\) possible choices for the leader, and once the leader is chosen, \\(2^{n-1}\\) subsets can be considered, which may include from none to all the remaining individuals of the group. We have therefore:\n\\[\nN=n2^{n-1}\n\\]\n\n\nExercise 2 How many words that consists of four distinct letters can be formed?\nAnswer The sample space is composed of \\(n=26\\) elements. The sequences we are interested are formed by \\(k=4\\) elements, repetitions are not allowed. Two words are considered distinct based on the order of appearance of the four letters into them. The number of words is then given by Equation 1:\n\\[\nN=D(n,k)=\\frac{n!}{(n-k)!}=\\frac{26!}{22!}=26\\cdot25\\cdot24\\cdot23=358,800\n\\]\n\n\nExercise 3 How many combinations exist using two letters out of four letters?\nAnswer The sample is composed of \\(n=4\\) elements. The sequences we are interested are formed by \\(k=2\\) distinct elements, repetitions are not allowed. Two sequences are considered the same if they differ just by the order of the elements in them, namely \\(a_1a_2\\) is the same as \\(a_2a_1\\). The number of combinations is then given by Equation 2:\n\\[\nN=C(n,k)={n\\choose k}=\\frac{4!}{2!2!}=6\n\\]\n\n\nExercise 4 How many distinct combinations can be formed from the letters ATALANTA?\nAnswer There are \\(n=8\\) letters which may be arranged in \\(n!\\) ways, but this leads to double counting. If the \\(k_1=4\\) “A”s are permuted, then nothing is changed, and similarly for the \\(k_2=2\\) “T”s. The total number of distinct combinations is then given by the multinomial coefficient:\n\\[\nN=\\frac{n!}{k_1!k_2!}=\\frac{8!}{4!2!}=7\\cdot6\\cdot5\\cdot4=840\n\\]"
  },
  {
    "objectID": "posts/resolution/index.html",
    "href": "posts/resolution/index.html",
    "title": "Frequency resolution of spectral analysis",
    "section": "",
    "text": "Frequency resolution is the size of the smallest frequency for which details in the frequency response and the spectrum can be resolved by the estimate. For example, a resolution of 0.1 Hz means that the frequency response variations at frequency intervals at or below 0.1 Hz cannot be resolved.\nConsider an analog band-limited signal \\(x(t)\\) with bandwidth \\(B\\) Hz; \\(x(t)\\) is observed over a sample period of \\(T_r\\) s (the length of the data record) and sampled at a sampling frequency of \\(f_s\\) Hz (\\(T_s=1/f_s\\) denotes the sampling interval). The total available number of samples of \\(x(t)\\) is \\(N=\\lfloor T_r/T_s\\rfloor\\), where \\(\\lfloor\\cdot\\rfloor\\) denotes the floor function (or greatest integer function), namely the function that takes as input a real number \\(r\\) and gives as output the greatest integer less than or equal to \\(r\\).\nAccording to the Shannon-Nyquist sampling theorem, if the sampling frequency \\(f_s\\) is chosen to be \\(2B\\), the maximum resolvable frequency is:\n\\[\nf_{\\text{max}}=\\frac{f_s}{2}\n\\]\nOn the other hand, the minimum resolvable frequency is inversely related to the sample period:\n\\[\nf_{\\text{min}}=\\frac{1}{T_r}=\\frac{1}{NT_s}\n\\tag{1}\\]\nand hence the number of frequencies that can be resolved from \\(f_{\\text{min}}\\) to \\(f_{\\text{max}}\\) is\n\\[\nN_f=\\frac{f_{\\text{max}}-f_{\\text{min}}}{\\Delta f}\n\\]\nwhere \\(\\Delta f\\) is the frequency resolution. Since \\(\\Delta f=f_{\\text{min}}\\), I also have:\n\\[\nN_f=\\dfrac{\\dfrac{f_s}{2}-\\dfrac{f_s}{N}}{\\dfrac{f_s}{N}}=\\frac{N}{2}-1\n\\]\nThis implies that will be \\(N/2\\) discrete frequencies from \\(0\\) to \\(f_{\\text{max}}\\).\nThe accurate detection of frequency components in the spectrum \\(X(f)\\) of the signal \\(x(t)\\) is challenged by the phenomenon known as spectral leakage, or amplitude ambiguity; it consists of ambiguous and false amplitudes occurring in the spectrum \\(X(f)\\) whenever the sample period \\(T_r\\) is not an integer multiple of all of the contributory periods in \\(x(t)\\). That is, false or ambiguous amplitudes will occur at frequencies that are immediately adjacent to the actual frequency.\nFor aperiodic signals, \\(T_r\\) theoretically must be infinite. For periodic signals, \\(T_r\\) must be equal to the least common integer multiple of all the periods contained in the signal. Application of the DFT (Discrete Fourier Transform) or FFT (Fast Fourier Transform) to an aperiodic signal implicitly assumes that the signal is infinite in length and formed by repeating the signal of length \\(T_r\\) an infinite number of times. This leads to discontinuities in the amplitude that occur at each integer multiple of \\(T_r\\). These discontinuities are step-like, which introduce false amplitudes that decrease around the main frequencies.\nAll the periods contained in the signal cannot be known before the spectral analysis is performed, therefore the stated condition on the sample period cannot be fulfilled; the best method to minimize the effect of spectral leakage is windowing.\nHerein, I just provide a brief explanation of windowing, without testing it in the examples to follow. Windowing reduces the amplitude of the discontinuities at the boundaries of each finite sequence of samples of \\(x(t)\\). It does so by multiplying the acquired sequence by a finite-length window with an amplitude that varies smoothly and gradually toward zero at the edges. This makes the endpoints of the waveform meet and, therefore, results in a continuous waveform without sharp transitions.\n\n\n\n\n\n\nCan discrete-time sinusoids be non-periodic?\n\n\n\nThe normalized frequency \\(f\\) in cycles per sample of a discrete-time sinusoid:\n\\[\nx[n]=\\cos(2\\pi fn)\n\\]\nis restricted to values in the interval \\(-1/2\\leq f\\leq1/2\\). This is because, for any discrete-time sinusoid with frequency \\(\\vert f\\vert&gt;1/2\\), an integer \\(m\\) exists such that \\(f=f_0+m\\) and \\(\\vert f_0\\vert\\leq1/2\\):\n\\[\n\\cos 2\\pi fn=\\cos[2\\pi (f_0+m)\\,n]=\\cos(2\\pi f_0n)+\\cos(2\\pi mn)=\\cos(2\\pi f_0n)\n\\]\nIt is worth noting that the highest rate of oscillation of discrete-time sinusoids is when, at every time instant, the output sample flips polarity with respect to the previous output sample:\n\\[\nf_0=\\frac{1}{2}\\rightarrow\\cos(2\\pi f_0n)=\\cos(\\pi n)=(-1)^n\n\\]\nMoreover, a discrete-time sinusoid can be periodic, i.e, characterized by patterns that exactly repeat themselves in time, if and only if its frequency can be expressed in terms of a rational number; moreover, an additive mixture of periodical discrete-time sinusoids (called here sinusoidal mixture) is periodic with period equal to the least common integer multiple of their periods.\n\n\nAs outlined above, a spectrum line at frequency \\(f\\) will be accurately represented by the DFT when \\(f_s\\geq2f_{\\text{max}}\\) and \\(T_r = mT\\), where \\(m=1,2,\\cdots\\), and \\(T=1/f\\). This implies that \\(N=mf_s/f\\). If \\(T_r\\) is not an integer multiple of \\(T\\), leakage will occur in the DFT. This appears as amplitudes at \\(f\\) spilling onto adjacent frequencies.\nAs an example, consider a sinusoidal mixture with three unit-amplitude components at 2.875 Hz, 3 Hz, 3.125 Hz. The sampling frequency is set to \\(f_s=16\\) Hz and the sample period to \\(T=8\\) s (\\(N=128\\)): the component at 2.875 Hz is observed for 23 periods, the component at 3 Hz for 24 periods, and the component at 3.125 Hz for 25 periods. The resolution calculated according to Equation 1 is 0.125 Hz.\nA chunk of code written in MATLAB (Natick, Massachusetts: The MathWorks, Inc., https://www.mathworks.com) shows how to simulate the sinusoidal mixture and to perform spectral analysis.\n% *****************************\n% sinusoidal mixture generation\n% *****************************\n\nfs = 16;   % sampling frequency, Hz\nTs = 1/fs; % sampling interval, s\nT  = 8;    % sample period, s\n\nt  = (0:Ts:T-Ts); % time domain, s\nN  = length(t);   % number of samples\n\nf1 = 2.9375;      % frequency component 1, Hz\nf2 = 3;           % frequency component 2, Hz\nf3 = 3.0625;      % frequency component 3, Hz\n\nx1 = cos(t.*(2*pi*f1)); % sinusoid 1\nx2 = cos(t.*(2*pi*f2)); % sinusoid 2\nx3 = cos(t.*(2*pi*f3)); % sinusoid 3\nX  = x1 + x2 + x3;      % sinusoidal mixture\n\n% ********************************************************\n% calculation of single-side spectrum (see comments below)\n% ********************************************************\n\nP              = fft(X, N);         % FFT calculation\nP2             = abs(P/N);          % double-side spectrum, rescaled by N     \nP1             = P2(1:N/2+1);  \nP1(:, 2:end-1) = 2*P1(2:end-1); \nY              = P1(1:end-1);       % single-side spectrum\nf              = (0:N/2-1).*(fs/N); % frequency domain, Hz\n\n\n\n\n\n\nComments\n\n\n\nThe documentation in MATLAB explains the procedure to convert the N-points FFT spectrum of the signal to the single-side amplitude spectrum:\n\nBecause the FFT function includes a scaling factor N between the original and the transformed signals, rescale the spectrum by dividing by N.\nTake the complex magnitude of the FFT spectrum. The two-side amplitude spectrum P2, where the spectrum in the positive frequencies is the complex conjugate of the spectrum in the negative frequencies, has half the peak amplitudes of the time-domain signal.\nTo convert to the single-side spectrum Y, take the first half P1 of the two-side spectrum P2 and multiply by 2. P1(1) and P1(end) are not multiplied by 2 because these amplitudes correspond to the zero and Nyquist frequencies, respectively, and they do not have the complex conjugate pairs in the negative frequencies.\nDefine the frequency domain f for the single-side spectrum Y.\n\n\n\nThe single-side spectrum of the sinusoidal mixture is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Single-side spectrum of the sinusoidal mixture composed of three components at 2.875 Hz, 3 Hz, 3.125 Hz; sampling frequency: 16 Hz; sample period: 8 s. For the sake of visualization the frequency interval shown is limited to 2.5-3.5 Hz.\n\n\n\nConsider now the case that the three components of the sinusoidal mixture have frequencies 2.9375 Hz, 3 Hz, 3.0625 Hz. To resolve them, I would need to double the frequency resolution of the FFT, up to 0.0625 Hz, with respect to the scenario of Figure 1. Erroneously, I double the sampling frequency. However, this expedient leaves the frequency resolution unaltered, since doubling the sample period without touching the sample period also doubles the number of samples. Moreover, the sample period is not properly chosen to avoid spectral leakage, as clearly seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 32 Hz; sample period: 8 s.\n\n\n\nTo resolve the components of the sinusoidal mixture and avoid spectral leakage, the frequency resolution of 0.0625 Hz can be achieved by extending the time period to \\(T_r=16\\) s, which corresponds to \\(N=256\\) samples at the original sampling frequency \\(f_s=16\\) Hz, see Figure 3.\n\n\n\n\n\n\nFigure 3: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 16 Hz; sample period: 16 s.\n\n\n\n\nTo conclude: whereas the sampling frequency sets the time resolution, the sample period occupies a central place in setting the frequency resolution when spectral analysis is performed. The sample period being the same, just increasing the sampling frequency simply increases the number of samples by the same ratio, leaving their ratio unaltered. The only means to increase the frequency resolution is to increase the sample period, trying at the same time to minimize the effects of spectral leakage."
  },
  {
    "objectID": "posts/random_incidence/index.html",
    "href": "posts/random_incidence/index.html",
    "title": "Random incidence",
    "section": "",
    "text": "Consider the situation when the continuous time axis of our observations is partitioned into a sequence of interarrival intervals. With the term arrival, we designate the occurrence of everything we are interested to observe, for instance, a particle emitted from radioactive material and captured by a radiation counter, a message reaching its destination queue, maybe the bus that we are anxiously waiting for at the bus stop. Usually, probabilistic models that attempt to describe these different type of arrivals share the same assumption, namely the interarrival times (i.e., the times between successive arrivals) are independent random variables: for instance, the continuous-time Poisson process (hopefully, not the right model to predict the next bus arrival anyway!) is the case where the interarrival times are modeled as independent identically exponentially distributed random variables.\n\n\n\n\n\n\nExponential distribution\n\n\n\nA continuous random variable \\(X\\) is said to be exponential, or exponentially distributed with parameter \\(\\lambda\\), when its cumulative distribution function (CDF) is written\n\\[\nF_X(x)=\\text{Pr}(X\\leq x)=1-e^{-\\lambda x},\\;x\\geq0\n\\]\nThe probability density function (PDF) is then given by:\n\\[\np_X(x)=\\frac{d}{dx}F_X(x)=\\lambda\\,e^{-\\lambda x},\\;x\\geq0\n\\]\nThe mean value, the mean square value and the variance of \\(X\\) are:\n\\[\n\\left\\{\n\\begin{split}\nE[X]&=\\int_0^{\\infty}xp_X(x)\\,dx=\\frac{1}{\\lambda}\\\\\nE[X^2]&=\\int_0^{\\infty}x^2p_X(x)\\,dx=\\frac{2}{\\lambda^2}\\\\\n\\text{Var}(X)&=E[X^2]-E[X]^2=\\frac{1}{\\lambda^2}\n\\end{split}\n\\right.\n\\]\nIntegration by parts can be used to calculate the two expectations \\(E[X]\\) and \\(E[X^2]\\).\n\n\n\n\n\n\n\n\nPoisson process\n\n\n\nConsider a sequence of independent exponential random variables \\(T_1,T_2,T_3\\cdots,\\) with common parameter \\(\\lambda\\), and let these stand for the interarrival times. The arrivals are then recorded at times \\(T_1,T_1+T_2,T_1+T_2+T_3,\\cdots\\) and so forth, to define a continuous-time Poisson process.\nA Poisson process is endowed with the following important properties:\n\nIndependence of non-overlapping sets of times. This is a direct consequence of the assumed independence of the interarrival times.\nFresh-start property. The part of the Poisson process that starts at any particular time \\(t&gt;0\\) is a probabilistic replica of the Poisson process starting at time 0, and is independent of the part of the process prior to time \\(t\\). This can be seen as a special case of point 1.\nMemoryless interarrival time distribution. If \\(T\\) is the time of the first arrival and if we know that \\(T&gt;t\\), then the remaining time \\(T−t\\) is exponentially distributed, with the same parameter \\(\\lambda\\):\n\n\\[\n\\begin{split}\n\\text{Pr}(T&gt;t+s\\,\\vert\\,T&gt;t)&=\\frac{\\text{Pr}(T&gt;t+s,T&gt;t)}{\\text{Pr}(T&gt;t)}\\\\\n&=\\frac{\\text{Pr}(T&gt;t+s)}{\\text{Pr}(T&gt;t)}=\\frac{1-F_T(t+s)}{1-F_T(t)}\\\\\n&=\\frac{e^{-\\lambda(t+s)}}{e^{-\\lambda t}}=e^{-\\lambda s}=\\text{Pr}(T&gt;s)\n\\end{split}\n\\]\nProperties 2.-3. can be rephrased saying that the ones whose life is modeled by an exponential distribution, well they should remain forever young: no matter how long they have lived so far, the remaining time to their death is predicted as if they are just born!\n\n\nThe term random incidence denotes the arrival of an observer at an arbitrary time \\(t^*\\) into a gap between two consecutive arrivals in an arrival-type process that is not necessarily described by a Poisson model, Figure 1.\n\n\n\n\n\n\nFigure 1: Illustration of the random incidence phenomenon.\n\n\n\nWe are not saying that \\(t^*\\) is random; in this regard, perhaps, using the term random incidence may appear misleading. However, the interval between the time of the previous arrival \\(t_p\\) and \\(t^*\\), and the interval between \\(t^*\\) and the time of the next arrival \\(t_n\\) are random. All we need to state to start our discussion is to assume that the observer enters the arrival-type process in a situation when a previous arrival has surely occurred: probabilistically, the gap \\(t_n-t_p\\) is then a well-defined quantity.\nSuppose that we know the probability law of the interarrival times \\(Y\\). Let us denote by \\(W\\) the random variable that describes the duration of the gap entered by random incidence, \\(W=T_n-T_p\\). Finally, we denote by \\(T\\) the random variable that describes the waiting time for the next arrival from when the gap is entered by random incidence, \\(T=T_n-T^*\\).\nIt is argued that the probability that \\(W\\) assumes a value between \\(w\\) and \\(w+dw\\) is proportional to both the the duration of the gap \\(w\\) and the relative frequency of occurrence of such gaps \\(p_Y(w)dw\\):\n\\[\n\\text{Pr}(w\\leq W\\leq w+dw)=p_W(w)dw\\propto w\\,p_Y(w)dw\n\\]\nTherefore:\n\\[\np_W(w)=\\frac{w\\,p_Y(w)}{E[Y]}\n\\tag{1}\\]\nwhere the constant of proportionality is calculated to enforce the constraint of normalization for the PDF \\(p_W(w)\\) of \\(W\\) (its integral from 0 to infinity must be 1).\nNow, given that a gap of length \\(w\\) is entered by random incidence, we are equally likely to be anywhere within the gap. More precisely, given \\(w\\), the time until the next arrival \\(T\\) has a uniform PDF:\n\\[\np_{T\\vert W}(t\\,\\vert\\,w)=\\frac{1}{w},\\;0\\leq t\\leq w\n\\tag{2}\\]\nwhere \\(p_{T\\vert W}(t\\,\\vert\\,w)\\) is the conditional PDF of \\(T\\) given \\(W\\). Using Equation 1, Equation 2 the joint PDF \\(p_{TW}(t,w)\\) can be written:\n\\[\np_{TW}(t,w)=p_{T\\vert W}(t\\,\\vert\\,w)\\,p_W(w)=\\frac{p_Y(w)}{E[Y]},\\;0\\leq t\\leq w&lt;\\infty\n\\]\nFinally, the marginal PDF \\(p_T(t)\\) of the waiting time for the next arrival from when the gap is entered by random incidence can be formed simply by integrating out \\(W\\):\n\\[\n\\boxed{p_T(t)=\\int_{t}^{\\infty}\\frac{p_Y(w)}{E[Y]}dw=\\frac{1-F_Y(t)}{E[Y]},\\;t\\geq0}\n\\tag{3}\\]\n\nExample 1 Consider a bus passenger arriving at a bus stop. The probabilistic law of the bus headways \\(F_Y(y)\\) will determine the probability law for the waiting time until the next bus arrives, \\(p_T(t)\\) via Equation 3, if we ignore interactions between successive buses and assume that the arrivals are identically distributed and independent.\nSuppose that buses maintain perfect headways, being always spaced \\(T_0\\) minutes apart:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad y&lt;T_0\\\\1&\\quad y\\geq T_0\\end{split}\\right.\\rightarrow p_Y(y)=\\delta(y-T_0)\\rightarrow E[Y]=T_0\n\\]\nwhere the PDF is expressed in terms of a delta function located at time \\(T_0\\). The PDF of \\(T\\) can be written:\n\\[\np_T(t)=\\left\\{\\begin{split}&\\frac{1}{T_0}&\\quad 0\\leq t\\leq T_0\\\\&0&\\quad t&gt;T_0\\end{split}\\right.\\rightarrow E[T]=\\frac{T_0}{2}\n\\]\nAs expected intuitively, the time until the next arrival, given random incidence, is uniformly distributed between \\(0\\) and \\(T_0\\), with mean value \\(E[T]=T_0/2\\): if \\(T_0=60\\) min the average waiting time is 30 min.\nNow, suppose that the bus headways are on the hour, and fifteen minutes after the hour. Thus, the interarrival times alternate between 15 and 45 minutes. If the bus passenger shows up at the bus stop at any time uniformly distributed within a hour, she/he has to wait for an average time of 15/2 min (with probability 1/4) and 45/2 min (with probability 3/4):\n\\[\nE[T]=\\frac{15}{2}\\cdot\\frac{1}{4}+\\frac{45}{2}\\cdot\\frac{3}{4}=18.75\\;\\text{min}\n\\]\nMore formally:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad0\\leq y&lt;15\\\\1/2&\\quad15\\leq y&lt;45\\\\1&\\quad y\\geq45\\end{split}\\right.\\rightarrow E[Y]=30\\;\\text{min}\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\left\\{\\begin{split}1/30&\\quad0\\leq t&lt;15\\\\1/60&\\quad15\\leq t&lt;45\\\\0&\\quad t\\geq45\\end{split}\\right.\\rightarrow E[T]=18.75\\,\\text{min}\n\\]\nOn average, the bus passenger has to wait longer than it might be expected taking into account \\(E[Y]\\) only, namely \\(E[Y]/2=15\\) min.\n\nThis is because an observer who arrives at an arbitrary time, the bus passenger in this example, is more likely to fall in a large rather than a small interarrival interval: large interarrival intervals tend therefore to determine longer waiting times!\n\n\n\nExample 2 Consider the (unrealistic) case that the bus headways are Poissonian. What does this mean? Basically, we state that the interarrival times are modeled by independent exponentially distributed random variables with rate \\(\\lambda\\) (the rate denotes the number of arrivals per unit time):\n\\[\nF_Y(y)=1-e^{-\\lambda t},\\;t\\geq 0\\rightarrow p_Y(y)=\\frac{F_Y(y)}{dy}=\\lambda e^{-\\lambda t},\\;t\\geq0\\rightarrow E[Y]=1/\\lambda\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\lambda e^{-\\lambda t},\\;t\\geq0\n\\]\nHence \\(T\\) is exponentially distributed with rate \\(\\lambda\\). The average time of waiting is \\(E[T]=1/\\lambda\\). No matter when the event occurred, the observer who arrives at an arbitrary time \\(t^*\\) (the bus passenger in this example) sees the Poisson process to start fresh at time \\(t^*\\), Figure 1. It is worth noting that, according to the properties of independence, start-fresh and memorylessness stated above, we can run a Poisson process either forwards or backwards in time, without its properties are modified. Hence, not only \\(T_n-T^*\\), but also \\(T^*-T_p\\) is exponentially distributed with parameter \\(\\lambda\\). Moreover, \\(T_n-T^*\\) and \\(T_n-T^*\\) are independent. We have therefore established that the gap \\(W\\) entered by random incidence is the sum of two independent exponential random variables with parameter \\(\\lambda\\), with mean \\(2/\\lambda\\). More formally, using Equation 1 we get:\n\\[\np_W(w)=\\lambda w\\,e^{-\\lambda w},\\;w\\geq0\n\\]\nWe recall that the random variable \\(X\\) Erlang of order \\(k\\) has PDF:\n\\[\np(x)=\\frac{\\lambda^kx^{k-1}e^{-\\lambda x}}{(k-1)!}\n\\]\nIn conclusion, the gap duration is a random variable Erlang of order 2.\n\nIn a similar fashion to Example 1, when landing into a Poisson process at an arbitrary time, we are more likely to fall in a large interarrival interval; the length of what we perceive as a typical interarrival interval is then greater than it is in reality."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "I am an associate professor at the BioRobotics Institute of the Scuola Superiore Sant’Anna, in Pisa, Italy. In my posts, I attempt to explain various concepts that I studied, in my double role of teacher and researcher.\nThere have been two main reasons for me to start this blog: first, learning - since writing is a very effective tool against shallow understanding (mine!); second, I wish to help others who may be struggling with similar problems.\nMy teaching is in the general area of instrumentation & measurement, statistics, signal processing and data analysis. My research focuses on developing algorithms for wearable inertial-sensor-based systems applied to motion analysis and assessment of human performance.\nRecently, I started studying the theory of diffusive processes for the modeling of time series of human motion.\nFor a record of my publications, please visit this site. Feel free to reach out to me via mail."
  }
]