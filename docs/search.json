[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCorrespondence analysis: Part I\n\n\n\n\n\n\nmultivariate statistics\n\n\n\nCorrespondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. Conceptually, CA is similar to principal component analysis, but applies to categorical rather than continuous data. In this post, I will briefly present the theory behind simple CA (the method used when data of only two categorical variables are analyzed), leaving details of how to carry the analysis using the R programming software to a future post.\n\n\n\n\n\nDec 9, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nAudio features for free\n\n\n\n\n\n\ndata mining\n\n\n\nSpotify is the leader in the audio streaming market, with its several million subscribers, including myself, and many more listeners who use the app for free. Although not necessarily known to ordinary users, each track in the Spotify library comes accompanied by an extensive list of numerical scores - the outcome of the audio analysis each track is submitted to using advanced algorithms developed by Spotify. No matter what the use is for them, these data, which are usually hidden to the user, can be retrieved using the Web API’s offered by Spotify. In this post I explain how this can be done.\n\n\n\n\n\nNov 12, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical simulation for stochastic differential equations\n\n\n\n\n\n\nstochastic calculus\n\n\n\nA down-to-the-bone post, where few issues concerning the numerical simulation of stochastic differential equations are discussed with just a limited amount of technical detail.\n\n\n\n\n\nJun 6, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nSignificant figures\n\n\n\n\n\n\nmeasurement\n\n\n\nThe use of calculators and computers leads to lab reports with far too many digits in every number produced. Assessing the correct number of significant figures is essential in reporting either experimental or computed results together with their stated uncertainties.\n\n\n\n\n\nMay 14, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nAugmented Dickey-Fuller test\n\n\n\n\n\n\ntime series\n\n\n\nIn statistics, an augmented Dickey–Fuller test (ADF) tests the null hypothesis of a unit root in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. This post explains how to use the ADF test in R, with an attempt to make the different test statistics clear and easily interpretable.\n\n\n\n\n\nMay 13, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGambler’s ruin\n\n\n\n\n\n\nprobability\n\n\n\nThe gambler’s ruin problem is often applied to gamblers with finite wealth playing against a bookie or casino assumed to have a much larger amount of wealth available, in principle infinite. It can then be proven that the probability of the gambler’s eventual ruin tends to 1 even in the scenario where the game is fair.\n\n\n\n\n\nMay 5, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate probability regions\n\n\n\n\n\n\nstatistics\n\n\n\nComputing probability regions in the space where sample data are assumed to live relates to the determination of regions that we are confident that the underlying population will occupy with the prescribed value of probability. After reviewing the theory for multivariate normal distributions, I present an example of application from the field of posturographic research.\n\n\n\n\n\nMay 2, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence intervals for proportions\n\n\n\n\n\n\nstatistics\n\n\n\nUsually confidence intervals for the estimation of proportions are based on methods that exploit the normal approximation to the binomial distribution. By simulation, two of these methods (Wilson and Wald) are tested for their ability to provide the stated coverage for small-to-large samples.\n\n\n\n\n\nApr 27, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\nprobability\n\n\n\nAlthough experimentalists are well familiar with the topic, nonetheless I believe it might be helpful to spend a few minutes for a review of basic formulae of combinatorial analysis.\n\n\n\n\n\nApr 24, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\nprobability\n\n\n\nIn this post, I briefly discuss the random incidence phenomenon, using the classical example of a person arriving at a bus stop at a random time, and waiting for the arrival of the next bus.\n\n\n\n\n\nApr 22, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\nsignal processing\n\n\n\nSpectral leakage and length of data record hamper our ability to resolve spectral lines by DFT/FFT analysis. In this post, I briefly discuss this problem, with examples using sinusoidal mixtures.\n\n\n\n\n\nApr 13, 2024\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Correspondence analysis: Part I\n\n\n\n\n\n\n\n\nDec 9, 2024\n\n\n\n\n\n\n\nAudio features for free\n\n\n\n\n\n\n\n\nNov 12, 2024\n\n\n\n\n\n\n\nNumerical simulation for stochastic differential equations\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\nSignificant figures\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\nAugmented Dickey-Fuller test\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\n\n\n\n\n\nGambler’s ruin\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nMultivariate probability regions\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\nConfidence intervals for proportions\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\n\n\nApr 13, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dickey_fuller/index.html",
    "href": "posts/dickey_fuller/index.html",
    "title": "Augmented Dickey-Fuller test",
    "section": "",
    "text": "Roughly, for a time series to be stationary three conditions are needed:\n\nit shows mean reversion, namely it fluctuates around a constant long-term mean\nit has finite variance that is time-invariant.\nautocorrelations decay relatively fast as lag lenghts increase.\n\nThe identification of stationary series can be done by checking whether the autocorrelation function (ACF) drops to zero relatively quickly; usually, the ACF of non-stationary data decreases slowly, and the first-lag value is often large and positive. However, this method is necessarily imprecise, leading to ambiguous situations that cannot be easily deciphered, especially when the sample size is small.\n\n\n\n\n\n\nBackshift notation\n\n\n\nThe backward shift operator \\(B\\) is defined as follows:\n\\[\nBY_t=Y_{t-1}\n\\]\nThe operator \\(B\\) operates on the \\(t\\)th sample of a time series, with the effect of shifting the sample back one period (first difference). Recall that two applications of the backward shift operator to \\(Y_t\\) shift the sample back two periods (second difference):\n\\[\nB(BY_t)=B^2Y_t=Y_{t-2}\n\\]\nFor example, in the case of monthly data, if we wish to consider the same month last year, the notation is \\(B^{12}Y_t=Y_{t-12}\\).\nThe backward shift operator is useful to describe the operation of differencing, the technique of election to stabilize the mean value of nonstationary time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\nA first-order difference is defined as follows:\n\\[\nY^{\\prime}_t=Y_t-Y_{t-1}=Y_t-BY_t=(1-B)Y_t\n\\]\nIf a second-order difference is considered, namely the first-order difference of a first-order difference, we can write:\n\\[\n\\begin{split}\nY^{\\prime\\prime}_t&=Y^{\\prime}_t-Y^{\\prime}_{t-1}\\\\\n&=(1-B)^2Y_t=Y_t-2BY_t+B^2Y_t\\\\\n&=Y_t-2Y_{t-1}+Y_{t-2}\n\\end{split}\n\\]\nA \\(d\\)-order difference can be written \\((1-B)^d\\). It is important to note that a second-order difference, denoted by \\((1-B)^2\\), is not the same as a second difference, which is denoted by \\(B^2\\).\nFor example, a seasonal difference followed by a first difference can be written:\n\\[\n\\begin{split}\nY^{\\prime}_t&=(1-B)(1-B^{12})Y_t\\\\\n&=(1-B-B^{12}-B^{13})Y_t\\\\\n&=Y_t-Y_{t-1}-Y_{t-12}-Y_{t-13}\n\\end{split}\n\\]\nSometimes, as I will always do in the following of this post, the notation \\(\\Delta\\) is also used to indicate the first-order difference, i.e., \\(\\Delta Y_t=(1-B)Y_t\\).\n\n\nA unit root process, also called difference-stationary process (DSP), is a data-generating process whose first difference is stationary.\nThere are two basic models for time series with linear growth characteristics:\n\nTrend stationary process\n\n\\[\nY_t=c+\\delta t+\\text{stationary process}\n\\]\n\nUnit root process\n\n\\[\nY_t=Y_{t-1}+\\text{stationary process}\n\\]\nThe processes are indistinguishable for short data records, in the sense that both a trend stationary process (TSP) and a DSP can fit short data records extremely well. However, the processes can be distinguished when restricted to a particular subclass of data-generating processes, such as AR(\\(p\\)) processes.\nConsider the case of an AR(1) process:\n\\[\nY_t=a_1Y_{t-1}+\\epsilon_t\n\\]\nWe may be interested in testing the hypothesis \\(a_1=1\\). Since under the null hypothesis the sequence \\(\\{Y_t\\}\\) is generated by a nonstationary process, and the variance becomes infinitely large as \\(t\\) increases, classical statistical methods cannot be used.\nDickey and Fuller devised a procedure to formally test for the presence of a unit root against a number of possible alternatives for explaining the data. It is important to recall that TSP and DSP can produce different forecasts and can give rise to spurious regressions, therefore their detection in time series is a truly important task."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#stationary-time-series",
    "href": "posts/dickey_fuller/index.html#stationary-time-series",
    "title": "Augmented Dickey-Fuller test",
    "section": "",
    "text": "Roughly, for a time series to be stationary three conditions are needed:\n\nit shows mean reversion, namely it fluctuates around a constant long-term mean\nit has finite variance that is time-invariant.\nautocorrelations decay relatively fast as lag lenghts increase.\n\nThe identification of stationary series can be done by checking whether the autocorrelation function (ACF) drops to zero relatively quickly; usually, the ACF of non-stationary data decreases slowly, and the first-lag value is often large and positive. However, this method is necessarily imprecise, leading to ambiguous situations that cannot be easily deciphered, especially when the sample size is small.\n\n\n\n\n\n\nBackshift notation\n\n\n\nThe backward shift operator \\(B\\) is defined as follows:\n\\[\nBY_t=Y_{t-1}\n\\]\nThe operator \\(B\\) operates on the \\(t\\)th sample of a time series, with the effect of shifting the sample back one period (first difference). Recall that two applications of the backward shift operator to \\(Y_t\\) shift the sample back two periods (second difference):\n\\[\nB(BY_t)=B^2Y_t=Y_{t-2}\n\\]\nFor example, in the case of monthly data, if we wish to consider the same month last year, the notation is \\(B^{12}Y_t=Y_{t-12}\\).\nThe backward shift operator is useful to describe the operation of differencing, the technique of election to stabilize the mean value of nonstationary time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\nA first-order difference is defined as follows:\n\\[\nY^{\\prime}_t=Y_t-Y_{t-1}=Y_t-BY_t=(1-B)Y_t\n\\]\nIf a second-order difference is considered, namely the first-order difference of a first-order difference, we can write:\n\\[\n\\begin{split}\nY^{\\prime\\prime}_t&=Y^{\\prime}_t-Y^{\\prime}_{t-1}\\\\\n&=(1-B)^2Y_t=Y_t-2BY_t+B^2Y_t\\\\\n&=Y_t-2Y_{t-1}+Y_{t-2}\n\\end{split}\n\\]\nA \\(d\\)-order difference can be written \\((1-B)^d\\). It is important to note that a second-order difference, denoted by \\((1-B)^2\\), is not the same as a second difference, which is denoted by \\(B^2\\).\nFor example, a seasonal difference followed by a first difference can be written:\n\\[\n\\begin{split}\nY^{\\prime}_t&=(1-B)(1-B^{12})Y_t\\\\\n&=(1-B-B^{12}-B^{13})Y_t\\\\\n&=Y_t-Y_{t-1}-Y_{t-12}-Y_{t-13}\n\\end{split}\n\\]\nSometimes, as I will always do in the following of this post, the notation \\(\\Delta\\) is also used to indicate the first-order difference, i.e., \\(\\Delta Y_t=(1-B)Y_t\\).\n\n\nA unit root process, also called difference-stationary process (DSP), is a data-generating process whose first difference is stationary.\nThere are two basic models for time series with linear growth characteristics:\n\nTrend stationary process\n\n\\[\nY_t=c+\\delta t+\\text{stationary process}\n\\]\n\nUnit root process\n\n\\[\nY_t=Y_{t-1}+\\text{stationary process}\n\\]\nThe processes are indistinguishable for short data records, in the sense that both a trend stationary process (TSP) and a DSP can fit short data records extremely well. However, the processes can be distinguished when restricted to a particular subclass of data-generating processes, such as AR(\\(p\\)) processes.\nConsider the case of an AR(1) process:\n\\[\nY_t=a_1Y_{t-1}+\\epsilon_t\n\\]\nWe may be interested in testing the hypothesis \\(a_1=1\\). Since under the null hypothesis the sequence \\(\\{Y_t\\}\\) is generated by a nonstationary process, and the variance becomes infinitely large as \\(t\\) increases, classical statistical methods cannot be used.\nDickey and Fuller devised a procedure to formally test for the presence of a unit root against a number of possible alternatives for explaining the data. It is important to recall that TSP and DSP can produce different forecasts and can give rise to spurious regressions, therefore their detection in time series is a truly important task."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#augmented-dickey-fuller-test",
    "href": "posts/dickey_fuller/index.html#augmented-dickey-fuller-test",
    "title": "Augmented Dickey-Fuller test",
    "section": "Augmented Dickey-Fuller test",
    "text": "Augmented Dickey-Fuller test\nA distinction between stationary and nonstationary time series is made by formal statistical procedures such as the ADF (Augmented Dickey-Fuller) test, which is frequently used since it also accounts for serial correlation in the time series.\nThree specifications of the ADF test are based on the following regression equations.\n\nType 1 (unit root with “none”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{1}\\]\nwhere \\(\\epsilon_t\\) is white Gaussian noise, stationary with zero mean and constant variance. The statistical hypothesis test is:\n\\[\n(\\tau_1)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\tag{2}\\]\nThe null hypothesis prescribes the existence of a unit-root in the model. Rejection of the null implies that the original time series does not have a unit root.\n\n\nType 2 (unit root with “drift”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{a_0}_{\\text{drift}}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{3}\\]\nThe statistical hypothesis tests are:\n\\[\n\\begin{split}\n&(\\phi_1)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_0=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_0\\neq0\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\tau_2)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\end{split}\n\\tag{4}\\]\n\n\nType 3 (unit root with “drift and trend”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{a_0}_{\\text{drift}}+\\underbrace{a_2t}_{\\text{trend}}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{5}\\]\nThe statistical hypothesis tests are:\n\\[\n\\begin{split}\n&(\\phi_2)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_0=0\\;\\text{and}\\;a_2=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_0\\neq0\\;\\text{  or}\\;\\;\\,a_2\\neq0\\\\\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\phi_3)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_2=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_2\\neq0\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\tau_3)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\end{split}\n\\tag{6}\\]\nEach of the six tests \\((\\tau_1),(\\tau_2),(\\tau_3),(\\phi_1),(\\phi_2),(\\phi_3)\\) in Equation 2, Equation 4 and Equation 6 corresponds to a progressively more complex linear regression. In all of them there is the root, but in the “drift” model there is also a drift term, and in the “drift and trend” model there are also drift and trend terms. All the coefficients in the models have an associated significance level. While the significance of the root coefficient is the most important and the main focus of the ADF test, we might also be interested in knowing whether or not the drift and trend coefficients are statistically significant.\n\n\n\n\n\n\nSummary of the Dickey-Fuller tests\n\n\n\n\\[\n\\begin{split}\n&\\begin{split}\n&\\textbf{Trend}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+a_0+a_2t+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\phi_2)\\;\\text{is rejected, unit root is NOT present OR there is trend OR there is drift}\\\\\n&\\text{if}\\;(\\phi_2)\\;\\text{fails to be rejected, unit root is present AND there is NO trend AND there is NO drift}\\\\\n&\\text{if}\\;(\\phi_3)\\;\\text{is rejected, unit root is NOT present OR there is trend}\\\\\n&\\text{if}\\;(\\phi_3)\\;\\text{fails to be rejected, unit root is present AND there is NO trend}\\\\\n&\\text{if}\\;(\\tau_3)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_3)\\;\\text{fails to be rejected, unit root is present}\\\\\n&\\end{split}\n&\\end{split}\\\\\n&\\begin{split}\n&\\textbf{Drift}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+a_0+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\phi_1)\\;\\text{is rejected, unit root is NOT present OR there is drift}\\\\\n&\\text{if}\\;(\\phi_1)\\;\\text{fails to be rejected, unit root is present AND there is NO drift}\\\\\n&\\text{if}\\;(\\tau_2)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_2)\\;\\text{fails to be rejected, unit root is present}\\\\\n&\\end{split}\n&\\end{split}\\\\\n&\\begin{split}\n&\\textbf{None}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\tau_1)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_1)\\;\\text{fails to be rejected, unit root is present}\n&\\end{split}\n&\\end{split}\n\\end{split}\n\\]\n\n\nAn important extension of the ADF test concerns the case when the noise error term is not white. If the error term is not white and we run the ADF test as it is without accounting for serial correlation, many more rejection of the null tend to be produced than stated at the specified significance level. As the ADF test also deal with the serial correlation by introducing lagged terms, hence we need to select this lag order. This is accomplished by investigating several information criteria, including the autocorrelation function (ACF), but henceforth I limit to use the automatic lag selection functionality provided by the same function of R I will use for running the ADF test. Recall that, in general, the test statistics of the ADF test are very sensitive to changes in the lag structure."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#example",
    "href": "posts/dickey_fuller/index.html#example",
    "title": "Augmented Dickey-Fuller test",
    "section": "Example",
    "text": "Example\nI use functions and data from the urca package. The data set contains the series that S. Johansen and K. Juselius considered for estimating the money demand function of Denmark (Johansen and Juselius 1990). A data frame with quarterly data from Denmark starting in 1974:Q1 until 1987:Q3 contains six variables, including the log real income LRY, whose evolution, together with that of its first difference, is shown in Figure 1.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(urca)\nlibrary(zoo)\n\ndata(denmark)\nattach(denmark)\ndenmark &lt;- as.data.frame(denmark)\na &lt;- as.character(ENTRY)\nk &lt;- length(a)\nfor (i in 1:k) substring(a[i], first = 5, last = 5) = \"-\"\nENTRY &lt;- as.yearqtr(as.factor(a))\n\ndf_ts  &lt;- data.frame(x = ENTRY, y = LRY)\ndf_dts &lt;- data.frame(x = ENTRY[1:k-1], y = diff(LRY))\n\nmy_theme = theme(\n  axis.title.x = element_text(size = 16),\n  axis.text.x = element_text(size = 14),\n  axis.title.y = element_text(size = 16),\n  axis.text.y = element_text(size = 14),\n  legend.title = element_text(size = 14),\n  legend.text = element_text(size = 14),\n  panel.border = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  panel.background = element_blank(),\n  axis.line = element_line(colour = \"black\"))\n\np_ts &lt;- ggplot(df_ts, aes(x, y)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"\",\n       y = \"LRY\") + \n  my_theme\n\np_dts &lt;- ggplot(df_dts, aes(x, y)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"\",\n       y = \"first difference, LRY\") + \n  my_theme\n\ngrid.newpage()\ngrid.draw(rbind(ggplotGrob(p_ts), ggplotGrob(p_dts), size = \"last\"))\n\n\n\n\n\n\n\n\nFigure 1: Time series to be tested for stationarity. On the left, the natural logarithm of real income vs. time; on the right its first difference.\n\n\n\n\n\nThe function ur.df() from the urca package performs the ADF test, with three types of models and related tests, named “none” (i.e., Equation 1-Equation 2), “drift” (i.e., Equation 3-Equation 4) and “trend” (Equation 5-Equation 6). The argument selectlags in ur.df() allows to perform automatic selection of the lag structure according to a predefined criterion, as shown in the following code block.\n\n\nCode\nmdl_none_ts   &lt;- ur.df(y = df_ts$y, type = \"none\", selectlags = c(\"BIC\"))\nmdl_drift_ts  &lt;- ur.df(y = df_ts$y, type = \"drift\", selectlags = c(\"BIC\"))\nmdl_trend_ts  &lt;- ur.df(y = df_ts$y, type = \"trend\", selectlags = c(\"BIC\"))\nmdl_none_dts  &lt;- ur.df(y = df_dts$y, type = \"none\", selectlags = c(\"BIC\"))\nmdl_drift_dts &lt;- ur.df(y = df_dts$y, type = \"drift\", selectlags = c(\"BIC\"))\nmdl_trend_dts &lt;- ur.df(y = df_dts$y, type = \"trend\", selectlags = c(\"BIC\"))\nsummary(mdl_trend_ts)\n\n\nThe summary produced when the ADF test is applied to the original time series with the type “trend” is shown in Figure 2.\n\n\n\n\n\n\nFigure 2: Results of fitting the “trend” regression model in the ADF test when applied to the `LRY`variable from the `denmark` dataset.\n\n\n\nThe part of interest for the analysis is within the rectangle highlighted in orange. The “Value of test-statistic is -2.4216 2.1927 2.9343” for tests \\((\\tau_3), (\\phi_2)\\) and \\((\\phi_3)\\) are given and the corresponding “Critical values for test statistics” at significance levels 1%, 5% and 10% are reported below, denoted by tau3, phi2 and phi3, respectively. For instance, for the test \\((\\tau_3)\\), given that the test statistic -2.4216 is within the three regions -4.04, -3.45, -3.15 (1%, 5%, 10%) where we fail to reject the null, we do not have evidence to reject the presence of a unit root in the regression model: we can say that there is a unit root. From the \\((\\phi_2)\\)-statistic, the joint null hypothesis is not rejected, therefore there is a unit root AND drift and trend terms are not needed. Similarly, the \\((\\phi_3)\\)-statistic shows that there is a unit root AND a trend term is not needed and the \\((\\tau_3)\\)-statistic shows that there is a unit root. A summary of test results for the variable LRY is reported within the rectangle highlighted in blue in Figure 3.\n\n\n\n\n\n\nFigure 3: Results of the three type of ADF tests for the variable `LRY`from the `denmark`dataset.\n\n\n\nA close examination of Figure 3 shows that all the tests applied to data of the variable LRY are consistent with failing to reject the corresponding nulls. From the \\((\\phi_1)\\)-statistic, the joint null hypothesis is not rejected, therefore there is a unit root AND the drift term is not needed, and the \\((\\tau_2)\\)-statistic shows that there is a unit root. Finally, the \\((\\tau_1)\\)-statistic also shows that there is a unit root.\nOn the other hand, all the tests applied to the first difference of the variable LRY reject the corresponding nulls, as shown in the same Figure 3 (summary within the rectangle highlighted in red): although drift and trend terms might be presumed, this variable does not therefore contain a unit root.\n\nFinally, we can conclude that the logarithm of real income contains a unit root and can be a stationary time series by taking the first difference."
  },
  {
    "objectID": "posts/combinatorics/index.html",
    "href": "posts/combinatorics/index.html",
    "title": "Pills of combinatorics",
    "section": "",
    "text": "At least in the elementary case of finite sample spaces (i.e., a finite number of outcomes can be generated by our experiment), we often find useful to consider the intuitive notion that the outcomes are equally likely; this allows to introduce what is known as the classical definition of probability. For any set \\(A\\) made of some collection of outcomes from the sample space, we define the probability of \\(A\\) as:\n\\[\n\\text{Pr}(A)=\\frac{\\text{number of cases in}\\;A}{\\text{number of cases in the sample space}}\n\\]\nAfter the sample space has been defined and its size determined, we calculate probabilities of sets by counting the number of possible cases.\nThe art of counting is a relevant part of a field in mathematics known as combinatorics. In this post, I present the basic principle of counting and apply it to a number of situations that are often encountered in probabilistic models."
  },
  {
    "objectID": "posts/combinatorics/index.html#the-counting-principle",
    "href": "posts/combinatorics/index.html#the-counting-principle",
    "title": "Pills of combinatorics",
    "section": "The counting principle",
    "text": "The counting principle\nConsider an experiment that consists of two consecutive stages. The possible outcomes of the first stage are \\(a_1,\\cdots,a_n\\) and, for each outcome from the first stage, \\(b_1,\\cdots,b_m\\) outcomes are possible for the second stage. The outcomes of the two-stage experiment are thus the ordered pairs \\((a_i,b_j),i=1,...,m;j=1,\\cdots,m\\), whose number is \\(nm\\). An obvious generalization is for an \\(r\\)-stage experiment, with \\(n_i,i=1,2,\\cdots,r\\) outcomes each. The total number of outcomes is\n\\[\nN=\\prod_{i=1}^rn_i\n\\]\n\nExample 1 (Number of subsets of an \\(n\\)-element set.) Consider an \\(n\\)-element set \\(\\{a_1,a_2,\\cdots,a_n\\}\\). The construction of a subset can be seen as an \\(n\\)-stage process, where, at the \\(i\\)th stage, the \\(i\\)th element is offered the opportunity to be a member of the subset or not (binary choice). Therefore the number of subsets is \\(N=2^n\\), inclusive of either the empty set (no elements are in one subset) or the sample space (all the elements are in another subset).\n\nConsider now the situation where we have \\(n\\) distinct objects in a box and we want to form groups by sequentially selecting \\(k\\) objects from it, without repetitions being allowed (sampling without replacement) or with repetitions being allowed (sampling with replacement). In the former case, of course, we cannot select more objects than they are in the box, namely \\(k\\leq n\\). In the latter case, the restriction \\(k\\leq n\\) does not apply and, although unlikely, a group can even consist of \\(k\\) replicas of the same object.\nMoreover, the selection process can differ in regard to wthether we are interested in the order of selection or not. If the order of selection matters, two groups that are made by the same objects, each one with its own number of occurrences, are considered distinct, whereas, if the order of selection does not matter, they should count as one case only in the probability calculation.\nBased on these properties, four different arrangements of \\(k\\) out of a collection of \\(n\\) objects have to be considered, namely (without/with) repetition-order of selection (does/does not) matter. Each arrangement has its own name and related formula of counting, as outlined in the following."
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-without-replacement",
    "href": "posts/combinatorics/index.html#sampling-without-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling without replacement",
    "text": "Sampling without replacement\n\nOrder of selection matters\nThe number of arrangements, called permutations, can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at the second stage, the choices are reduced by one unit, i.e., \\(n-1\\); when we reach the \\(k\\)-stage, we are left with \\(n-k+1\\) choices. Applying the principle of counting, we have:\n\\[\nD(n,k)=n(n-1)\\cdots (n-k+1)=\\frac{n!}{(n-k)!}\n\\tag{1}\\]\nwhere the factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\), i.e., \\(n!=1\\cdot2\\cdots n\\). Recall that, when \\(k=n\\), \\(D(n,n)=n!\\) (\\(0!=1!=1\\)).\n\n\nOrder of selection does not matter\nThe number of arrangements, called combinations, can be calculated by noting that there exist \\(P(k,k)=k!\\) sequences of length \\(k\\) that differ from one another just in terms of the order of the presentation of their elements. They contribute only one arrangement to the total number of them:\n\\[\nC(n,k)=\\frac{D(n,k)}{D(k,k)}=\\frac{n!}{(n-k)!k!}={n\\choose k}\n\\tag{2}\\]\nwhere the binomial coefficient \\({n\\choose k}\\), indexed by the pair of integers \\(n\\geq k\\geq0\\), is considered.\n\nExample 2 (Newton’s binomial theorem) For any real number \\(n\\) that is not a non-negative integer, the theorem states that:\n\\[\n\\sum_{k=0}^n{n\\choose k}a^kb^{n-k}=(a+b)^n\n\\]\nwhen \\(a,b\\in\\mathbb{R}\\).\nSince \\({n\\choose k}\\) is the number of \\(k\\)-element sequences of a given \\(n\\)-element set, the sum over \\(k\\) of \\({n\\choose k}\\) counts the number of subsets of all possible sizes. Using the result of Example 1:\n\\[\n\\sum_{k=0}^{n}{n\\choose k}=2^n\n\\]\nThis result is also a simple application of the Newton’s binomial theorem for \\(a=b=1\\).\n\nRecall that a combination is a choice of \\(k\\) elements out of an \\(n\\)-element set without regard to order. This is the same as partitioning the set in two: one part contains \\(k\\) elements and the other contains the remaining \\(n−k\\). We can generalize by considering partitions in more than two subsets. We have \\(n\\) distinct objects and we are given non-negative integers \\(n_1,n_2,\\cdots,n_r\\), whose sum is equal to \\(n\\). The \\(n\\) items are to be divided into \\(r\\) disjoint groups, with the \\(i\\)th group containing exactly \\(n_i\\) items. The total number of groups is given by the multinomial coefficient:\n\\[\nN_r={n\\choose n_1n_2\\cdots n_r}=\\frac{n!}{n_1!n_2!\\cdots n_r!}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-with-replacement",
    "href": "posts/combinatorics/index.html#sampling-with-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling with replacement",
    "text": "Sampling with replacement\n\nOrder of selection matters\nThe number of arrangements, called dispositions can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at all the other stages, the choices are still \\(n\\). Applying the principle of counting, we have:\n\\[\nD^*(n,k)=n^k\n\\tag{3}\\]\n\n\nOrder of selection does not matter\nThe number of arrangements, called partitions is given by:\n\\[\nC^*(n,k)=\\frac{(n+k-1)!}{k!(n-1)!}={n+k-1\\choose k}\n\\tag{4}\\]\nTo understand the formula of Equation 4, think of writing \\(C^*(n,k)\\) when, for instance, \\(n=6,k=4\\) as, say, \\(a_1a_2a_3a_2\\) or equivalently \\(a_1*a_2**a_3*a_4\\), where any element \\(a_i,i=1,2,\\cdots,n\\) is followed by a number of asterisks equal to the number of its occurrences in the sequence (the total number of asterisks in the equivalent representation needs to be equal to \\(k\\)). Another example: \\(a_2a_3a_3a_3\\) and \\(a_1a_2*a_3***a_4\\). It is observed that a one-to-one correspondence exists between the original arrangements and all possible permutations in the alignment of elements and asterisks in the equivalent representation. Since each alignment starts with \\(a_1\\), we need to permute \\(n+k-1\\) elements, among which \\(k\\) (the asterisks) and \\(n-1\\) (the elements \\(a_i,i=2,\\cdots,n\\)) are equal."
  },
  {
    "objectID": "posts/combinatorics/index.html#formulae",
    "href": "posts/combinatorics/index.html#formulae",
    "title": "Pills of combinatorics",
    "section": "Formulae",
    "text": "Formulae\nIn conclusion, the four different arrangements of \\(k\\) out of a collection of \\(n\\) objects can be computed as follows:\n\\[\n\\begin{split}\n&\\quad\\quad\\textbf{without replacement}&\\quad\\quad\\textbf{with replacement}\\\\\n\\\\\n\\textbf{order matters}&\\quad\\quad\\text{permutations}&\\quad\\quad\\text{dispositions}\\\\\n&\\quad\\quad D(n,k)=\\dfrac{n!}{(n-k)!}&\\quad\\quad D^*(n,k)=n^k\\\\\n\\textbf{order does not matter}&\\quad\\quad\\text{combinations}&\\quad\\quad\\text{partitions}\\\\\n&\\quad\\quad C(n,k)={n\\choose k}&\\quad\\quad C^*(n,k)={n+k-1\\choose k}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#exercises",
    "href": "posts/combinatorics/index.html#exercises",
    "title": "Pills of combinatorics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Given a group of \\(n\\) individuals, count how many subgroups can be formed having one particular person as the leader, and a number (from 0 to \\(n-1\\)) of additional members.\nAnswer The counting principle can be applied as follows. First, we have \\(n\\) possible choices for the leader, and once the leader is chosen, \\(2^{n-1}\\) subsets can be considered, which may include from none to all the remaining individuals of the group. We have therefore:\n\\[\nN=n2^{n-1}\n\\]\n\n\nExercise 2 How many words that consists of four distinct letters can be formed?\nAnswer The sample space is composed of \\(n=26\\) elements. The sequences we are interested are formed by \\(k=4\\) elements, repetitions are not allowed. Two words are considered distinct based on the order of appearance of the four letters into them. The number of words is then given by Equation 1:\n\\[\nN=D(n,k)=\\frac{n!}{(n-k)!}=\\frac{26!}{22!}=26\\cdot25\\cdot24\\cdot23=358,800\n\\]\n\n\nExercise 3 How many combinations exist using two letters out of four letters?\nAnswer The sample is composed of \\(n=4\\) elements. The sequences we are interested are formed by \\(k=2\\) distinct elements, repetitions are not allowed. Two sequences are considered the same if they differ just by the order of the elements in them, namely \\(a_1a_2\\) is the same as \\(a_2a_1\\). The number of combinations is then given by Equation 2:\n\\[\nN=C(n,k)={n\\choose k}=\\frac{4!}{2!2!}=6\n\\]\n\n\nExercise 4 How many distinct combinations can be formed from the letters ATALANTA?\nAnswer There are \\(n=8\\) letters which may be arranged in \\(n!\\) ways, but this leads to double counting. If the \\(k_1=4\\) “A”s are permuted, then nothing is changed, and similarly for the \\(k_2=2\\) “T”s. The total number of distinct combinations is then given by the multinomial coefficient:\n\\[\nN=\\frac{n!}{k_1!k_2!}=\\frac{8!}{4!2!}=7\\cdot6\\cdot5\\cdot4=840\n\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html",
    "href": "posts/multivariate_probability_regions/index.html",
    "title": "Multivariate probability regions",
    "section": "",
    "text": "Consider the case of a multivariate random variable \\(\\mathbf{X}\\in\\mathbb{R}^d\\), whose distribution is normal with mean value \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{C}\\):\n\\[\n\\mathbf{C}=\\begin{bmatrix}\\sigma_1^2&\\cdots&\\sigma_{1d}\\\\\\vdots&\\ddots&\\vdots\\\\\\sigma_{d1}&\\cdots&\\sigma^2_d\\end{bmatrix}\n\\]\nfrom which we draw a sample of \\(n\\) data points:\n\\[\n\\mathbf{x}_i=\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}\n\\]\nWe suppose that either the mean value or the covariance matrix are unknown; they can be estimated using the sample mean, denoted by \\(\\bar{\\mathbf{x}}\\):\n\\[\n\\bar{\\mathbf{x}}=\\begin{bmatrix}\\bar{x}_1\\\\\\vdots\\\\\\bar{x}_d\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\n\\]\nand the sample covariance matrix, denoted by \\(\\mathbf{\\Sigma}\\):\n\\[\n\\mathbf{\\Sigma}=\\{\\,\\sigma_{jk},j,k=1,\\cdots d\\,\\}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n\\]\nwhose \\(jk\\)th element is written:\n\\[\n\\Sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)\n\\]\nThe \\(95\\%\\) prediction interval is a \\(d\\)-dimensional prediction hyperellipsoid, which, formally, is written as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{1}\\]\nwhere \\(\\mathbf{x}_{\\text{new}}\\) is the new observation for which the prediction interval is desired. \\(F_{0.95,n,n-d}\\) is the quantile with probability 0.95 of the Fisher’s \\(F\\)-distribution with \\(d\\) and \\((n-d)\\) degrees of freedom.\nEquation 1 is the formula for predicting the next single observation where we only have estimates of the mean value and the covariance matrix from the sample.\nAnother interesting formula can be elaborated as for the computation of the volume of the prediction hyperellipsoid. Recall that the prediction hyperellipsoid is the transformation of the hypersphere of radius\n\\[\nr=\\sqrt{F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{2}\\]\nby the linear transform of matrix \\(\\Sigma^{1/2}\\). Let \\(V\\) the volume of the hypersphere of radius \\(r\\) in an \\(d\\)-dimensional space. The volume of the hyperellipsoid can be obtained from \\(V\\) by multiplying with the determinant of the linear transform:\n\\[\nV_d=\\underbrace{\\frac{2}{d}\\frac{\\pi^{d/2}}{\\Gamma(d/2)}r^d}_{V}\\sqrt{\\det(\\Sigma)}\n\\tag{3}\\]\n\n\n\n\n\n\nUnivariate and bivariate random variables\n\n\n\nUnivariate random variable\nWhen \\(d=1\\), Equation 1 reads:\n\\[\n\\frac{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert}{s}\\leq\\sqrt{F_{0.95,1,n-1}^{\\phantom{'}}}\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{4}\\]\nwhere \\(s\\) is the sample standard deviation:\n\\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nIt is noted that\n\\[\nF_{0.95,1,n-1}=t_{0.975,n-1}^2\n\\tag{5}\\]\nwhere \\(t_{0.975,n-1}\\) is the quantile with probability 0.975 of the \\(t\\)-Student distribution with \\(n-1\\) degrees of freedom. Therefore Equation 4 can be written:\n\\[\n\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq t_{0.975,n-1}s\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{6}\\]\nFor large \\(n\\), when the central limit theorem can be applied, Equation 6 can be simplified:\n\\[\n\\boxed{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq z_{0.975}s}\n\\]\nwhere \\(z_{0.975}=1.96\\) is the quantile with probability 0.975 of the standard normal distribution.\nBivariate random variable\nWhen \\(d=2\\), Equation 1 reads:\n\\[\n(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq2F_{0.95,2,n-2}\\frac{(n-1)(n+1)}{n(n-2)}\n\\tag{7}\\]\nSince \\(2F_{0.95,2,\\infty}=\\chi^2(2)\\), for large \\(n\\) we can simplity Equation 7 as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq\\chi^2(0.95,2)}\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#prediction-hyperellipsoid",
    "href": "posts/multivariate_probability_regions/index.html#prediction-hyperellipsoid",
    "title": "Multivariate probability regions",
    "section": "",
    "text": "Consider the case of a multivariate random variable \\(\\mathbf{X}\\in\\mathbb{R}^d\\), whose distribution is normal with mean value \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{C}\\):\n\\[\n\\mathbf{C}=\\begin{bmatrix}\\sigma_1^2&\\cdots&\\sigma_{1d}\\\\\\vdots&\\ddots&\\vdots\\\\\\sigma_{d1}&\\cdots&\\sigma^2_d\\end{bmatrix}\n\\]\nfrom which we draw a sample of \\(n\\) data points:\n\\[\n\\mathbf{x}_i=\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}\n\\]\nWe suppose that either the mean value or the covariance matrix are unknown; they can be estimated using the sample mean, denoted by \\(\\bar{\\mathbf{x}}\\):\n\\[\n\\bar{\\mathbf{x}}=\\begin{bmatrix}\\bar{x}_1\\\\\\vdots\\\\\\bar{x}_d\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\n\\]\nand the sample covariance matrix, denoted by \\(\\mathbf{\\Sigma}\\):\n\\[\n\\mathbf{\\Sigma}=\\{\\,\\sigma_{jk},j,k=1,\\cdots d\\,\\}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n\\]\nwhose \\(jk\\)th element is written:\n\\[\n\\Sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)\n\\]\nThe \\(95\\%\\) prediction interval is a \\(d\\)-dimensional prediction hyperellipsoid, which, formally, is written as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{1}\\]\nwhere \\(\\mathbf{x}_{\\text{new}}\\) is the new observation for which the prediction interval is desired. \\(F_{0.95,n,n-d}\\) is the quantile with probability 0.95 of the Fisher’s \\(F\\)-distribution with \\(d\\) and \\((n-d)\\) degrees of freedom.\nEquation 1 is the formula for predicting the next single observation where we only have estimates of the mean value and the covariance matrix from the sample.\nAnother interesting formula can be elaborated as for the computation of the volume of the prediction hyperellipsoid. Recall that the prediction hyperellipsoid is the transformation of the hypersphere of radius\n\\[\nr=\\sqrt{F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{2}\\]\nby the linear transform of matrix \\(\\Sigma^{1/2}\\). Let \\(V\\) the volume of the hypersphere of radius \\(r\\) in an \\(d\\)-dimensional space. The volume of the hyperellipsoid can be obtained from \\(V\\) by multiplying with the determinant of the linear transform:\n\\[\nV_d=\\underbrace{\\frac{2}{d}\\frac{\\pi^{d/2}}{\\Gamma(d/2)}r^d}_{V}\\sqrt{\\det(\\Sigma)}\n\\tag{3}\\]\n\n\n\n\n\n\nUnivariate and bivariate random variables\n\n\n\nUnivariate random variable\nWhen \\(d=1\\), Equation 1 reads:\n\\[\n\\frac{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert}{s}\\leq\\sqrt{F_{0.95,1,n-1}^{\\phantom{'}}}\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{4}\\]\nwhere \\(s\\) is the sample standard deviation:\n\\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nIt is noted that\n\\[\nF_{0.95,1,n-1}=t_{0.975,n-1}^2\n\\tag{5}\\]\nwhere \\(t_{0.975,n-1}\\) is the quantile with probability 0.975 of the \\(t\\)-Student distribution with \\(n-1\\) degrees of freedom. Therefore Equation 4 can be written:\n\\[\n\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq t_{0.975,n-1}s\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{6}\\]\nFor large \\(n\\), when the central limit theorem can be applied, Equation 6 can be simplified:\n\\[\n\\boxed{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq z_{0.975}s}\n\\]\nwhere \\(z_{0.975}=1.96\\) is the quantile with probability 0.975 of the standard normal distribution.\nBivariate random variable\nWhen \\(d=2\\), Equation 1 reads:\n\\[\n(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq2F_{0.95,2,n-2}\\frac{(n-1)(n+1)}{n(n-2)}\n\\tag{7}\\]\nSince \\(2F_{0.95,2,\\infty}=\\chi^2(2)\\), for large \\(n\\) we can simplity Equation 7 as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq\\chi^2(0.95,2)}\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#concentration-ellipse",
    "href": "posts/multivariate_probability_regions/index.html#concentration-ellipse",
    "title": "Multivariate probability regions",
    "section": "Concentration ellipse",
    "text": "Concentration ellipse\nEquation 8 is the equation of the \\(95\\%\\) prediction ellipse for a bivariate normal distribution, whose sample mean value and sample covariance matrix are calculated from a large-size dataset. The ellipse is also called the concentration ellipse. Using Equation 5 the area \\(A=V_2\\) of the concentration ellipse can be computed from Equation 2-Equation 3 when \\(n\\) is large as follows:\n\\[\n\\boxed{A=\\pi\\sqrt{\\det(\\Sigma)}\\,\\chi^2_{0.95,2}}\n\\tag{9}\\]\n\nExample 1 (Simulation) The dataset is composed of 1000 points simulated from a bivariate normal distribution with mean value and covariance matrix:\n\\[\n\\mathbf{\\mu}=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\quad\\mathbf{C}=\\begin{bmatrix}5&2\\\\2&4\\end{bmatrix}\n\\]\nI used the function rmvnorm() from the package rvnorm for the simulation of the dataset, and the function stat_ellipse() of ggplot2 for drawing the ellipse, Figure 1. Using stat_ellipse() dispensed me from computing eigenvalues and eigenvectors of the sample covariance matrix, which are needed to evaluate the orientation and the eccentricity of the ellipse in the Cartesian plane. This computation can be based on a simple principal component analysis (PCA).\n\n\nCode\nlibrary(tidyverse)\nlibrary(mvtnorm)\n\nset.seed(1492)\nsigma &lt;- matrix(c(5, 2, 2, 4), ncol = 2)\nn &lt;- 1000\nx &lt;- rmvnorm(n = n, mean = c(1, 2), sigma = sigma)\n\ndist &lt;- qchisq(0.95, 2)                # to define the 95% probability contour\nm &lt;- matrix(data = c(0, 0), ncol = 1) \nfor (i in 1:2) m[i] &lt;- mean(x[, i])    # sample mean value\nC &lt;- cov(x)                            # sample covariance matrix\nS &lt;- solve(C)                          # inverse of C\nQ &lt;- numeric(n)                        \nfor (i in 1:n) Q[i] &lt;- t(x[i, ] - m) %*% S %*% (x[i, ] - m) # computation of the quadratic form\n\ndf &lt;- data.frame(x, Q)\ndf &lt;- df %&gt;%\n  mutate(inside = Q &lt;= dist)\naest &lt;- round(100*(1 - sum(!df$inside)/n), 2) # estimated coverage\narea &lt;- pi*dist*sqrt(det(C)) # area of the concentration ellipse \n\nmy_theme = theme(\n    axis.title.x = element_text(size = 16),\n    axis.text.x = element_text(size = 14),\n    axis.title.y = element_text(size = 16),\n    axis.text.y = element_text(size = 14),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14),\n    panel.border = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(colour = \"black\"),\n    aspect.ratio = 1,\n    legend.position = \"none\")\n\nggplot(df, aes(x = X1, y = X2, color = inside)) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  geom_point(size = 1) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", level = 0.95, \n               color = \"blue\", fill = \"blue\", alpha = 0.2) +\n  scale_x_continuous(limits = c(-10, 10)) + \n  scale_y_continuous(limits = c(-10, 10)) + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 1: The curve corresponding to the value \\(\\chi^2(0.95,2)=5.99\\) contains \\(95\\%\\) of the variable pairs; the points falling outside the concentration ellipse are plotted in red.\n\n\n\n\n\nThe coverage of the concentration ellipse is 94.9%. Moreover, the code calculates the area enclosed by the concentration ellipse, Equation 9: 74 (arbitrary units)."
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#an-application-to-posturography",
    "href": "posts/multivariate_probability_regions/index.html#an-application-to-posturography",
    "title": "Multivariate probability regions",
    "section": "An application to posturography",
    "text": "An application to posturography\nPostural control is often quantified by recording the trajectory of the center of pressure (CoP) during human quiet standing. This quantification has many important applications, including the assessment of balance disorders and the early detection of balance degradation to prevent falls in geriatric populations. The CoP trajectory in time, reported as the anteroposterior (AP) component (forward/backward) and the mediolateral (ML) component (left/right) paired together to form the so-called stabilogram, is typically acquired by means of a force platform, on top of which tested subjects are asked to stand in their upright posture for a while, typically 60 s, as still as possible. Manipulation of testing conditions, e.g., presence or absence of vision, different kind of surfaces for the feet to stay on are possible, so as to gain further insights about the postural control system performance.\nAmong the many CoP features that have been introduced in the literature, few of them have to do with the concentration ellipse of the CoP. E.g., the principal sway direction, to represent the relative contribution of the ML and AP components to the CoP fluctuations, in terms of the angle between the AP axis and the direction of the main eigenvector produced by the PCA. Another feature of interest, the one considered in this post, is the area of the concentration ellipse, also called the sway area (Schubert and Kirchner 2014). For instance, an increase in this feature value among elderly people has been associated with a higher risk of fall. I want to show here an example of calculation of the sway area using the code above, by taking an exemplary stabilogram from a recently published dataset (Santos et al. 2017), Figure 2.\n\n\nCode\nlibrary(ggpubr)\n\ndata &lt;- read.table(file = 'PDS01OR1grf.txt', header = TRUE, sep = \"\\t\")\nt &lt;- data$Time                  # 6000 samples acquired at a rate of 100 Hz\nCoP_AP &lt;- data$COPNET_X*1e2     # anteroposterior component (AP), cm\nCoP_ML &lt;- data$COPNET_Z*1e2     # mediolateral component (ML), cm\nCoP_AP &lt;- CoP_AP - mean(CoP_AP) # detrending\nCoP_ML &lt;- CoP_ML - mean(CoP_ML) # detrending\n\ndf &lt;- data.frame(time = t, x_ap = CoP_AP, x_ml = CoP_ML)\n\np_ap &lt;- ggplot(df, aes(x = time, y = x_ap)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"time, s\",\n       y = \"AP direction, cm\") + \n  scale_y_continuous(limits = c(-2, 2)) +\n  my_theme\n\np_ml &lt;- ggplot(df, aes(x = time, y = x_ml)) +\n  geom_line(color = \"red\") +\n  labs(x = \"time, s\",\n       y = \"ML direction, cm\") + \n  scale_y_continuous(limits = c(-2, 2)) +\n  my_theme\n\nggarrange(p_ap, p_ml)\n\n\n\n\n\n\n\n\nFigure 2: Anteroposterior and mediolateral components of the CoP for the subject with ID 1 from the Duarte’s dataset, tested Open-Eyes on a Rigid surface (first trial out of three).\n\n\n\n\n\nThe stabilogram with the concentration ellipse superimposed on it are shown in Figure 3.\n\n\nCode\nx &lt;- cbind(df$x_ml, df$x_ap) # detrending already done\nd &lt;- qchisq(0.95, 2)         # to define the 95% probability contour\nn &lt;- nrow(x)\nC &lt;- cov(x)                  # sample covariance matrix\nS &lt;- solve(C)                # inverse of C\nQ &lt;- numeric(n)                        \nfor (i in 1:n) Q[i] &lt;- t(x[i, ]) %*% S %*% x[i, ] # computation of the quadratic form\n\ndf_Q &lt;- data.frame(x, Q)\ndf_Q &lt;- df_Q %&gt;%\n  mutate(inside = Q &lt;= dist)\naest &lt;- round(100*(1 - sum(!df_Q$inside)/n), 2) # estimated coverage\narea &lt;- pi*dist*sqrt(det(C)) # area of the concentration ellipse \n\nggplot(df_Q, aes(x = X1, y = X2, color = inside)) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  geom_point(size = 0.5) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", level = 0.95, \n               color = \"blue\", fill = \"blue\", alpha = 0.2) +\n  labs(x = \"ML component, cm\",\n       y = \"AP component, cm\") +\n  scale_x_continuous(limits = c(-2, 2)) + \n  scale_y_continuous(limits = c(-2, 2)) + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 3: Exemplary stabilogram with superimposed the concentration ellipse - parts of the CoP trajectory outside the concentration ellipse are displayed in red.\n\n\n\n\n\nThe coverage of the concentration ellipse is 97%. The sway area is 3.09 cm\\(^2\\)."
  },
  {
    "objectID": "posts/gambler/index.html",
    "href": "posts/gambler/index.html",
    "title": "Gambler’s ruin",
    "section": "",
    "text": "The gambler’s ruin problem, first mentioned by Blaise Pascal in one of his letters to Pierre Fermat, was then reformulated by Christiaan Huygens, leading to important advances in the mathematical theory of probability. The statement of the problem considered here is slightly different from the one described by either Pascal or Huygens (Bertsekas and Tsitsiklis 2008).\n\nConsider a gambler who starts with an initial wealth of €\\(k\\) and then on each successive bet either wins €1 or loses €1 with probabilities \\(p\\) and \\(q=1−p\\) respectively. Different bets are assumed independent. The gambler’s objective is to reach a total wealth of €\\(n\\) (\\(n&gt;k\\)). If the gambler succeeds, then the gambler is said to win the game. The gambler stops playing after winning or getting ruined (running out of money), whichever happens first.\n\nCalculate the probability that the gambler wins. In particular, which is the probability of the gambler becoming infinitely rich?"
  },
  {
    "objectID": "posts/gambler/index.html#problem",
    "href": "posts/gambler/index.html#problem",
    "title": "Gambler’s ruin",
    "section": "",
    "text": "The gambler’s ruin problem, first mentioned by Blaise Pascal in one of his letters to Pierre Fermat, was then reformulated by Christiaan Huygens, leading to important advances in the mathematical theory of probability. The statement of the problem considered here is slightly different from the one described by either Pascal or Huygens (Bertsekas and Tsitsiklis 2008).\n\nConsider a gambler who starts with an initial wealth of €\\(k\\) and then on each successive bet either wins €1 or loses €1 with probabilities \\(p\\) and \\(q=1−p\\) respectively. Different bets are assumed independent. The gambler’s objective is to reach a total wealth of €\\(n\\) (\\(n&gt;k\\)). If the gambler succeeds, then the gambler is said to win the game. The gambler stops playing after winning or getting ruined (running out of money), whichever happens first.\n\nCalculate the probability that the gambler wins. In particular, which is the probability of the gambler becoming infinitely rich?"
  },
  {
    "objectID": "posts/gambler/index.html#solution",
    "href": "posts/gambler/index.html#solution",
    "title": "Gambler’s ruin",
    "section": "Solution",
    "text": "Solution\nLet us denote by \\(A\\) the event that the gambler ends up with €\\(n\\) and by \\(F\\) the event that the gambler wins the first bet. The probability of the event \\(A\\) given that the gambler starts with €\\(k\\) is written \\(w_k\\). The total probability law allows us to write:\n\\[\n\\begin{split}\nw_k&=P(A\\,\\vert\\,F)P(F)+P(A\\,\\vert\\,\\overline{F})P(\\overline{F})\\\\\n&=pP(A\\,\\vert\\,F)+(1-p)P(A\\,\\vert\\,\\overline{F})\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\overline{F}\\) is the complement of \\(F\\), namely the event \\(\\overline{F}\\) is that the gambler loses the first bet.\nNow, the event \\(A\\,\\vert\\,F\\) is the event \\(A\\) given that the gambler starts with €\\((k+1)\\), whose probability is \\(w_{k+1}\\) (because of the independence from the past). The event \\(A\\,\\vert\\,\\overline{F}\\) is the event \\(A\\) given that the gambler starts with €\\((k-1)\\), whose probability is \\(w_{k-1}\\) (because of the independence from the past). Equation 1 reads:\n\\[\nw_k=p\\,w_{k+1}+(1-p)\\,w_{k-1}\n\\tag{2}\\]\nIf we define \\(r=(1-p)/p\\), we can also write:\n\\[\n\\left\\{\n\\begin{split}\n&w_{k+1}-w_k=r(w_k-w_{k-1}),\\;0&lt;k&lt;n\\\\\n&w_0=0\\\\\n&w_n=1\n\\end{split}\n\\right.\n\\tag{3}\\]\nSince \\(w_{k+1}=w_k+r^k(w_1-w_0)=w_k+r^kw_1=w_1\\sum_{i=0}^kr^i\\), we can calculate \\(w_k\\) as follows:\n\\[\nw_k=\\left\\{\n\\begin{split}\n&\\frac{1-r^k}{1-r}w_1&\\quad\\text{if}\\;r\\neq1\\\\\n&kw_1&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.\n\\tag{4}\\]\nWe also know that \\(w_n=1\\). Therefore, we can solve Equation 4 for \\(w_1\\):\n\\[\nw_1=\\left\\{\n\\begin{split}\n&\\frac{1-r}{1-r^n}&\\quad\\text{if}\\;r\\neq1\\\\\n&\\frac{1}{n}&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.\n\\]\nPlugging this expression of \\(w_1\\) into Equation 4, we obtain:\n\\[\n\\boxed{w_k=\\left\\{\n\\begin{split}\n&\\frac{1-r^k}{1-r^n}&\\quad\\text{if}\\;r\\neq1\\\\\n&\\frac{k}{n}&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.}\n\\tag{5}\\]\nThe probability that the gambler becomes infinitely rich (event \\(R\\)) is written:\n\\[\n\\left\\{\n\\begin{split}\n\\text{Pr}(R)&=\\lim_{n\\rightarrow\\infty}w_k=1-\\left(\\frac{1-p}{p}\\right)^k&gt;0,&\\quad p&gt;1/2\\;(r&lt;1)\\\\\n\\text{Pr}(R)&=\\lim_{n\\rightarrow\\infty}w_k=0,&\\quad p\\leq1/2\\;(r\\geq1)\\\\\n\\end{split}\n\\right.\n\\tag{6}\\]\nTo interpret the meaning of Equation 6, suppose that the gambler starting with an initial wealth of €\\(k\\) wishes to continue gambling, with the intention of earning as much money as possible. So there is no winning value €\\(n\\): the gambler will only stop if ruined. If \\(p&gt;1/2\\) (each gamble is in favor of the gambler), then there is a positive probability that the gambler will never get ruined but instead will become infinitely rich. Conversely, if \\(p\\leq0.5\\) (each gamble is not in favor of the gambler), then the gambler will get ruined with probability one."
  },
  {
    "objectID": "posts/gambler/index.html#markov-chain-model",
    "href": "posts/gambler/index.html#markov-chain-model",
    "title": "Gambler’s ruin",
    "section": "Markov chain model",
    "text": "Markov chain model\nThe discrete-state discrete-time Markov chain show in Figure 1 can be proposed to model the gambler process (Bertsekas and Tsitsiklis 2008). The \\(n+1\\) states, numbered from 0 to \\(n\\), represent the gambler’s wealth at the start of each bet. The first state 0 corresponds to ruin (getting out of money), the last state \\(n\\) corresponds to win. Given the rules of the game, the states 0 and \\(n\\) are absorbing, in the sense that it is not possible to escape from each of them anymore, once they are entered. All the other states are transient.\n\n\n\n\n\n\nFigure 1: Transition probability graph for the gambler’s ruin problem.\n\n\n\nThe problem is then to find the probabilities of absorption at each one of the two absorbing states. These absorption probabilities depend on the initial state \\(k\\;(0&lt;k&lt;n)\\). It is worthy noting that the memoryless character of the process implies that, if the gambler happens to revisit the initial state \\(k\\) after a while, the absorption probabilities are the same they were initially.\n\n\n\n\n\n\nTransition probability matrix\n\n\n\nThe state of a discrete-state discrete-time Markov chain is denoted by \\(X_t\\), that can change at certain discrete time instants \\(n\\). The state space of the chain is composed of a finite set \\(\\mathscr{S}=\\{1,\\cdots,m\\}\\). The Markov chain is described in terms of its transition probability \\(p_{ij}\\):\n\\[\np_{ij}=P(X_{t+1}=j\\,\\vert\\,X_t=i),\\quad i,j\\in\\mathscr{S}\n\\tag{7}\\]\nThe key assumption underlying Markov chains is that the Markov property holds:\n\\[\nP(X_{t+1}=j\\,\\vert\\,X_t=i,X_{t-1}=i_{t-1},\\cdots,X_0=i_0)=P(X_{t+1}=j \\,\\vert\\,X_t=i)=p_{ij}\n\\tag{8}\\]\nfor all times \\(t\\), all states \\(i,j\\in\\mathscr{S}\\) and all possible sequences \\(i_0,i_1,\\cdots,i_{t-1}\\) of earlier states. Thus, the probability law of the next state \\(X_{t+1}\\) depends on the past only through the value of the present state \\(X_t\\).\nAll of the elements of a Markov chain model can be encoded in a transition probability matrix, which is simply a two-dimensional array whose elements at the \\(i\\)th row and \\(j\\)th column is \\(P_{ij}\\):\n\\[\n\\begin{bmatrix}\np_{11}&p_{12}&\\cdots&p_{1m}\\\\\np_{21}&p_{22}&\\cdots&p_{2m}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\np_{m1}&p_{m2}&\\cdots&p_{mm}\n\\end{bmatrix}\n\\tag{9}\\]\nConsider a Markov chain in which each state is either transient or absorbing. We fix a particular absorbing state \\(s\\). It is possible to show that the probabilities \\(w_k\\) of eventually reaching state \\(s\\), starting from \\(k\\), are the unique solution of the equations \\(w_s=1, w_i=0\\), for all absorbing \\(i\\neq s\\), and for all transient \\(i\\):\n\\[\nw_i=\\sum_{j=1}^mp_{ij}w_j\n\\tag{10}\\]\n\n\n\nExample 1 (Gambler’s discrete-time Markov chain) For \\(n=4,p,q=1-p\\) the transition probability matrix of the gambler’s discrete-time Markov chain is given by:\n\\[\n\\begin{bmatrix}\n1&0&0&0&0\\\\\nq&0&p&0&0\\\\\n0&q&0&p&0\\\\\n0&0&q&0&p\\\\\n0&0&0&0&1\n\\end{bmatrix}\n\\]\nIn the case of the gambler’s ruin problem, Equation 10 can be reformulated as follows:\n\\[\nw_k=(1-p)\\,w_{k-1}+p\\,w_{k+1}\n\\]\nwith \\(w_0=0,w_n=1\\), leading to Equation 2."
  },
  {
    "objectID": "posts/gambler/index.html#simulation",
    "href": "posts/gambler/index.html#simulation",
    "title": "Gambler’s ruin",
    "section": "Simulation",
    "text": "Simulation\nThe following code written in R simulates the gambler process (adapted from (Dobrow 2016)).\n\n\nCode\nset.seed(2001)\ngambler &lt;- function(k, n, p) {\n  stake &lt;- k\n    while (stake &gt; 0 & stake &lt; n) {\n        bet &lt;- sample(c(-1, 1), 1, prob = c(1-p, p))\n        stake &lt;- stake + bet\n    }\n    if (stake == n) return(1) else return(0)\n}   \nk &lt;- 15         # initial wealth\nn &lt;- 120        # final wealth\np &lt;- 0.5        # probability of win at each bet\nntrials &lt;- 1000 # number of independent trials\n\nsim &lt;- replicate(ntrials, gambler(k, n, p))\np_ruin &lt;- 1 - sum(sim)/ntrials \n\n\nThe probability of ruin estimated over 1000 iterations of the gambler process is 0.888, in close agreement with the value 0.875 computed from Equation 5.\nIt is noted that, at each time step, the gambler appears to move randomly, either to the left or to the right by a fixed unit distance with probability \\(p\\) and \\(1-p\\), respectively. The gambler undergoes a simple random walk in one dimension, whose state space is the set of the integers \\(\\mathbb{Z}\\). The limitations to values of \\(\\mathbb{Z}\\) between 0 (ruin) and \\(n\\) (win) are the consequence of the gambling rules: these rules settle two absorbing barriers to the motion of the random walker. Nine exemplary sample paths of the random walk are shown in Figure 2, for the case that \\(k=30,n=120,p=0.5\\).\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\n\nset.seed(2001)\nk &lt;- 30       # initial wealth\nn &lt;- 120      # final wealth\np &lt;- 0.5      # probability of win at each bet\nntrials &lt;- 9  # number of independent trials\nnstep &lt;- 1000 # number of steps\ntraj &lt;- matrix(NA, nrow = nstep, ncol = ntrials)\nfor (i in 1:ntrials) {\n  traj[1, i] &lt;- k\n  jmax &lt;- nstep\n  for (j in 2:nstep) {\n          bet &lt;- sample(c(-1, 1), 1, prob = c(1-p, p))\n          traj[j, i] &lt;- traj[j-1, i] + bet\n          if (traj[j, i] == 0 | traj[j, i] == n) {\n            break\n          }\n  }\n}   \nt &lt;- rep(seq(0, nstep-1, by = 1), ntrials)\npath &lt;- rep(c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"), each = nstep)\ntraj &lt;- c(traj)\ndf &lt;- data.frame(t = t, path = path, data = traj)\n\nmy_theme = theme(\n    axis.title.x = element_text(size = 14),\n    axis.text.x = element_text(size = 12),\n    axis.title.y = element_text(size = 14),\n    axis.text.y = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    panel.border = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    panel.spacing = unit(1, \"lines\"),\n    axis.line = element_line(colour = \"black\"))\n\nggplot(df, aes(x = t, y = data, path = path)) + \n  geom_line() +\n  labs(x = \"bets\",\n       y = \"gambler's wealth\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  geom_hline(yintercept = 120, linetype = \"dashed\") +\n  scale_y_continuous(breaks = c(0, 30, 60, 90, 120)) +\n  facet_wrap(~path, labeller = \"label_both\") + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 2: Gambler’s random walk: nine sample paths. The outcome of up to a maximum of 1000 bets is considered here."
  },
  {
    "objectID": "posts/probability_estimation/index.html",
    "href": "posts/probability_estimation/index.html",
    "title": "Confidence intervals for proportions",
    "section": "",
    "text": "Consider a Bernoulli test with \\(n\\) independent trials and \\(x\\) successes that are recorded in the test. The proportion \\(f=x/n\\) is assumed to be the estimate of the true probability \\(p\\); we want to determine the values \\(p_1\\) and \\(p_2\\) of the confidence interval (CI) with the required confidence level associated to \\(f\\), namely \\(CL=100(1-\\alpha)\\%\\).\nThe Clopper-Pearson equations can be used to calculate the CI:\n\\[\n\\left\\{\n\\begin{split}\n\\sum_{k=x}^{\\infty}{n\\choose k}p_1^k(1-p_1)^{n-k}&=c_1\\\\\n\\sum_{k=0}^x{n\\choose k}p_2^k(1-p_2)^{n-k}&=c_2\n\\end{split}\n\\right.\n\\tag{1}\\]\nwhere \\(CL=1-c_1-c_2\\). One common choice for \\(c_1\\) and \\(c_2\\) is the symmetric interval, \\(c_1=c_2=(1-CL)/2\\). For instance, when \\(CL=95\\%\\), \\(\\alpha=0.05\\) and \\(c_1=c_2=2.5\\%\\). The Clopper-Pearson interval is also called the exact interval, since the solution of Equation 1 with respect to \\(p_1\\) and \\(p_2\\) gives the correct probability estimate, even for small samples.\nIt is noted that the binomial model has been used in Equation 1 to capture events of the type “number of successes out of a given number of independent trials”, namely:\n\\[\n\\text{Pr}(k\\;\\text{successes out of}\\;n\\;\\text{trials})={n\\choose k}p^k(1-p)^{n-k}\n\\]\nwhere \\(0\\leq k\\leq n\\) and \\({n\\choose k}\\) is the number of combinations of \\(k\\)-length sequences from an \\(n\\)-length sequence (I discussed this in a previous post of mine).\nWe recall that a binomial random variable \\(X\\) with parameters \\(n\\) (the number of independent trials) and \\(p\\) (the probability of success) has mean value and standard deviation:\n\\[\n\\left\\{\n\\begin{split}\n\\mu_X&=np\\\\\n\\sigma_X&=\\sqrt{np(1-p)}\n\\end{split}\n\\right.\n\\]\nTherefore the random variable \\(F=X/n\\) has mean value and standard deviation:\n\\[\n\\left\\{\n\\begin{split}\n\\mu_F&=p\\\\\n\\sigma_F&=\\frac{\\sigma_X}{\\sqrt{n}}=\\sqrt{\\frac{p(1-p)}{n}}\n\\end{split}\n\\right.\n\\]\nThe normal approximation to the binomial distribution, as stated by the De Moivre-Laplace theorem, is considered as an alternative to Equation 1 for CI construction. This theorem states that the random variable\n\\[\nT=\\frac{F-p}{\\sigma_F}\n\\]\nis approximately normally distributed - in practice, good approximations are achieved for \\(np,n(1-p)\\gg1\\).\nGiven a variate \\(t\\) of \\(T\\), the CI can be written as follows:\n\\[\n\\frac{\\vert\\,f-p\\,\\vert}{\\sqrt{\\dfrac{p(1-p)}{n}}}\\leq t_{1-\\alpha/2}\n\\tag{2}\\]\nwhere \\(t_{1-\\alpha/2}\\) is the quantile of the normal distribution at specified probability \\(1-\\alpha/2\\): for \\(\\alpha=0.05\\), \\(t_{1-\\alpha/2}=1.96\\).\nSquaring Equation 2 we get:\n\\[\nn(f-p)^2\\leq t_{1-\\alpha/2}^2\\,p(1-p)\n\\]\nWe can solve the resulting second-order equation with respect to the unknown \\(p\\), yielding the Wilson interval:\n\\[\np\\in\\dfrac{f+\\dfrac{t^2_{1-\\alpha/2}}{2n}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\\pm\\dfrac{t_{1-\\alpha/2}\\sqrt{\\dfrac{t^2_{1-\\alpha/2}}{4n^2}+\\dfrac{f(1-f)}{n}}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\n\\tag{3}\\]\nIt is noted that the Wilson interval is not centered on the measured proportion \\(f\\) but on the value \\((f+t_{1-\\alpha/2}^2/2n)/(t_{1-\\alpha/2}^2/n+1)\\), which is a function of \\(f\\) and the number of trials \\(n\\). This is due to the asymmetry of the binomial distribution for small \\(n\\).\n\n\n\n\n\n\nContinuity correction\n\n\n\nThe accuracy of Equation 3 can be improved by applying the continuity correction to the proportion \\(f=x/n\\). This correction improves the accuracy of CI construction for discrete variables when they can be approximated by normal variables:\n\\[\nf_{\\pm}=\\frac{x\\pm0.5}{n}\\rightarrow\\left\\{\\begin{split}f_{+}&=\\min\\left(1,\\dfrac{x+0.5}{n}\\right)\\\\f_{-}&=\\max\\left(0,\\dfrac{x-0.5}{n}\\right)\\end{split}\\right.\n\\]\nThe interval is estimated as follows:\n\\[\np\\in[\\max(0,p_{-}),\\min(1,p_{+})]\n\\]\nwhere:\n\\[\np_{\\pm}\\in\\dfrac{f_{\\pm}+\\dfrac{t^2_{1-\\alpha/2}}{2n}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\\pm\\dfrac{t_{1-\\alpha/2}\\sqrt{\\dfrac{t^2_{1-\\alpha/2}}{4n^2}+\\dfrac{f_{\\pm}(1-f_{\\pm})}{n}}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\n\\]\n\n\nThe Wilson interval (with continuity correction) is generally considered valid for \\(n&gt;10\\). For \\(n\\gg1\\) (namely, \\(n&gt;30\\)), a very popular approximate formulation of the CI is the Wald interval:\n\\[\np\\in f\\pm t_{1-\\alpha/2}\\sqrt{\\dfrac{f(1-f)}{n}}\n\\tag{4}\\]\nThe Wald interval, which does not require any continuity correction, is symmetric around the estimated proportion. For \\(n\\gg1\\) it is noted indeed that the Wilson interval tends to become symmetric and identical to the Wald interval.\nThe question is when \\(n\\) is large enough for these methods to produce accurate inference. The R code below is a fully reproducible code to generate coverage plots for the Wilson interval (with continuity correction) and the Wald interval.\n\n\nCode\nset.seed(1789)\nn  &lt;- 30                          # number of Bernoulli trials\nL  &lt;- 1000                        # number of simulated sequences \na  &lt;- 95                          # confidence level\nt  &lt;- qnorm((1+a/100)/2)          # quantile of the normal distribution\np  &lt;- seq(0.05, 0.95, by = 0.01)  # tested values of probability\nnp &lt;- length(p)                   # number of tested values of probability\nc_wald   &lt;- numeric(np)           # Wald approach\nc_wilson &lt;- numeric(np)           # Wilson approach\n\nfor(i in 1:np) {\n  for(j in 1:L) {\n    x &lt;- rbinom(1, size = n, prob = p[i])  # number of successes\n    f &lt;- x/n                               # proportion of successes\n    fl &lt;- max(0, (x-0.5)/n) # continuity correction\n    fu &lt;- min(1, (x+0.5)/n) \n    CIl &lt;- (fl+t^2/(2*n))/(1+t^2/n) - t*sqrt(fl*(1-fl)/n + t^2/(4*n^2))/(1+t^2/n)\n    CIu &lt;- (fu+t^2/(2*n))/(1+t^2/n) + t*sqrt(fu*(1-fu)/n + t^2/(4*n^2))/(1+t^2/n)\n    CI_lower_wilson &lt;- max(0.0, CIl) # Wilson interval\n    CI_upper_wilson &lt;- min(1.0, CIu)\n    CI_lower_wald &lt;- f - t*sqrt(f*(1-f)/n) # Wald interval\n    CI_upper_wald &lt;- f + t*sqrt(f*(1-f)/n)\n    if(CI_lower_wilson &lt;= p[i] && CI_upper_wilson &gt;= p[i]) c_wilson[i] = c_wilson[i] + 1\n    if(CI_lower_wald &lt;= p[i] && CI_upper_wald &gt;= p[i]) c_wald[i] = c_wald[i] + 1\n  }\n}\nc_wilson &lt;- 100*c_wilson/L # estimated coverage\nc_wald   &lt;- 100*c_wald/L   \n\n\n\n\n\n\n\n\n\n\nFigure 1: Coverage curves for the binomial distribution with \\(n=30\\).\n\n\n\n\n\nThe simulation results of Figure 1 (\\(n=30\\)) show that, whereas the Wilson method tends just to provide some limited over-coverage for all values of \\(p\\) (this is simply due to \\(x\\) being present in both the sums of Equation 1), the Wald interval performs poorly, with severe under-coverage, especially for low and high values of \\(p\\). These results seem to raise objections against the use of the Wald interval, even in situations when \\(n\\gg1\\): Figure 2 shows the coverage plots in the case of \\(n=100\\).\n\n\n\n\n\n\n\n\nFigure 2: Coverage curves for the binomial distribution with \\(n=100\\).\n\n\n\n\n\nAlthough the performance of the Wald method has slightly improved, the Wilson method still outperforms it, especially for small and high values of \\(p\\).\n\nTo conclude, when estimating the confidence interval associated to the estimate of proportions, the Wald interval - the most basic confidence interval usually considered by experimentalists - appears to be seriously flawed, also for values of the number of trials that are considered large in most statistical textbooks. The Wilson interval with continuity correction tends to perform much better and its use should be encouraged."
  },
  {
    "objectID": "posts/numerical_simulation/index.html",
    "href": "posts/numerical_simulation/index.html",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "",
    "text": "We consider here the class of stochastic processes known as diffusion processes, which are solutions to stochastic differential equation (SDE) of the form:\n\\[\ndX(t)=b(t,X(t))\\,dt+\\sigma(t,X(t))\\,dW(t)\n\\tag{1}\\]\nwith some initial condition \\(X(0)\\) which can be regarded as being either constant or random. The SDE is interpreted in the Itô sense:\n\\[\nX(t)=X(0)+\\int_0^tb(s,X(s))\\,ds+\\int_0^t\\sigma(s,X(s))\\,dW(s)\n\\tag{2}\\]\nand \\(\\{W(t),t\\in[0,T]\\}\\) is the so-called Brownian motion or Wiener process.\nThe two deterministic functions \\(b(\\cdot,\\cdot)\\) and \\(\\sigma(\\cdot,\\cdot)\\) are called, respectively, the drift and the diffusion coefficients of the SDE. Under a number of assumptions regarding their properties (i.e., global Lipschitz and linear growth, see (Iacus 2008) for mathematical details) the SDE has a unique solution with continuous sample paths such that\n\\[\nE\\left[\\int_0^T\\vert\\,X(t)\\,\\vert^2\\,dt\\right]&lt;\\infty\n\\tag{3}\\]\nFor the sake of simpler notation, \\(X(t)\\rightarrow X_t\\) is used in the following, therefore Equation 1 and Equation 2 can be written:\n\\[\n\\left\\{\n\\begin{split}\ndX_t&=b(t,X_t)\\,dt+\\sigma(t,X_t)\\,dW_t\\\\\nX_t&=X_0+\\int_0^tb(s,X_s)\\,ds+\\int_0^t\\sigma(s,X_s)\\,dW_s\n\\end{split}\n\\right.\n\\tag{4}\\]\nThe Itô formula is an important tool of stochastic calculus. It can be regarded as the stochastic version of the Taylor expansion of a function \\(f(t,X_t)\\) stopped at the second order, where \\(X_t\\) is a diffusion process. If the function \\(f(\\cdot,\\cdot)\\) is a twice differentiable function on both arguments, we have:\n\\[\ndf(t,X_t)=f_t(t,X_t)\\,dt+f_x(t,X_t)\\,dX_t+\\frac{1}{2}f_{xx}(t,X_t)(dX_t)^2\n\\tag{5}\\]\nwhere:\n\\[\nf_t(t,x)=\\dfrac{\\partial}{\\partial t}f(t,x),\\quad f_x(t,x)=\\dfrac{\\partial f(t,x)}{\\partial x},\\quad f_{xx}(t,x)=\\dfrac{\\partial^2 f(t,x)}{\\partial x^2}\n\\]\nUsing Equation 1 and the Itô lemma, which states that \\((dW_t)^2=dt\\), we have:\n\\[\n(dX_t)^2=\\left[b(t,X_t)\\,dt+\\sigma(t,X_t)\\,dW_t\\right]^2=\\sigma^2(t,X_t)\\,dt+\\text{O}(dt^{3/2})\n\\tag{6}\\]\nEquation 5 can thus be written:\n\\[\ndf(t,X_t)=\\left[f_t(t,X_t)+f_x(t,X_t)b(t,X_t)+\\frac{1}{2}f_{xx}(t,X_t)\\right]dt+f_x(t,X_t)\\sigma(t,X_t)\\,dW_t\n\\tag{7}\\]"
  },
  {
    "objectID": "posts/numerical_simulation/index.html#approximation-methods",
    "href": "posts/numerical_simulation/index.html#approximation-methods",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "Approximation methods",
    "text": "Approximation methods\nTo apply a numerical method to the generic SDE of Equation 1 over the time interval \\([0,T]\\), the time interval needs to be discretized (Higham 2001). Let \\(\\Delta t=T/M\\) for some positive integer \\(M\\), and \\(t_n=n\\Delta t\\). The numerical approximation to \\(X(t_n)\\) will be denoted \\(X_n\\) (\\(n=1,\\cdots,M\\)).\nThe Euler-Maruyama method takes the form:\n\\[\n\\left\\{\n\\begin{split}\nX_n&=X_{n-1}+b(X_{n-1})\\Delta t+\\sigma(X_{n-1})\\Delta W_{n-1}\\\\\nX_0&=x_0\n\\end{split}\n\\right.\n\\tag{8}\\]\nIt is worth noting that in the case when \\(\\sigma(X_t)=0\\), then the problem becomes deterministic: this approximation method reduces to the standard Euler method for the ordinary differential equation \\(\\dot{X}_t=b(t,X_t)\\).\nThe Milstein’s method adds a correction to the stochastic increment to produce the solution to the SDE. The correction arises because the Taylor expansion must be modified in the case of Itô calculus. Truncating the Taylor expansion to the second order at an appropriate point yields:\n\\[\n\\left\\{\n\\begin{split}\nX_n&=X_{n-1}+b(t_{n-1},X_{n-1})\\Delta t+\\sigma(t_{n-1},X_{n-1})\\Delta W_{n-1}\\\\\n&+\\dfrac{1}{2}\\sigma(X_{n-1})\\sigma_x(X_{n-1})\\left[(\\Delta W_{n-1})^2-\\Delta t\\right]\\\\\nX_0&=x_0\n\\end{split}\n\\right.\n\\tag{9}\\]\n\n\n\n\n\n\nStrong and weak convergence\n\n\n\nThe methods of approximation of the continuous solution to an SDE are classified according to their different properties. Mainly two criteria of optimality are used in the literature: the strong and the weak (orders of) convergence.\nA time-discretized approximation \\(X_\\delta\\) of a continuous-time process \\(X\\), with \\(\\Delta t\\) the time increment of the discretization, is said to be of strong order of convergence \\(\\gamma\\) to \\(X\\) if for any fixed time horizon \\(T\\) it holds true that\n\\[\nE[\\,\\vert X_{\\Delta t}(\\tau)-X(\\tau)\\vert\\,]\\leq C\\Delta t^\\gamma,\\quad\\forall\\Delta t&lt;\\Delta t_0\n\\tag{10}\\]\nfor any \\(\\tau\\in[0,T]\\), with \\(\\Delta t_0&gt;0\\) and \\(C\\) a constant that does not depend on \\(\\Delta t\\).\nAlong with the strong convergence, the weak convergence can also be defined. In the same conditions as above, \\(X_{\\Delta t}\\) is said to converge weakly of order \\(\\gamma\\) to \\(X\\) if for any fixed horizon \\(T\\) and any \\(2(\\beta+1)\\) continuous differentiable function \\(g\\) of polynomial growth, it holds true that\n\\[\n\\vert\\,E_g[X_{\\Delta t}(\\tau)]-E_g[X(\\tau)]\\,\\vert\\leq C\\Delta t^\\beta\n\\tag{11}\\]\nfor any \\(\\tau\\in[0,T]\\), with \\(\\Delta t_0&gt;0\\) and \\(C\\) a constant that does not depend on \\(\\Delta t\\).\nWhereas the strong order of convergence measures the rate at which the mean of the error decays as \\(\\Delta t\\rightarrow0\\), the weak order of convergence measures the rate of decay of the error of the means.\nMethods of approximation of some order that strongly converge usually have a higher order of weak convergence. This is the case with the Euler-Maruyama method, which is strongly convergent of order \\(\\gamma=1/2\\) and weakly convergent of order \\(\\beta=1\\) (under some smoothness conditions on the coefficients of the SDE). The Milstein’s method has both weak and strong order of convergence, \\(\\gamma=\\beta=1\\), which is superior to the Euler–Maruyama method.\nIt is interesting noting that, when the diffusion coefficient does not depend on the state variable of the process, the Euler-Maruyama and Milstein’s methods coincide. This is one important case, therefore, when the Euler-Maruyama method is strongly convergent of order \\(\\gamma=1\\)."
  },
  {
    "objectID": "posts/numerical_simulation/index.html#numerical-test",
    "href": "posts/numerical_simulation/index.html#numerical-test",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "Numerical test",
    "text": "Numerical test\n\n\n\n\n\n\nGeometric Brownian motion\n\n\n\nThe geometric Brownian motion (GBM) is the solution to the SDE:\n\\[\n\\left\\{\n\\begin{split}\ndX_t&=\\lambda\\,X_t\\,dt+\\mu\\,X_t\\,dW_t\\\\\nX_0&=x_0\n\\end{split}\n\\right.\n\\tag{12}\\]\nwhere the parameter \\(\\lambda\\) is interpreted as the constant interest rate and \\(\\mu&gt;0\\) as the volatility. For a GBM, the drift and diffusion coefficients are both linearly related to the state variable \\(X\\), namely \\(b(t,X_t)=\\lambda\\,X_t\\) and \\(\\sigma(X_t)=\\mu\\,X_t\\).\nThe explicit solution of the SDE Equation 12 can be found by operating the following change of variable:\n\\[\n\\begin{split}\nY_t=\\log X_t\\quad\\rightarrow\\quad dY_t&=\\frac{1}{X_t}\\,dX_t-\\frac{1}{2X_t^2}\\,(dX_t)^2\\\\\n&=\\left(\\lambda-\\frac{1}{2}\\mu^2\\right)dt+\\mu\\,dW_t\n\\end{split}\n\\tag{13}\\]\nWe obtain:\n\\[\nX_t=x_0\\exp\\left[\\left(\\lambda-\\frac{1}{2}\\mu^2\\right)t+\\mu\\,W_t\\right]\n\\tag{14}\\]\n\n\nFor the GBM, the following stochastic difference equations are obtained for the Euler-Maruyama method:\n\\[\n\\left\\{\n\\begin{split}\nX^E_n&=X^E_{n-1}(1+\\lambda\\Delta t+\\mu\\Delta W_{n-1})\\\\\nX^E_0&=x_0\n\\end{split}\n\\right.\n\\tag{15}\\]\nand for the Milstein method:\n\\[\n\\left\\{\n\\begin{split}\nX^M_n&=X^M_{n-1}+\\lambda X^M_{n-1}\\Delta t+\\mu X^M_{n-1}\\Delta W_{n-1}\\\\\n&+\\dfrac{1}{2}\\mu^2 X^M_{n-1}\\left[(\\Delta W_{n-1})^2-\\Delta t\\right]\\\\\n&=X^M_{n-1}\\left[1+\\left(\\lambda-\\dfrac{1}{2}\\mu^2\\right)\\Delta t+\\mu\\Delta W_{n-1}+\\dfrac{1}{2}\\mu^2(\\Delta W_{n-1})^2\\right]\\\\\nX^M_0&=x_0\n\\end{split}\n\\right.\n\\tag{16}\\]\nRecall that \\(\\Delta W_{n-1}=\\sqrt{\\Delta t}\\,Z\\), where \\(Z\\propto N(0,1)\\).\nEquation 15 reads:\n\\[\n\\left\\{\n\\begin{split}\nX^E_n&=X^E_{n-1}(1+\\lambda\\Delta t+\\mu\\sqrt{\\Delta t}\\,Z)\\\\\nX^E_0&=x_0\n\\end{split}\n\\right.\n\\tag{17}\\]\nEquation 16 reads:\n\\[\n\\left\\{\n\\begin{split}\nX^M_n&=X^M_{n-1}\\left[1+\\left(\\lambda+\\dfrac{1}{2}\\mu^2(Z^2-1)\\right)\\Delta t+\\mu\\sqrt{\\Delta t}\\,Z\\right]\\\\\nX^M_0&=x_0\\\\\n\\end{split}\n\\right.\n\\tag{18}\\]\nComparing the exact solution Equation 14 with Equation 18 shows how the Milstein method makes the Taylor expansion exact up to order \\(\\text{O}(\\Delta t)\\).\nThe following code will test the strong convergence of both methods when they are applied to the linear SDE Equation 12, with \\(\\lambda=2,\\mu=1,x_0=1\\). One thousand sample paths are simulated over the time interval \\([0,1]\\), for step sizes being integer multiples of \\(2^{-9}\\;(8,4,2,1)\\). The order of convergence is tested at the end point of the chosen time interval. The results of the simulations are shown in Figure 1.\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(1792)\nlambda  &lt;- 2 \nmu      &lt;- 1\nXzero   &lt;- 1\nT       &lt;- 1\nN       &lt;- 2^9\ndt      &lt;- 1/N\nM       &lt;- 1000\nXerr_EM &lt;- matrix(0, nrow = M, ncol = 5)\nXerr_M  &lt;- matrix(0, nrow = M, ncol = 5)\n\nfor (i in seq(1, M, by = 1)) {\n  dW    &lt;- rnorm(N, mean = 0, sd = sqrt(dt))\n  W     &lt;- cumsum(dW)\n  Xtrue &lt;- Xzero*exp((lambda - 0.5*mu^2)*T + mu*W[N])\n  for (j in 1:5) {\n    R  &lt;- 2^(j-1)    \n    Dt &lt;- R*dt \n    L  &lt;- N/R\n    Xtemp_EM &lt;- Xzero\n    Xtemp_M  &lt;- Xzero\n    for (k in seq(1, L, by = 1)) {\n      Winc     &lt;- sum(dW[(R*(k-1)+1):(R*k)])\n      Xtemp_EM &lt;- Xtemp_EM*(1 + Dt*lambda + mu*Winc)\n      Xtemp_M  &lt;- Xtemp_M*(1 + Dt*lambda + 0.5*mu^2*(Winc^2 - Dt) + mu*Winc)\n    }\n    Xerr_EM[i, j] &lt;- abs(Xtemp_EM - Xtrue)\n    Xerr_M[i, j]  &lt;- abs(Xtemp_M - Xtrue)\n  }\n}\nDtvals &lt;- c(1, 2, 4, 8)*dt\nXm_EM  &lt;- array(0, dim = 4)\nXm_M   &lt;- array(0, dim = 4)\nfor (i in 1:4) {\n  Xm_EM[i] &lt;- mean(Xerr_EM[, i])\n  Xm_M[i]  &lt;- mean(Xerr_M[, i])\n}\nt   &lt;- rep(Dtvals, by = 2)\ng   &lt;- rep(c(\"Euler-Maruyama\", \"Milstein\"), each = 4)\ne   &lt;- c(Xm_EM, Xm_M)\ne_t &lt;- c(sqrt(t), t)\ndf &lt;- data.frame(t = t, method = g, error = e, error_true = e_t)\n\n\n\n\n\n\n\n\n\n\nFigure 1: Strong error plot for the Euler-Maruyama and Milstein’s methods; dashed lines give the appropriate reference slope for each method, \\(\\gamma=0.5\\) (Euler-Maruyama) and \\(\\gamma=1\\) (Milstein)."
  },
  {
    "objectID": "posts/numerical_simulation/index.html#lamperti-transform",
    "href": "posts/numerical_simulation/index.html#lamperti-transform",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "Lamperti transform",
    "text": "Lamperti transform\nAn interesting application of the Itô formula Equation 7 is now discussed in connection with a formulation of Equation 4 where the diffusion coefficient is not an explicit function of time, but only depends on the state variable \\(X\\):\n\\[\ndX_t=b(t,X_t)\\,dt+\\sigma(X_t)\\,dW_t\n\\tag{19}\\]\nThe Lamperti transform is defined as follows:\n\\[\nY_t=F(X_t)=\\int_z^{X_t}\\frac{1}{\\sigma(s)}\\,ds\n\\tag{20}\\]\nwhere \\(z\\) is any arbitrary value in the state space of \\(X\\). We assume that the function \\(F(\\cdot)\\) defines a one-to-one mapping from the state space of \\(X\\) to \\(\\mathbb{R}\\). We have:\n\\[\nF_t(X_t)=0\\quad\\quad F_x(X_t)=\\dfrac{1}{\\sigma(X_t)}\\quad\\quad F_{xx}(X_t)=-\\dfrac{\\sigma_x(X_t)}{\\sigma^2(X_t)}\n\\tag{21}\\]\nApplying the Itô formula Equation 5, we obtain:\n\\[\n\\left\\{\n\\begin{split}\ndY_t&=\\left[\\dfrac{b(t,X_t))}{\\sigma(X_t)}-\\dfrac{1}{2}\\sigma_x(X_t)\\right]dt+dW_t\\\\\nX_t&=F^{-1}(Y_t)\n\\end{split}\n\\right.\n\\tag{22}\\]\nThe Lamperti transform changes the generic SDE Equation 4 into another SDE with unitary diffusion coefficient. As an example of application of the Lamperti transform, we consider here a result on transformations of SDEs that is relevant for the topic of interest in this post.\nFirst, we discretize the Lamperti transform of Equation 22 using the Euler-Maruyama method (see Equation 8):\n\\[\nY_n=Y_{n-1}+\\left[\\dfrac{b(t_{n-1},X_{n-1})}{\\sigma(X_{n-1})}-\\dfrac{1}{2}\\sigma_x(X_{n-1})\\right]\\Delta t+\\sqrt{\\Delta t}\\,Z\n\\tag{23}\\]\nSecond, we apply the Taylor expansion to the inverse transform \\(X_t=G(Y_t)\\), noting that:\n\\[\n\\begin{split}\nG_y(Y_t)&=\\sigma(G(Y_t))\\\\\nG_{yy}(Y_t)&=\\sigma(G(Y_t))\\sigma_x(G(Y_t))\n\\end{split}\n\\tag{24}\\]\nWe obtain:\n\\[\nG(Y_{n})=G(Y_{n-1})+G_y(Y_{n-1})(Y_n-Y_{n-1})+\\frac{1}{2}G_{yy}(Y_{n-1})(Y_n-Y_{n-1})^2\n\\tag{25}\\]\nand hence:\n\\[\n\\begin{split}\nX_n&=X_{n-1}+\\left[b(t_{n-1},X_{n-1})-\\dfrac{1}{2}\\sigma(X_{n-1})\\sigma_x(X_{n-1})\\right]\\Delta t\\\\\n&+\\sigma(X_{n-1})\\sqrt{\\Delta t}\\,Z+\\frac{1}{2}\\sigma(X_{n-1})\\sigma_x(X_{n-1})\\Delta t\\,Z^2\n\\end{split}\n\\tag{26}\\]\nIn conclusion, the Milstein method on the original process and the Euler-Maruyama method on the transformed process are equal up to and including the order \\(\\text{O}(\\Delta t)\\).\nIn general if a transformation such as the Lamperti transform eliminates the interactions between the state of the process and the increments of the Wiener process, the performance of the Euler-Maruyama method on the transformed SDE are expected to improve.\nThe logarithmic transformation \\(F(X_t)=\\log X_t\\) is an example of nonlinear transformation that is capable of eliminating the interaction between the state \\(X_t\\) of the GBM process and the increments of the Wiener process \\(dW_t\\), as shown in Equation 13. Therefore the Euler-Maruyama method can be confidently applied to the transformed process; the Milstein method is then obtained by simply taking the Taylor expansion of the inverse transform."
  },
  {
    "objectID": "posts/audio_features/index.html",
    "href": "posts/audio_features/index.html",
    "title": "Audio features for free",
    "section": "",
    "text": "In this post, I’ll be exploring how to pull back audio features and other information available for any track in the Spotify library. This information is usually hidden to users, however it can be easily retrieved. This will be done here using the spotifyr R wrapper package, whose description sounds as a quick and easy wrapper for pulling back audio features from Spotify’s Web API in bulk. Information about the various audio features offered by Spotify in the analysis of each track, as well as what all those features actually mean, is part of the discussion herein.\nThe first step in accessing Spotify data is to get an API key. This can be done by first logging into the Personal Dashboard available in the “Spotify for Developers” page (this requires that I signed on to a valid account, of course!) - see Figure 1.\nFrom within the ‘Basic information page’ of the app lyrics I have created in my Dashboard, I was directed to select “Create a Client ID”. After filling out the required questions, I received my Client ID and Client Secret. The following code chunk is then needed to get the Spotify access token:\nlibrary(spotifyr)\n\nid &lt;- 'my client ID'\nsecret &lt;- 'my client secret'\n\nSys.setenv(SPOTIFY_CLIENT_ID = id)\nSys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\naccess_token &lt;- get_spotify_access_token()\nFor one of the several playlists that I have created using my Spotify account, namely trial, I invoke the function get_user_playlists() to obtain the associated playlist_id. This is possible once I have retrieved the information about the desired playlist using, e.g., the verb filter from tidyverse, as in the code chunk below, where how to use the Spotify ID to complete the authentication process is also shown.\nlibrary(tidyverse)\n\nmy_id &lt;- 'my Spotify ID'\nplaylist_id &lt;- get_user_playlists(my_id) %&gt;%\n  filter(name == 'trial') %&gt;%\n  pull(id)\nAll is done. Now, a number of functions are available in spotifyr to get access to the audio features of all the tracks contained in the playlist trial.\ntracks &lt;- get_playlist_tracks(playlist_id = playlist_id, \n                              limit = 100, \n                              authorization = access_token)\nfeatures &lt;- get_track_audio_features(tracks,\n                                     authorization = access_token)\nThe call to get_playlist_tracks() with limit fixed to 100 (default maximum value) clarifies that only data of up 100 tracks can be retrieved from any Spotify playlist using just one call. The argument authorization requires a valid access token, as explained above. With the function get_track_audio_features() we finally get the values of the audio features. These numerical representations of various characteristics of each music track of interest are derived, behind the scenes, through the advanced audio analysis algorithms developed by Spotify as an important part of their business strategy. It is worth noting that tracks contains a wealth of other relevant information, including track title, author, album, release date and so forth."
  },
  {
    "objectID": "posts/audio_features/index.html#audio-features",
    "href": "posts/audio_features/index.html#audio-features",
    "title": "Audio features for free",
    "section": "Audio features",
    "text": "Audio features\nThe following table reports a list of the audio features, see here. Since my native language is Italian, I translate each English term (in bold) into a corresponding Italian term (in italic.) Next to the column Audio feature, the column Explanation provides information regarding what each audio feature can mean.\n\n\n\nAudio feature\nExplanation\n\n\n\n\nacousticness (acusticità)\nIndex from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence that the track is acoustic, i.e., with little or no electronic elements.\n\n\ndanceability (ballabilità)\nIndex from 0.0 to 1.0 describing how suitable a track is for dancing. The index is based on a combination of tempo, rhythm stability, beat strength and overall regularity. A higher danceability score indicates a more danceable track.\n\n\nduration_ms (durata)\nTrack duration, in ms.\n\n\nenergy (energia)\nIndex from 0.0 to 1.0, which helps representing the intensity and activity level of a track. Tracks with high energy values tend to be more upbeat and energetic (they sound faster, louder and noisier than tracks with lower energy). Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\ninstrumentalness (proporzione di parte strumentale su parte vocale)\nIndex from 0.0 to 1.0, which helps predicting whether a track contains no vocals. A higher instrumentalness values indicates a higher likelihood that the track is purely instrumental. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n\nkey (chiave) (*)\nThe estimated overall key of the track. Integers, from -1 to 11, map to pitches using standard Pitch Class notation. E.g., 0 = C (do), 1 = C♯/D♭ (do diesis/re bemolle), 2 = D (re) up to 11 = B (si). If no key is detected, the value is -1.\n\n\nliveness (vivacità)\nIndex from 0.0 to 1.0, which helps detecting the presence of an audience in the recording. A higher liveness values suggests the track was performed live.\n\n\nloudness (presenza)\nThis index represents the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Typical values range between -60 and 0 dB.\n\n\nmode (tonalità) (*)\nMode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor by 0.\n\n\nspeechiness (proporzione di parlato)\nIndex from 0.0 to 1.0, which helps identifying tracks that contain spoken words. Higher values indicate more speech-like sounds, distinguishing them from purely instrumental tracks. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech. Values below 0.33 most likely represent music and other non-speech-like tracks.\n\n\ntempo (tempo)\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\ntime signature (indicazione del tempo) (*)\nIt yiels an estimated overall time signature for a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of “3/4”, to “7/4”.\n\n\nvalence (positività emotiva)\nAlso known as the “positiveness” index, valence describes, in a range from 0.0 to 1.0, the musical positiveness conveyed by a track. Tracks with higher valence values tend to sound more positive, cheerful, and euphoric, while lower scores suggest more negative, sad, or depressing emotions.\n\n\n\n(*) Key, mode and time signature relate to the fundamental musical components of a track - these features are essential for understanding its harmonic structure and rhythmic pattern.\nAnother parameter that can be retrieved is named popularity. The popularity of a track will be between 0 and 100, with 100 being the most popular. The popularity is calculated by an algorithm and is mostly based on the total number of plays the track has had and how recent those plays are. Generally speaking, tracks that are being played a lot now will have a higher popularity than tracks that were played a lot in the past."
  },
  {
    "objectID": "posts/audio_features/index.html#example",
    "href": "posts/audio_features/index.html#example",
    "title": "Audio features for free",
    "section": "Example",
    "text": "Example\nAs a matter of example, my Spotify’s playlist trial contains just four tracks. They were extracted from those published by a beloved singer of mine, Sergio Caputo in his debut album “Un Sabato Italiano”, which was released in 1983. The magazine “Rolling Stone Italia” ranks the album “Un Sabato Italiano” at the 37th position in the list of the greatest 100 Italian albums of all time (here).\nThe songs in this album are a sort of diary of the life that the singer lived in the early Eighties. We can enjoy the chance to find ourselves immersed with him and his great friend Rino into smoky bars and complicated love stories, dealt with in noisy never-ending Saturday nights. Prevalent in all the tracks is the rhythm of swing and jazz. The high quality of the lyrics concurs, with the music, to evoke brilliantly those atmospheres, and even today the album can be considered a remarkable testimonianza of the mood of the early Eighties.\nFor the sake of better visualization, I grouped the results of the audio analysis of these tracks in the following three tables; moreover, I condensed the values of the three parameters key, mode and time signature into only one cell.\n\n\n\nKey, mode, time signature, tempo, duration and loudness\n\n\ntrack title\nkey-mode-time signature\ntempo, BPM\nduration, min\nloudness, dB\n\n\n\n\nMercy bocù\nF major in 3/4\n116.19\n4.06\n-14.09\n\n\nBimba se sapessi\nF major in 4/4\n153.76\n3.45\n-13.70\n\n\nMettimi giù\nG major in 4/4\n76.35\n4.35\n-18.34\n\n\nSpicchio di luna\nG major in 4/4\n77.73\n3.18\n-14.47\n\n\n\n\n\n\n\n\nAcousticness, instrumentalness, liveness and speechiness\n\n\ntrack title\nacousticness\ninstrumentalness\nliveness\nspeechiness\n\n\n\n\nMercy bocù\n0.40\n0\n0.15\n0.05\n\n\nBimba se sapessi\n0.30\n0\n0.18\n0.06\n\n\nMettimi giù\n0.12\n0\n0.09\n0.07\n\n\nSpicchio di luna\n0.47\n0\n0.11\n0.09\n\n\n\n\n\n\n\n\nDanceability, energy, valence and popularity\n\n\ntrack title\ndanceability\nenergy\nvalence\npopularity\n\n\n\n\nMercy bocù\n0.60\n0.40\n0.42\n26\n\n\nBimba se sapessi\n0.55\n0.48\n0.88\n32\n\n\nMettimi giù\n0.67\n0.46\n0.89\n19\n\n\nSpicchio di luna\n0.40\n0.36\n0.14\n26\n\n\n\n\n\n\n\n\nTranslating music in numbers as exemplified above, might be, and actually is, the starting point of some heavy data mining for doing interesting research at the intersection between several different domains, which may include data science, statistics, machine learning, sociology, psychology, neuroscience, just to mention a few of them. Something more about these opportunities will be discussed in future posts … up to now, that’s my two cents for the day!\n\nThe thumbnail image is credited to Spotify icons created by Creative - Flaticon"
  },
  {
    "objectID": "posts/resolution/index.html",
    "href": "posts/resolution/index.html",
    "title": "Frequency resolution of spectral analysis",
    "section": "",
    "text": "Frequency resolution is the size of the smallest frequency for which details in the frequency response and the spectrum can be resolved by the estimate. For example, a resolution of 0.1 Hz means that the frequency response variations at frequency intervals at or below 0.1 Hz cannot be resolved.\nConsider an analog band-limited signal \\(x(t)\\) with bandwidth \\(B\\) Hz; \\(x(t)\\) is observed over a sample period of \\(T_r\\) s (the length of the data record) and sampled at a sampling frequency of \\(f_s\\) Hz (\\(T_s=1/f_s\\) denotes the sampling interval). The total available number of samples of \\(x(t)\\) is \\(N=\\lfloor T_r/T_s\\rfloor\\), where \\(\\lfloor\\cdot\\rfloor\\) denotes the floor function (or greatest integer function), namely the function that takes as input a real number \\(r\\) and gives as output the greatest integer less than or equal to \\(r\\).\nAccording to the Shannon-Nyquist sampling theorem, if the sampling frequency \\(f_s\\) is chosen to be \\(2B\\), the maximum resolvable frequency is:\n\\[\nf_{\\text{max}}=\\frac{f_s}{2}\n\\]\nOn the other hand, the minimum resolvable frequency is inversely related to the sample period:\n\\[\nf_{\\text{min}}=\\frac{1}{T_r}=\\frac{1}{NT_s}\n\\tag{1}\\]\nand hence the number of frequencies that can be resolved from \\(f_{\\text{min}}\\) to \\(f_{\\text{max}}\\) is\n\\[\nN_f=\\frac{f_{\\text{max}}-f_{\\text{min}}}{\\Delta f}\n\\]\nwhere \\(\\Delta f\\) is the frequency resolution. Since \\(\\Delta f=f_{\\text{min}}\\), I also have:\n\\[\nN_f=\\dfrac{\\dfrac{f_s}{2}-\\dfrac{f_s}{N}}{\\dfrac{f_s}{N}}=\\frac{N}{2}-1\n\\]\nThis implies that will be \\(N/2\\) discrete frequencies from \\(0\\) to \\(f_{\\text{max}}\\).\nThe accurate detection of frequency components in the spectrum \\(X(f)\\) of the signal \\(x(t)\\) is challenged by the phenomenon known as spectral leakage, or amplitude ambiguity; it consists of ambiguous and false amplitudes occurring in the spectrum \\(X(f)\\) whenever the sample period \\(T_r\\) is not an integer multiple of all of the contributory periods in \\(x(t)\\). That is, false or ambiguous amplitudes will occur at frequencies that are immediately adjacent to the actual frequency.\nFor aperiodic signals, \\(T_r\\) theoretically must be infinite. For periodic signals, \\(T_r\\) must be equal to the least common integer multiple of all the periods contained in the signal. Application of the DFT (Discrete Fourier Transform) or FFT (Fast Fourier Transform) to an aperiodic signal implicitly assumes that the signal is infinite in length and formed by repeating the signal of length \\(T_r\\) an infinite number of times. This leads to discontinuities in the amplitude that occur at each integer multiple of \\(T_r\\). These discontinuities are step-like, which introduce false amplitudes that decrease around the main frequencies.\nAll the periods contained in the signal cannot be known before the spectral analysis is performed, therefore the stated condition on the sample period cannot be fulfilled; the best method to minimize the effect of spectral leakage is windowing.\nHerein, I just provide a brief explanation of windowing, without testing it in the examples to follow. Windowing reduces the amplitude of the discontinuities at the boundaries of each finite sequence of samples of \\(x(t)\\). It does so by multiplying the acquired sequence by a finite-length window with an amplitude that varies smoothly and gradually toward zero at the edges. This makes the endpoints of the waveform meet and, therefore, results in a continuous waveform without sharp transitions.\n\n\n\n\n\n\nCan discrete-time sinusoids be non-periodic?\n\n\n\nThe normalized frequency \\(f\\) in cycles per sample of a discrete-time sinusoid:\n\\[\nx[n]=\\cos(2\\pi fn)\n\\]\nis restricted to values in the interval \\(-1/2\\leq f\\leq1/2\\). This is because, for any discrete-time sinusoid with frequency \\(\\vert f\\vert&gt;1/2\\), an integer \\(m\\) exists such that \\(f=f_0+m\\) and \\(\\vert f_0\\vert\\leq1/2\\):\n\\[\n\\cos 2\\pi fn=\\cos[2\\pi (f_0+m)\\,n]=\\cos(2\\pi f_0n)+\\cos(2\\pi mn)=\\cos(2\\pi f_0n)\n\\]\nIt is worth noting that the highest rate of oscillation of discrete-time sinusoids is when, at every time instant, the output sample flips polarity with respect to the previous output sample:\n\\[\nf_0=\\frac{1}{2}\\rightarrow\\cos(2\\pi f_0n)=\\cos(\\pi n)=(-1)^n\n\\]\nMoreover, a discrete-time sinusoid can be periodic, i.e, characterized by patterns that exactly repeat themselves in time, if and only if its frequency can be expressed in terms of a rational number; moreover, an additive mixture of periodical discrete-time sinusoids (called here sinusoidal mixture) is periodic with period equal to the least common integer multiple of their periods.\n\n\nAs outlined above, a spectrum line at frequency \\(f\\) will be accurately represented by the DFT when \\(f_s\\geq2f_{\\text{max}}\\) and \\(T_r = mT\\), where \\(m=1,2,\\cdots\\), and \\(T=1/f\\). This implies that \\(N=mf_s/f\\). If \\(T_r\\) is not an integer multiple of \\(T\\), leakage will occur in the DFT. This appears as amplitudes at \\(f\\) spilling onto adjacent frequencies.\nAs an example, consider a sinusoidal mixture with three unit-amplitude components at 2.875 Hz, 3 Hz, 3.125 Hz. The sampling frequency is set to \\(f_s=16\\) Hz and the sample period to \\(T=8\\) s (\\(N=128\\)): the component at 2.875 Hz is observed for 23 periods, the component at 3 Hz for 24 periods, and the component at 3.125 Hz for 25 periods. The resolution calculated according to Equation 1 is 0.125 Hz.\nA chunk of code written in MATLAB (Natick, Massachusetts: The MathWorks, Inc., https://www.mathworks.com) shows how to simulate the sinusoidal mixture and to perform spectral analysis.\n% *****************************\n% sinusoidal mixture generation\n% *****************************\n\nfs = 16;   % sampling frequency, Hz\nTs = 1/fs; % sampling interval, s\nT  = 8;    % sample period, s\n\nt  = (0:Ts:T-Ts); % time domain, s\nN  = length(t);   % number of samples\n\nf1 = 2.9375;      % frequency component 1, Hz\nf2 = 3;           % frequency component 2, Hz\nf3 = 3.0625;      % frequency component 3, Hz\n\nx1 = cos(t.*(2*pi*f1)); % sinusoid 1\nx2 = cos(t.*(2*pi*f2)); % sinusoid 2\nx3 = cos(t.*(2*pi*f3)); % sinusoid 3\nX  = x1 + x2 + x3;      % sinusoidal mixture\n\n% ********************************************************\n% calculation of single-side spectrum (see comments below)\n% ********************************************************\n\nP              = fft(X, N);         % FFT calculation\nP2             = abs(P/N);          % double-side spectrum, rescaled by N     \nP1             = P2(1:N/2+1);  \nP1(:, 2:end-1) = 2*P1(2:end-1); \nY              = P1(1:end-1);       % single-side spectrum\nf              = (0:N/2-1).*(fs/N); % frequency domain, Hz\n\n\n\n\n\n\nComments\n\n\n\nThe documentation in MATLAB explains the procedure to convert the N-points FFT spectrum of the signal to the single-side amplitude spectrum:\n\nBecause the FFT function includes a scaling factor N between the original and the transformed signals, rescale the spectrum by dividing by N.\nTake the complex magnitude of the FFT spectrum. The two-side amplitude spectrum P2, where the spectrum in the positive frequencies is the complex conjugate of the spectrum in the negative frequencies, has half the peak amplitudes of the time-domain signal.\nTo convert to the single-side spectrum Y, take the first half P1 of the two-side spectrum P2 and multiply by 2. P1(1) and P1(end) are not multiplied by 2 because these amplitudes correspond to the zero and Nyquist frequencies, respectively, and they do not have the complex conjugate pairs in the negative frequencies.\nDefine the frequency domain f for the single-side spectrum Y.\n\n\n\nThe single-side spectrum of the sinusoidal mixture is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Single-side spectrum of the sinusoidal mixture composed of three components at 2.875 Hz, 3 Hz, 3.125 Hz; sampling frequency: 16 Hz; sample period: 8 s. For the sake of visualization the frequency interval shown is limited to 2.5-3.5 Hz.\n\n\n\nConsider now the case that the three components of the sinusoidal mixture have frequencies 2.9375 Hz, 3 Hz, 3.0625 Hz. To resolve them, I would need to double the frequency resolution of the FFT, up to 0.0625 Hz, with respect to the scenario of Figure 1. Erroneously, I double the sampling frequency. However, this expedient leaves the frequency resolution unaltered, since doubling the sample period without touching the sample period also doubles the number of samples. Moreover, the sample period is not properly chosen to avoid spectral leakage, as clearly seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 32 Hz; sample period: 8 s.\n\n\n\nTo resolve the components of the sinusoidal mixture and avoid spectral leakage, the frequency resolution of 0.0625 Hz can be achieved by extending the time period to \\(T_r=16\\) s, which corresponds to \\(N=256\\) samples at the original sampling frequency \\(f_s=16\\) Hz, see Figure 3.\n\n\n\n\n\n\nFigure 3: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 16 Hz; sample period: 16 s.\n\n\n\n\nTo conclude: whereas the sampling frequency sets the time resolution, the sample period occupies a central place in setting the frequency resolution when spectral analysis is performed. The sample period being the same, just increasing the sampling frequency simply increases the number of samples by the same ratio, leaving their ratio unaltered. The only means to increase the frequency resolution is to increase the sample period, trying at the same time to minimize the effects of spectral leakage."
  },
  {
    "objectID": "posts/significant_figures/index.html",
    "href": "posts/significant_figures/index.html",
    "title": "Significant figures",
    "section": "",
    "text": "It happens frequently that the number of digits that are available to report measurement results is high. Usually, measurement results are produced by carrying arithmetic operations with computers or calculators, whose level of numerical precision, albeit finite, is too high given the true information gathered by the measurements. In other words, the precision can be excessive, and too many digits can simply swamp the observer, making the message in the measurements more obscure. Significant figures, also referred to as significant digits, are specific digits within a number written in positional notation that carry both reliability and necessity in reporting a measurement result. Proper use of significant figures is thus an essential element in the presentation of both experimental and calculated results together with their associated uncertainty.\n\n\nA number of rules exist for determining how many significant figures are in a number:\n\nnon-zero digits are always significant\n\n\n4.6 has two significant figures\n\n\nleading zeros placed before the first non-zero digit are not significant (they are called placeholders)\n\n\n0.046 has two significant figures\n\n\ntrailing zeros placed after all other digits but behind a decimal point are significant\n\n\n4.60 has three significant figures\n\nThe leftmost digit which is not a zero is referred to as the most significant digit (MSD); the rightmost digit of a decimal number is the least significant digit (LSD), regardless it is a zero or not: 4 and 0 are thus, respectively, the MSD and the LSD of 4.60; 4 and 1 are, respectively, the MSD and the LSD of 4.61. Every digit between the LSD and the MSD, including zeros, should be counted as significant figures, hence 4.60 and 40.60 have, respectively, three and four significant figures.\nAmbiguous situations arise when zeros are at the end of the number and not behind a decimal point as, for example, in the number 4600. Confusion can be avoided if the number is expressed in scientific notation.\n\n\n\n\n\n\nScientific notation\n\n\n\nScientific notation is a way of expressing numbers that are much too large or much too small to be conveniently written in decimal form (i.e., their representation would involve a long string of digits). In scientific notation, nonzero numbers are written in the form:\n\\[\nm\\times10^n\n\\]\nwhere \\(n\\) is an integer, and the coefficient \\(m\\) is a nonzero real number (usually \\(1\\leq\\vert\\,m\\,\\vert&lt;10\\)). The integer \\(n\\) is called the exponent and the real number \\(m\\) is called the mantissa. If the number is negative, then a minus sign precedes \\(m\\), as in ordinary decimal notation.\n\n\nIn scientific notation, the number 4600 can be written using a different number of significant figures, based on rule 3. above:\n\\[\n\\begin{split}\n4.600\\times 10^3&\\quad\\text{four significant figures}\\\\\n4.60\\times 10^3&\\quad\\text{three significant figures}\\\\\n4.6\\times 10^3&\\quad\\text{two significant figures}\n\\end{split}\n\\]\n\n\n\nA number can be rounded so as to drop digits until a prescribed number of significant figures is retained in the final representation. Recall that all the digits after the decimal point to the right of the desired LSD are to be dropped and not replaced with zeros, which otherwise should add to the number of significant figures (rule 3 above). The rules of rounding are the following:\n\nif the digit to the right of the desired LSD is greater than 5, add 1 to the LSD, otherwise do nothing\n\n\nExample - round at the fourth significant figure\n\n\\[\n\\begin{split}\n53.8\\underline{7}4&\\rightarrow53.87\\\\\n53.8\\underline{7}9&\\rightarrow53.88\n\\end{split}\n\\]\n\nif the digit to the right of the LSD is 5, apply a tie-breaking rule, also called the five rule. When the first digit to be dropped is 5, the leading digit next to it is examined. If this digit is even, including zero, it is left unaltered; otherwise, one unit is added. This helps avoiding the accumulation of errors that would be otherwise determined by rounding systematically up or down. Using the five rule, five out of ten cases consist of rounding up and five out of ten cases consist of rounding down.\n\n\nExample - round at the fifth significant figure\n\n\\[\n\\begin{split}\n726.8\\underline{0}51\\rightarrow 726.80\\\\\n726.8\\underline{3}51\\rightarrow 726.84\\\\\n726.8\\underline{6}51\\rightarrow 726.86\\\\\n726.8\\underline{9}51\\rightarrow 726.90\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/significant_figures/index.html#significant-figures",
    "href": "posts/significant_figures/index.html#significant-figures",
    "title": "Significant figures",
    "section": "",
    "text": "It happens frequently that the number of digits that are available to report measurement results is high. Usually, measurement results are produced by carrying arithmetic operations with computers or calculators, whose level of numerical precision, albeit finite, is too high given the true information gathered by the measurements. In other words, the precision can be excessive, and too many digits can simply swamp the observer, making the message in the measurements more obscure. Significant figures, also referred to as significant digits, are specific digits within a number written in positional notation that carry both reliability and necessity in reporting a measurement result. Proper use of significant figures is thus an essential element in the presentation of both experimental and calculated results together with their associated uncertainty.\n\n\nA number of rules exist for determining how many significant figures are in a number:\n\nnon-zero digits are always significant\n\n\n4.6 has two significant figures\n\n\nleading zeros placed before the first non-zero digit are not significant (they are called placeholders)\n\n\n0.046 has two significant figures\n\n\ntrailing zeros placed after all other digits but behind a decimal point are significant\n\n\n4.60 has three significant figures\n\nThe leftmost digit which is not a zero is referred to as the most significant digit (MSD); the rightmost digit of a decimal number is the least significant digit (LSD), regardless it is a zero or not: 4 and 0 are thus, respectively, the MSD and the LSD of 4.60; 4 and 1 are, respectively, the MSD and the LSD of 4.61. Every digit between the LSD and the MSD, including zeros, should be counted as significant figures, hence 4.60 and 40.60 have, respectively, three and four significant figures.\nAmbiguous situations arise when zeros are at the end of the number and not behind a decimal point as, for example, in the number 4600. Confusion can be avoided if the number is expressed in scientific notation.\n\n\n\n\n\n\nScientific notation\n\n\n\nScientific notation is a way of expressing numbers that are much too large or much too small to be conveniently written in decimal form (i.e., their representation would involve a long string of digits). In scientific notation, nonzero numbers are written in the form:\n\\[\nm\\times10^n\n\\]\nwhere \\(n\\) is an integer, and the coefficient \\(m\\) is a nonzero real number (usually \\(1\\leq\\vert\\,m\\,\\vert&lt;10\\)). The integer \\(n\\) is called the exponent and the real number \\(m\\) is called the mantissa. If the number is negative, then a minus sign precedes \\(m\\), as in ordinary decimal notation.\n\n\nIn scientific notation, the number 4600 can be written using a different number of significant figures, based on rule 3. above:\n\\[\n\\begin{split}\n4.600\\times 10^3&\\quad\\text{four significant figures}\\\\\n4.60\\times 10^3&\\quad\\text{three significant figures}\\\\\n4.6\\times 10^3&\\quad\\text{two significant figures}\n\\end{split}\n\\]\n\n\n\nA number can be rounded so as to drop digits until a prescribed number of significant figures is retained in the final representation. Recall that all the digits after the decimal point to the right of the desired LSD are to be dropped and not replaced with zeros, which otherwise should add to the number of significant figures (rule 3 above). The rules of rounding are the following:\n\nif the digit to the right of the desired LSD is greater than 5, add 1 to the LSD, otherwise do nothing\n\n\nExample - round at the fourth significant figure\n\n\\[\n\\begin{split}\n53.8\\underline{7}4&\\rightarrow53.87\\\\\n53.8\\underline{7}9&\\rightarrow53.88\n\\end{split}\n\\]\n\nif the digit to the right of the LSD is 5, apply a tie-breaking rule, also called the five rule. When the first digit to be dropped is 5, the leading digit next to it is examined. If this digit is even, including zero, it is left unaltered; otherwise, one unit is added. This helps avoiding the accumulation of errors that would be otherwise determined by rounding systematically up or down. Using the five rule, five out of ten cases consist of rounding up and five out of ten cases consist of rounding down.\n\n\nExample - round at the fifth significant figure\n\n\\[\n\\begin{split}\n726.8\\underline{0}51\\rightarrow 726.80\\\\\n726.8\\underline{3}51\\rightarrow 726.84\\\\\n726.8\\underline{6}51\\rightarrow 726.86\\\\\n726.8\\underline{9}51\\rightarrow 726.90\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/significant_figures/index.html#finite-precision-arithmetic",
    "href": "posts/significant_figures/index.html#finite-precision-arithmetic",
    "title": "Significant figures",
    "section": "Finite precision arithmetic",
    "text": "Finite precision arithmetic\nIn mathematical operations involving significant figures, the result cannot be more precise than the least precise number. Calculations in finite precision arithmetic can be done following a few simple rules. One rule applies to multiplication and division, and another rule applies to addition and subtraction. Recall that values that are considered exact numbers, e.g., known conversion factors or physical constants, are not to be included in the determination of the number of significant figures.\n\nMultiplication and division\n\nWhen we multiply/divide two numbers, we should add their relative uncertainties. The uncertainty of the result is given roughly by the number of the digits, regardless of their placement.\n\nIn a calculation involving multiplication/division the number of significant figures in the result should equal the least number of significant figures in any one of the numbers being multiplied or divided.\nIn the following example, the number 1.6 is reported with two significant figures; the number 2, seen as a known constant, can be considered having an infinite number of significant figures, whereas the number 2.0 has two significant figures. The result should be reported with two significant figures in both cases:\n\\[\n\\begin{split}\n&1.6\\times2=3.2&\\\\\n&1.61\\times2.0=3.2&\\quad\\text{not}\\;3.22\n\\end{split}\n\\]\n\n\nAddition and subtraction\n\nWhen we add/subtract two numbers, we should add their uncertainties. The uncertainty of the result is given roughly by the placement of the digits, not by the number of digits.\n\nIn a calculation involving addition/subtraction, the number of decimal places in the result should equal the least number of decimal places in any one of the numbers being added or subtracted.\nIn the following example, the number 132.03 is reported with five significant figures, and the number 3.210 is reported with four significant figures. However, when the two numbers are added, what matters really is the number of decimal places, i.e., two for the number 132.03 and three for the number 3.210. The result should be reported with two decimal digits and not reported using four significant figures.\n\\[\n\\begin{split}\n&132.03+3.210=135.24&\\;\\text{and not}\\;135.2\\\\\n&132.03+3=135&\\;\\text{and not}\\;135.03\\\\\n&132.03+3.00=135.03&\\;\\text{and not}\\;135\\\\\n\\end{split}\n\\]\nThe prescription about the minimum number of decimal places of any of the numbers involved in the calculation can be explained by considering that, implicitly, the precision of any measurement is dictated by the decimal place. For a measurement of length expressed in meters, for example, the second decimal digit implies a measurement precise to the hundredths (centimeter-level), the third decimal digit to the thousandth (millimeter-level). So by keeping the result with the minimum number of decimal places we basically state that we do not want to imply to get a result more precise than the least precise measurement that was needed to produce the result itself.\n\n\nMultiple arithmetic operations\n\nIn a calculation involving multiple arithmetic operations, the rules are applied without rounding results after each intermediate step. Instead keep track of the rightmost digit that would be retained. The operations would be performed in the following order:\n\n\noperations in parentheses ( )\nmultiplication\ndivision\naddition\nsubtraction\n\nIt is important to always perform intermediate calculations without rounding the numbers that are involved in the operations. If numbers are rounded every time during many sequential calculations, the results are skewed and some systematic error is surely introduced. Only after that all calculations are carried out with all digits retained at each step, the final result has to be rounded to the desired number of significant figures.\nAs an example, two numbers reported with five significant figures each are added, and the final result is rounded to three significant figures. If the addends are first rounded to three significant figures and then added, the result we produce is wrong:\n\\[\n\\begin{split}\n&1.4248+1.2732=2.6980\\rightarrow 2.70&\\quad\\text{correct}\\\\\n&1.42+1.27=2.69\\rightarrow 2.70&\\quad\\text{wrong}\n\\end{split}\n\\]\n\nExample 1 (Sequential calculation) Suppose that we want to perform the following operation:\n\\[\n(2.5\\times3.42)+13.681-0.1\n\\]\n\nperform first the product between parentheses - we keep track of the first decimal place, which would be retained based on rule B above.\n\n\\[\n2.5\\times3.42=8.\\underline{5}500\n\\]\n\nperform addition - although, based on rule A above, the result would be expressed using five significant figures, only the first decimal place is kept tracked:\n\n\\[\n8.5500+13.681=22.\\underline{2}310\n\\]\n\nperform subtraction:\n\n\\[\n22.2310-0.1=22.\\underline{1}31\n\\]\n\nrounding to three significant figures:\n\n\\[\n(2.5\\times3.42)+13.681-0.1\\rightarrow22.1\n\\]\n\nWhen doing multi-step calculations, we need:\n\nto keep at least one more significant figure in intermediate results than needed in the final answer. Furthermore, never round intermediate answers: rounding, say, to two significant figures in an intermediate answer, and then writing three significant figures in the final answer is wrong.\nnot to write more significant figures in the final result (of a measurement process) than justified (by the measurement uncertainty)."
  },
  {
    "objectID": "posts/significant_figures/index.html#significant-figures-and-measurement-uncertainty",
    "href": "posts/significant_figures/index.html#significant-figures-and-measurement-uncertainty",
    "title": "Significant figures",
    "section": "Significant figures and measurement uncertainty",
    "text": "Significant figures and measurement uncertainty\nThe value of one measurand must be delivered by rounding the digit loaded by the measurement uncertainty \\(U\\), where \\(U\\) is represented by a number with, usually, no more than one or two significant figures (rounded up, possibly). The additional uncertainty due to rounding must be checked for being negligible compared to \\(U\\). Essentially, \\(U\\) gives an estimate of the errors incurred in the measurement.\nFor example, if we have a length \\(L=(12.37\\pm0.10)\\;\\text{cm}\\), we can report the length as \\(L=12.4\\;\\text{cm}\\). When we express a number with three significant figures, what we are saying is that the first two digits are essentially exactly correct, and the last one is uncertain by a small amount (generally it is only uncertain by about \\(\\pm1\\)). In the example above, we rounded our answer to \\(12.4\\;\\text{cm}\\) because our answer is uncertain to \\(\\pm0.1\\;\\text{cm}\\), namely our answer is uncertain in the last digit by about 1.\n\nExample 2 (Rounding) Round the measurement \\(z=12.0349\\;\\text{cm}\\), whose uncertainty is stated being \\(\\Delta z=0.153\\;\\text{cm}\\).\n\nround the uncertainty to two significant figures:\n\n\\[\n\\Delta z=0.15\\,\\text{cm}\n\\]\n\nround \\(z\\) using the same number of decimal places as \\(\\Delta z\\):\n\n\\[\nz=12.03\\,\\text{cm}\n\\]\n\nprovide the measurement report:\n\n\\[\nz\\pm\\Delta z=(12.03\\pm0.15)\\,\\text{cm}\n\\]\n\n\nExample 3 (Use of the scientific notation) When the answer is given in scientific notation, the uncertainty should be given in scientific notation with the same power of ten as the answer. Suppose that \\(z=1.43\\times10^6\\;\\text{s}\\) and \\(\\Delta z=2\\times10^4\\;\\text{s}\\):\n\\[\nz\\pm\\Delta z=(1.43\\pm0.02)\\,10^6\\;\\text{s}\n\\]\n\n\nExample 4 (Addition/subtraction of uncertain numbers) The length of two blocks is measured, and the measurements are \\(l_1=1.13\\;\\text{m}\\) (considered precise to the level of centimeters) and \\(l_2=0.551\\;\\text{m}\\) (considered precise to the level of millimeters). We need to compute the length \\(l\\) of the block resulting from stacking the two blocks together:\n\\[\nl=l_1+l_2=1.681\\;\\text{m}\\rightarrow l=1.68\\;\\text{m}\n\\]\nIt does not make any physical sense to consider the length of the overall block precise to the level of the millimeters, given that one of the two blocks is measured less precisely. The result should be at least as precise as the least precise term involved in the addition, as stated by the rule for addition/subtraction of uncertain numbers.\n\n\nExample 5 (Multiplication/division of uncertain numbers) A rectangular floor needs to be covered by a number of squared tiles. According to the measurements that are available, the rectangular floor has width \\(w=1.91\\;\\text{m}\\) and length \\(l=1.57\\;\\text{m}\\) and each squared tile has size \\(a=0.15\\;\\text{m}\\). All measurements are considered precise to the level of centimeters, and three significant figures should then be considered for their numerical representation. The number of tiles can be easily calculated:\n\\[\nN\\approx\\dfrac{w\\,l}{a^2}=133.2756\\;\\text{m}^2\\,\\text{m}^{-2}\n\\]\nTo comply with the rule for multiplication/division between uncertain numbers, \\(133.2756\\) has to be rounded using three significant figures, yielding the integer \\(N=133\\), which is then the number of tiles expected to cover the floor.\n\n\nExample 6 (Conversion of scale) A measurement of temperature is performed, leading to the following report:\n\\[\nT_c=(54.0\\pm0.5)\\;^{\\circ}\\text{C}\n\\]\nWe want to convert this expression in units of kelvin:\n\\[\nT_K=T_C+273.15=(327.15\\pm0.5)\\;\\text{K}\n\\]\nThe uncertainty expressed in degree Celsius (\\(\\pm0.5\\;^{\\circ}\\text{C}\\)) translates directly in the uncertainty expressed in kelvin (\\(\\pm 0.5\\;\\text{K}\\)). This is because transforming a measurement expressed in degree Celsius into a measurement expressed in kelvin implies a change of offset, but not a change of scale. Since the uncertainty loads the first decimal digit of the numerical representation of the measured temperature, we should report \\(T_K\\) with one decimal digit, which requires rounding (based on the five rule):\n\\[\nT_k=(327.2\\pm0.5)\\;\\text{K}\n\\]\nThe prescription of the minimum number of significant figures (rule for the addition/subtraction of uncertain numbers) would yield \\(T_k=327\\;\\text{K}\\). This is because \\(54.0\\) has three significant figures, and \\(273.15\\) can be considered to have an infinite number of significant figures, since it is a known constant; hence \\(T_K\\) should be reported with three significant figures according to the rule for addition/subtraction of uncertain numbers. However, this rule is superseded by considering the prescription concerning how to express the measurement uncertainty.\n\n\nThe concept of significant figures and their relation with measurement uncertainty has been briefly reviewed. This topic is important because many measured quantities are often reported with more significant figures than necessary, in the face of the loaded uncertainty. Reporting too many digits is confusing for the reader and of no relevance as for the information content associated to the measurements."
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html",
    "href": "posts/correspondence_analysis_1/index.html",
    "title": "Correspondence analysis: Part I",
    "section": "",
    "text": "Correspondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. If data of only two variables are involved, the method is usually called Simple Correspondence Analysis (SCA); if data of more than two variables are considered in the dataset, then the method is usually called Multiple Correspondence Analysis (MCA).\nSCA is often used in combination with a standard chi-squared test of independence for two categorical variables that form a contingency table. We recall that a contingency table displays frequencies for combinations of two categorical variables; analysts also refer to contingency tables as crosstabulation and two-way tables. Contingency tables classify outcomes for one variable in rows and the other in columns. The values at the row and column intersections are frequencies for each unique combination of the two variables. These values can suggest whether or not the two variables are correlated.\n\n\nCode\n# Libraries \n\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(corrplot)\nlibrary(factoextra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#introduction",
    "href": "posts/correspondence_analysis_1/index.html#introduction",
    "title": "Correspondence analysis: Part I",
    "section": "",
    "text": "Correspondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. If data of only two variables are involved, the method is usually called Simple Correspondence Analysis (SCA); if data of more than two variables are considered in the dataset, then the method is usually called Multiple Correspondence Analysis (MCA).\nSCA is often used in combination with a standard chi-squared test of independence for two categorical variables that form a contingency table. We recall that a contingency table displays frequencies for combinations of two categorical variables; analysts also refer to contingency tables as crosstabulation and two-way tables. Contingency tables classify outcomes for one variable in rows and the other in columns. The values at the row and column intersections are frequencies for each unique combination of the two variables. These values can suggest whether or not the two variables are correlated.\n\n\nCode\n# Libraries \n\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(corrplot)\nlibrary(factoextra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#exemplary-dataset",
    "href": "posts/correspondence_analysis_1/index.html#exemplary-dataset",
    "title": "Correspondence analysis: Part I",
    "section": "Exemplary dataset",
    "text": "Exemplary dataset\nhousetasks, available in the package factoextra, is a data frame that contains the frequency of execution of 13 house tasks performed by the couple in four different ways: a) the wife only; b) alternatively; c) the husband only; d) jointly.\n\n\nCode\ndata(housetasks)\nhousetasks %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    caption = \"House tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks contingency table\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n156\n14\n2\n4\n\n\nMain_meal\n124\n20\n5\n4\n\n\nDinner\n77\n11\n7\n13\n\n\nBreakfeast\n82\n36\n15\n7\n\n\nTidying\n53\n11\n1\n57\n\n\nDishes\n32\n24\n4\n53\n\n\nShopping\n33\n23\n9\n55\n\n\nOfficial\n12\n46\n23\n15\n\n\nDriving\n10\n51\n75\n3\n\n\nFinances\n13\n13\n21\n66\n\n\nInsurance\n8\n1\n53\n77\n\n\nRepairs\n0\n3\n160\n2\n\n\nHolidays\n0\n1\n6\n153\n\n\n\n\n\n\n\nTo easily interpret the contingency table, a graphical matrix can be drawn using the function balloonplot() from the gplots package. In this graph, each cell contains a dot whose size reflects the relative magnitude of the value it contains.\n\n\nCode\n# Convert the data frame to a table\ndt &lt;- as.table(as.matrix(housetasks))\n# Graph\nballoonplot(t(dt), main = \"House tasks contingency table\", xlab = \"\", ylab = \"\",\n            label = FALSE, show.margins = FALSE)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#theory-an-overview",
    "href": "posts/correspondence_analysis_1/index.html#theory-an-overview",
    "title": "Correspondence analysis: Part I",
    "section": "Theory: An overview",
    "text": "Theory: An overview\nLet us consider a generic two-dimensional contingency table \\(\\bf{T}\\), having \\(R\\) rows (corresponding to the levels of the categorical variable \\(X\\)) and \\(C\\) columns (corresponding to the levels of the categorical variable \\(Y\\)). The table is built based on a dataset of \\(n\\) observations, which are distributed across the \\(R\\cdot C\\) cells of \\(\\bf{T}\\). Each cell shows the count \\(n_{ij},\\,i=1,\\cdots,R;\\,j=1,\\cdots,C\\).\n\nKey terms\nRow marginals\nThe row marginals (or row margins) can be computed by taking the row sums of the counts \\(n_{ij}\\):\n\\[\nn_{i+}=\\sum_{j=1}^C n_{ij},\\;i=1,\\dots,R\n\\]\nColumn marginals\nThe column marginals (or col margins) can be computed by taking the column sums of the counts \\(n_{ij}\\):\n\\[\nn_{+j}=\\sum_{i=1}^R n_{ij}\\;j=1,\\dots,C\n\\]\nGrand total\nThe gran total is the complete number after everything has been added up:\n\\[\nn_{++}=\\sum_{i=1}^R n_{i+}=\\sum_{j=1}^Cn_{+j}=\\sum_{i=1}^R\\sum_{j=1}^Cn_{ij}=n\n\\]\n\n\nCode\n# Row marginals\nrow.sum &lt;- apply(housetasks, 1, sum)\n# Column marginals\ncol.sum &lt;- apply(housetasks, 2, sum)\n# Gran total\nn &lt;- sum(housetasks)\nhousetasks %&gt;%\n  mutate(total = row.sum) %&gt;%\n  rbind(\"Total\" = c(col.sum, n)) %&gt;%\n  kable(\n    align = \"rrrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\", \"Total\"),\n    caption = \"House task contingency table with row, column margins and gran total\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse task contingency table with row, column margins and gran total\n\n\n\nWife\nAlternating\nHusband\nJointly\nTotal\n\n\n\n\nLaundry\n156\n14\n2\n4\n176\n\n\nMain_meal\n124\n20\n5\n4\n153\n\n\nDinner\n77\n11\n7\n13\n108\n\n\nBreakfeast\n82\n36\n15\n7\n140\n\n\nTidying\n53\n11\n1\n57\n122\n\n\nDishes\n32\n24\n4\n53\n113\n\n\nShopping\n33\n23\n9\n55\n120\n\n\nOfficial\n12\n46\n23\n15\n96\n\n\nDriving\n10\n51\n75\n3\n139\n\n\nFinances\n13\n13\n21\n66\n113\n\n\nInsurance\n8\n1\n53\n77\n139\n\n\nRepairs\n0\n3\n160\n2\n165\n\n\nHolidays\n0\n1\n6\n153\n160\n\n\nTotal\n600\n254\n381\n509\n1744\n\n\n\n\n\n\n\nThe gran total is obtained by summing the values of the row margin or the values of the col margin (\\(n=\\) 1744).\nTo compare rows (or columns), their profiles can be analyzed, in the search for similar rows (or columns).\nRow profile\nThe profile of a row (or row profile) is calculated by taking each row point and dividing by the corresponding value of the row margin:\n\\[\n\\underline{a}_i,\\;i=1,\\dots,R=\\{\\underline{a}_{i_j},\\;j=1,\\dots,C\\}=\\{n_{ij}/n_{i+},\\;j=1,\\dots,C\\}\n\\]\nAverage row profile\nThe average row profile can be computed from the col margin after dividing it by the grand total:\n\\[\n\\underline{r}=\\{r_j=n_{+j}/n,\\;j=1,\\dots,C\\}\n\\]\n\n\nCode\n# Row profile\nrow.profile &lt;- housetasks/row.sum\n# Average row profile\naverage.row.profile &lt;- col.sum/n\n# Row margins\nrow.profile.sum &lt;- apply(row.profile, 1, sum)\n\nrow.profile %&gt;%\n  mutate(TOTAL = row.profile.sum) %&gt;%\n  rbind(`Average row profile` = c(average.row.profile, sum(average.row.profile))) %&gt;%\n  kable(\n    align = \"rrrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\", \"Total\"),\n    digits = 4,\n    caption = \"House tasks data frame with row profiles and average row profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks data frame with row profiles and average row profile\n\n\n\nWife\nAlternating\nHusband\nJointly\nTotal\n\n\n\n\nLaundry\n0.8864\n0.0795\n0.0114\n0.0227\n1\n\n\nMain_meal\n0.8105\n0.1307\n0.0327\n0.0261\n1\n\n\nDinner\n0.7130\n0.1019\n0.0648\n0.1204\n1\n\n\nBreakfeast\n0.5857\n0.2571\n0.1071\n0.0500\n1\n\n\nTidying\n0.4344\n0.0902\n0.0082\n0.4672\n1\n\n\nDishes\n0.2832\n0.2124\n0.0354\n0.4690\n1\n\n\nShopping\n0.2750\n0.1917\n0.0750\n0.4583\n1\n\n\nOfficial\n0.1250\n0.4792\n0.2396\n0.1562\n1\n\n\nDriving\n0.0719\n0.3669\n0.5396\n0.0216\n1\n\n\nFinances\n0.1150\n0.1150\n0.1858\n0.5841\n1\n\n\nInsurance\n0.0576\n0.0072\n0.3813\n0.5540\n1\n\n\nRepairs\n0.0000\n0.0182\n0.9697\n0.0121\n1\n\n\nHolidays\n0.0000\n0.0063\n0.0375\n0.9562\n1\n\n\nAverage row profile\n0.3440\n0.1456\n0.2185\n0.2919\n1\n\n\n\n\n\n\n\nColumn profile\nThe profile of a column (or col profile) is calculated by taking each column point and dividing by the corresponding value of the col margin:\n\\[\n\\underline{b}_j,\\;j=1,\\dots,C=\\{\\underline{b}_{j_i},\\;i=1,\\dots,R\\}=\\{n_{ij}/n_{+j},\\;i=1,\\dots,R\\}\n\\]\nAverage column profile\nThe average column profile (or average col profile) can be computed from the row marginal after dividing it by the grand total:\n\\[\n\\underline{c}=\\{c_i=n_{i+}/n,\\;i=1,\\dots,R\\}\n\\]\n\n\nCode\n# Column profile\ncol.profile &lt;- t(housetasks)/col.sum\ncol.profile &lt;- as.data.frame(t(col.profile))\n# Average column profile\naverage.col.profile &lt;- row.sum/n\n# Column margins\ncol.profile.sum &lt;- apply(col.profile, 2, sum)\n\ncol.profile %&gt;%\n  mutate(`Average col profile` = average.col.profile) %&gt;%\n  rbind(Total = c(col.profile.sum, sum(average.col.profile))) %&gt;%\n  kable(\n    align = \"rrrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\", \"Average col profile\"),\n    digits = 4,\n    caption = \"House tasks data frame with col profiles and average col profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks data frame with col profiles and average col profile\n\n\n\nWife\nAlternating\nHusband\nJointly\nAverage col profile\n\n\n\n\nLaundry\n0.2600\n0.0551\n0.0052\n0.0079\n0.1009\n\n\nMain_meal\n0.2067\n0.0787\n0.0131\n0.0079\n0.0877\n\n\nDinner\n0.1283\n0.0433\n0.0184\n0.0255\n0.0619\n\n\nBreakfeast\n0.1367\n0.1417\n0.0394\n0.0138\n0.0803\n\n\nTidying\n0.0883\n0.0433\n0.0026\n0.1120\n0.0700\n\n\nDishes\n0.0533\n0.0945\n0.0105\n0.1041\n0.0648\n\n\nShopping\n0.0550\n0.0906\n0.0236\n0.1081\n0.0688\n\n\nOfficial\n0.0200\n0.1811\n0.0604\n0.0295\n0.0550\n\n\nDriving\n0.0167\n0.2008\n0.1969\n0.0059\n0.0797\n\n\nFinances\n0.0217\n0.0512\n0.0551\n0.1297\n0.0648\n\n\nInsurance\n0.0133\n0.0039\n0.1391\n0.1513\n0.0797\n\n\nRepairs\n0.0000\n0.0118\n0.4199\n0.0039\n0.0946\n\n\nHolidays\n0.0000\n0.0039\n0.0157\n0.3006\n0.0917\n\n\nTotal\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n\n\n\n\n\nDistance (or similarity) between row profiles\nThis metrics is useful to compare how similar to each other are two rows of the contingency table:\n\\[\nd^2_{kl},\\;k,l=1,\\dots,R=\\Vert \\underline{a}_k-\\underline{a}_{l}\\Vert^2_\\underline{r}=\\large\\sum_{j=1}^C\\dfrac{(\\underline{a}_{k_j}-\\underline{a}_{l_j})^2}{r_j}\n\\]\nRecall that we need to normalize each component within the sum to the corresponding element of the average row profile.\n\n\nCode\n# \"Dinner\" and \"Driving\" profiles\ndinner.p &lt;- row.profile[\"Dinner\", ]\ndriving.p &lt;- row.profile[\"Driving\", ]\n# Distance between \"Dinner\" and \"Driving\"\nd2_1 &lt;- sum(((dinner.p - driving.p)^2)/average.row.profile)\n# \"Breakfeast\" profile\nbreakfast.p &lt;- row.profile[\"Breakfeast\", ]\n# Distance between \"Dinner\" and \"Breakfeast\"\nd2_2 &lt;- sum(((dinner.p - breakfast.p)^2)/average.row.profile)\n\n\nExample - The similarity between “Dinner” and “Driving” profiles is 2.742, whereas the similarity between “Dinner” and “Breakfeast” is 0.238.\nDistance (or similarity) between col profiles\nThis metrics is useful to compare how similar to each other are two columns of the contingency table:\n\\[\nd^2_{kl},\\;k,l=1,\\dots,C=\\Vert \\underline{b}_k-\\underline{b}_l\\Vert^2_\\underline{c}=\\large\\sum_{i=1}^R\\dfrac{(\\underline{b}_{k_i}-\\underline{b}_{l_i})^2}{c_i}\n\\]\nRecall that we need to normalize each component within the sum to the corresponding element of the average col profile.\n\n\nCode\n# \"Wife\" and \"Husband\" profiles\nwife.p &lt;- col.profile[, \"Wife\"]\nhusband.p &lt;- col.profile[, \"Husband\"]\n# Distance between \"Wife\" and \"Husband\"\nd2_1 &lt;- sum(((wife.p - husband.p)^2) / average.col.profile)\n# \"Jointly\" profile\njointly.p &lt;- col.profile[, \"Jointly\"]\n# Distance between \"Wife\" and \"Jointly\"\nd2_2 &lt;- sum(((wife.p - jointly.p)^2) / average.col.profile)\n\n\nExample - The similarity between “Wife” and “Husband” profiles is 4.05, whereas the similarity between “Wife” and “Jointly” is 2.935.\nDistance (or similarity) between each row profile and the average row profile\nThis metrics is useful to compare how similar to the average row profile is any row profile.\n\\[\nd^2_{k},\\;k=1,\\dots,R=\\Vert\\underline{a}_k-\\underline{r}\\Vert^2_\\underline{r}=\\large\\sum_{j=1}^C\\dfrac{(\\underline{a}_{k_j}-r_{j})^2}{r_j}\n\\]\n\n\nCode\nd2.row &lt;- apply(row.profile, 1, \n        function(row.p, av.p){sum(((row.p - av.p)^2)/av.p)}, \n        average.row.profile)\n\nas.matrix(round(d2.row,3)) %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Task\", \"Similarity\"),\n    digits = 3,\n    caption = \"Similarity between each row profile and the average row profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSimilarity between each row profile and the average row profile\n\n\nTask\nSimilarity\n\n\n\n\nLaundry\n1.329\n\n\nMain_meal\n1.034\n\n\nDinner\n0.618\n\n\nBreakfeast\n0.512\n\n\nTidying\n0.353\n\n\nDishes\n0.302\n\n\nShopping\n0.218\n\n\nOfficial\n0.968\n\n\nDriving\n1.274\n\n\nFinances\n0.456\n\n\nInsurance\n0.727\n\n\nRepairs\n3.307\n\n\nHolidays\n2.140\n\n\n\n\n\n\n\nDistance (or similarity) between each col profile and the average col profile\nThis metrics is useful to compare how similar to the average col profile is any col profile.\n\\[\nd^2_{l},\\;l=1,\\dots,C=\\Vert\\underline{b}_l-\\underline{c}\\Vert^2_\\underline{c}=\\large\\sum_{i=1}^R\\dfrac{(\\underline{b}_{l_i}-c_i)^2}{c_i}\n\\]\n\n\nCode\nd2.col &lt;- apply(col.profile, 2, \n        function(col.p, av.p){sum(((col.p - av.p)^2)/av.p)}, \n        average.col.profile)\n\nas.matrix(round(d2.col,3)) %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Couple\", \"Similarity\"),\n    digits = 3,\n    caption = \"Similarity between each col profile and the average col profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSimilarity between each col profile and the average col profile\n\n\nCouple\nSimilarity\n\n\n\n\nWife\n0.875\n\n\nAlternating\n0.809\n\n\nHusband\n1.746\n\n\nJointly\n1.078\n\n\n\n\n\n\n\nDistance matrix\nThe similarity can be computed between each row (col) profile and the other row (col) profiles in the contingency table, using, for example, the function dist-matrix() (here).\n\n\nCode\n# data: a data frame or matrix \n# average.profile: average profile\ndist.matrix &lt;- function(data, average.profile) {\n   mat &lt;- as.matrix(t(data))\n   n &lt;- ncol(mat)\n   dist.mat &lt;- matrix(NA, n, n)\n   diag(dist.mat) &lt;- 0\n    for (i in 1:(n - 1)) {\n        for (j in (i + 1):n) {\n            d2 &lt;- sum(((mat[, i] - mat[, j])^2) / average.profile)\n            dist.mat[i, j] &lt;- dist.mat[j, i] &lt;- d2\n        }\n    }\n  colnames(dist.mat) &lt;- rownames(dist.mat) &lt;- colnames(mat)\n  dist.mat\n}\n\n\nThe result is a distance matrix (a kind of correlation or dissimilarity matrix), either orientated per rows or columns. The distance between row (col) profiles can be computed using the package corrplot for visualization.\nRow distance matrix\n\n\nCode\n# Distance matrix (row)\ndist.mat &lt;- dist.matrix(row.profile, average.row.profile)\ndist.mat &lt;- round(dist.mat, 2)\n# Visualize the matrix (row)\ncorrplot(dist.mat, type = \"upper\", is.corr = FALSE, col = colorRampPalette(c(\"white\", \"steelblue\"))(128))\n\n\n\n\n\n\n\n\n\nCol distance matrix\n\n\nCode\n# Distance matrix (column)\ndist.mat &lt;- dist.matrix(t(col.profile), average.col.profile)\ndist.mat &lt;- round(dist.mat, 2)\n# Visualize the matrix (column)\ncorrplot(dist.mat, type = \"upper\", is.corr = FALSE, col = colorRampPalette(c(\"white\", \"steelblue\"))(128))\n\n\n\n\n\n\n\n\n\nRow mass and inertia\nThe row mass can be defined as the total frequency of each row, obtained by dividing its row sum by the gran total. The row inertia is calculated as the row mass multiplied by the squared distance between the row and the average row profile. The inertia is a measure of the information contained in each row.\n\n\nCode\n# Row mass\nrow.sum &lt;- apply(housetasks, 1, sum)\ngrand.total &lt;- sum(housetasks)\nrow.mass &lt;- row.sum/grand.total\n# Row inertia\nrow.inertia &lt;- row.mass*d2.row\n\nrow.inertia %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Task\", \"Inertia\"),\n    digits = 3,\n    caption = \"Row inertia of the house tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nRow inertia of the house tasks contingency table\n\n\nTask\nInertia\n\n\n\n\nLaundry\n0.134\n\n\nMain_meal\n0.091\n\n\nDinner\n0.038\n\n\nBreakfeast\n0.041\n\n\nTidying\n0.025\n\n\nDishes\n0.020\n\n\nShopping\n0.015\n\n\nOfficial\n0.053\n\n\nDriving\n0.102\n\n\nFinances\n0.030\n\n\nInsurance\n0.058\n\n\nRepairs\n0.313\n\n\nHolidays\n0.196\n\n\n\n\n\n\n\nColumn mass and inertia\nThe column mass can be defined as the total frequency of each column, obtained by dividing its col sum by the gran total. The column inertia is calculated as the column mass multiplied by the squared distance between the column and the average col profile. The inertia is a measure of the information contained in each column.\n\n\nCode\n# Column mass\ncol.sum &lt;- apply(housetasks, 2, sum)\ngrand.total &lt;- sum(housetasks)\ncol.mass &lt;- col.sum/grand.total\n# Column inertia\ncol.inertia &lt;- col.mass*d2.col\n\ncol.inertia %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Couple\", \"Inertia\"),\n    digits = 3,\n    caption = \"Column inertia of the house tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nColumn inertia of the house tasks contingency table\n\n\nCouple\nInertia\n\n\n\n\nWife\n0.301\n\n\nAlternating\n0.118\n\n\nHusband\n0.381\n\n\nJointly\n0.315\n\n\n\n\n\n\n\nTotal inertia\nThe total inertia is the total information contained in the contingency table.\n\n\nCode\n# Total inertia\ninertia_total_row &lt;- sum(row.inertia)\ninertia_total_col &lt;- sum(col.inertia)\n\n\nThe total inertia is computed as the sum of row inertias (or equivalently, as the sum of column inertias), in the present case we get the value 1.115.\n\n\nCode\n# Row mass\nrow.sum &lt;- apply(housetasks, 1, sum)\ngrand.total &lt;- sum(housetasks)\nrow.mass &lt;- row.sum/grand.total\n# Row inertia\nrow.inertia &lt;- row.mass*d2.row\n# Total inertia\ninertia_total_row &lt;- sum(row.inertia)\n\n# Column mass\ncol.sum &lt;- apply(housetasks, 2, sum)\ncol.mass &lt;- col.sum/grand.total\n# Column inertia\ncol.inertia &lt;- col.mass*d2.col\n# Total inertia\ninertia_total_col &lt;- sum(col.inertia)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#results",
    "href": "posts/correspondence_analysis_1/index.html#results",
    "title": "Correspondence analysis: Part I",
    "section": "Results",
    "text": "Results\nThe result for rows can be summarized in terms of the squared distance between row profiles and the average row profile (i.e., d2.row), the row mass (i.e., row mass), and the row inertia (i.e., row.inertia). Similarly, the results for columns can be summarized using d2.col, col.mass and col.inertia.\n\n\nCode\nrow &lt;- cbind.data.frame(d2 = d2.row, mass = row.mass, inertia = row.inertia)\ncol &lt;- cbind.data.frame(d2 = d2.col, mass = col.mass, inertia = col.inertia)\n\nrow %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Task\", \"Squared distance\", \"Mass\", \"Inertia\"),\n    digits = 3,\n    caption = \"Summary statistics for the rows of the contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSummary statistics for the rows of the contingency table\n\n\nTask\nSquared distance\nMass\nInertia\n\n\n\n\nLaundry\n1.329\n0.101\n0.134\n\n\nMain_meal\n1.034\n0.088\n0.091\n\n\nDinner\n0.618\n0.062\n0.038\n\n\nBreakfeast\n0.512\n0.080\n0.041\n\n\nTidying\n0.353\n0.070\n0.025\n\n\nDishes\n0.302\n0.065\n0.020\n\n\nShopping\n0.218\n0.069\n0.015\n\n\nOfficial\n0.968\n0.055\n0.053\n\n\nDriving\n1.274\n0.080\n0.102\n\n\nFinances\n0.456\n0.065\n0.030\n\n\nInsurance\n0.727\n0.080\n0.058\n\n\nRepairs\n3.307\n0.095\n0.313\n\n\nHolidays\n2.140\n0.092\n0.196\n\n\n\n\n\n\n\nCode\ncol %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"couple\", \"squared distance\", \"mass\", \"inertia\"),\n    digits = 3,\n    caption = \"Summary statistics for the columns of the contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSummary statistics for the columns of the contingency table\n\n\ncouple\nsquared distance\nmass\ninertia\n\n\n\n\nWife\n0.875\n0.344\n0.301\n\n\nAlternating\n0.809\n0.146\n0.118\n\n\nHusband\n1.746\n0.218\n0.381\n\n\nJointly\n1.078\n0.292\n0.315\n\n\n\n\n\n\n\n\nChi-square test\nThe chi-square statistic (\\(\\chi^2\\)) is an overall measure of the difference between the frequencies observed in a contingency table and the expected frequencies calculated under the assumption of homogeneity of row profiles (or column profiles). Geometrically, the inertia measures how far away row profiles (or column profiles) are from their average profiles. The average profile can be thought of as the representative of the hypothesis of homogeneity (i.e., equality) of profiles. The distances between profiles are measured using the distance of the chi-square (distance \\(\\chi^2\\)). This distance is similar in its formulation to the Euclidean distance between points in a physical space, except that any quadratic difference between coordinates is divided by the corresponding element of the mean profile.\nIt can be shown that the total inertia \\(\\Phi\\) of a contingency table is the statistic (\\(\\chi^2\\)) divided by the grand total \\(n\\):\n\\[\n\\Phi=\\dfrac{\\chi^2}{n}=\\sum_{i}^Rc_i\\Vert\\underline{a}_i-\\underline{r}\\Vert^2_{\\underline{r}}=\\sum_{j}^Cr_j\\Vert\\underline{b}_j-\\underline{c}\\Vert^2_{\\underline{c}}\n\\]\nThe Chi-squared test can be used to examine whether rows and columns of a contingency table are statistically significantly associated:\n\nNull hypothesis (H0): the row and the column variables of the contingency table are independent;\nAlternative hypothesis (H1): row and column variables are dependent\n\nFor each cell of the table, we have to calculate the expected value under H0. For a given cell \\((i,j)\\), the expected value is calculated by taking the product of the \\(i\\)-th element of the row margin (sum of the elements in the \\(i\\)-th row of the contingency table) and the \\(j\\)-th element of the col margin (sum of the elements in the \\(j\\)-th column of the contingency table), divided by the gran total.\n\n\n\n\n\n\nStatistical testing of homogeneity/independence\n\n\n\nSuppose that the population is divided into \\(R\\) groups and each group (or the entire population) is divided into \\(C\\) categories. We would like to test whether the distribution of categories in each group is the same (homogeneity test). If observations \\(X_1,\\dots,X_n\\) are sampled independently from the entire population then homogeneity over groups is the same as independence of groups and categories. If we have homogeneity:\n\\[\n\\text{Pr}(\\text{Category}_j\\vert\\text{Group}_i)=\\text{Pr}(\\text{Category}_j)\n\\]\nthen we have independence:\n\\[\n\\text{Pr}(\\text{Category}_j,\\text{Group}_i)=\\text{Pr}(\\text{Category}_j\\vert\\text{Group}_i)\\text{Pr}(\\text{Group}_i)=\\text{Pr}(\\text{Category}_j)\\text{Pr}(\\text{Group}_i)\n\\]\nIt is also possible to move the other way around (i.e., start from independence to obtain homogeneity). In other words, to test homogeneity, we can use the test of independence. The computation of the expected cell counts under the null hypothesis H0 (the row and the column variables of the contingency table are independent) is done according to the scheme of Figure 1.\n\n\n\n\n\n\nFigure 1: The expected counts are involved in testing either homegeneity or independence\n\n\n\n\n\nFor example, in the case of the house tasks contingency table, the scheme of computation for the cell “Shopping”-“Alternating” is shown in Figure 2.\n\n\n\n\n\n\nFigure 2: How to calculate the expected counts\n\n\n\n\n\nCode\n# Table of expected counts\nE = (row.sum %*% t(col.sum))/n\n\nrow.names(E) &lt;- row.names(housetasks)\nE %&gt;% \n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    digits = 2,\n    caption = \"Table of expected counts\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nTable of expected counts\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n60.55\n25.63\n38.45\n51.37\n\n\nMain_meal\n52.64\n22.28\n33.42\n44.65\n\n\nDinner\n37.16\n15.73\n23.59\n31.52\n\n\nBreakfeast\n48.17\n20.39\n30.58\n40.86\n\n\nTidying\n41.97\n17.77\n26.65\n35.61\n\n\nDishes\n38.88\n16.46\n24.69\n32.98\n\n\nShopping\n41.28\n17.48\n26.22\n35.02\n\n\nOfficial\n33.03\n13.98\n20.97\n28.02\n\n\nDriving\n47.82\n20.24\n30.37\n40.57\n\n\nFinances\n38.88\n16.46\n24.69\n32.98\n\n\nInsurance\n47.82\n20.24\n30.37\n40.57\n\n\nRepairs\n56.77\n24.03\n36.05\n48.16\n\n\nHolidays\n55.05\n23.30\n34.95\n46.70\n\n\n\n\n\n\n\nThe calculated Chi-square statistic \\(\\chi^2\\) is compared to the critical value (obtained from statistical tables) with \\(df=(R-1)(C-1)\\) degrees of freedom and \\(p=0.05\\). Recall that \\(R, C\\) are the number of rows and columns in the contingency table, respectively. If the calculated Chi-square statistic is greater than the critical value, then we must conclude that the row and the column variables are not independent of each other. This implies that they are significantly associated. The function chisq.test() can be used to implement the test.\nThe result of chisq.test() function is a list containing the following components:\n\nstatistic: the value the chi-squared test statistic.\nparameter: the degrees of freedom\np.value: the p-value of the test\nobserved: the observed count\nexpected: the expected count\nresiduals: Pearson residuals\n\nIt is worth noting that the total inertia is equal to the total Chi-square score divided by the grand total. The square root of the total inertia is called trace and may be interpreted as a correlation coefficient. Any value of the trace &gt; 0.2 indicates a significant dependency between rows and columns. In the present example 1.056.\n\n\nPearson residuals\nIf we want to know the most contributing cells to the total Chi-square score, we just have to calculate the Chi-square statistic for each cell:\n\\[\nr_{ij} = \\dfrac{o_{ij}-e_{ij}}{\\sqrt{e_{ij}}},\\,i=1,\\dots,R;\\,j=1,\\dots,C\n\\]\nThe above formula returns the so-called Pearson residuals (\\(r\\)) for each cell (or standardized residuals). Cells with the highest absolute standardized residuals contribute the most to the total Chi-square score. Pearson residuals can be easily extracted from the output of the function chisq.test().\n\n\nCode\nchisq &lt;- chisq.test(housetasks)\nchisq\n\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\nCode\n# Observed counts\nchisq$observed\n\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\nShopping     33          23       9      55\nOfficial     12          46      23      15\nDriving      10          51      75       3\nFinances     13          13      21      66\nInsurance     8           1      53      77\nRepairs       0           3     160       2\nHolidays      0           1       6     153\n\n\nCode\n# Expected counts\nround(chisq$expected, 2)\n\n\n            Wife Alternating Husband Jointly\nLaundry    60.55       25.63   38.45   51.37\nMain_meal  52.64       22.28   33.42   44.65\nDinner     37.16       15.73   23.59   31.52\nBreakfeast 48.17       20.39   30.58   40.86\nTidying    41.97       17.77   26.65   35.61\nDishes     38.88       16.46   24.69   32.98\nShopping   41.28       17.48   26.22   35.02\nOfficial   33.03       13.98   20.97   28.02\nDriving    47.82       20.24   30.37   40.57\nFinances   38.88       16.46   24.69   32.98\nInsurance  47.82       20.24   30.37   40.57\nRepairs    56.77       24.03   36.05   48.16\nHolidays   55.05       23.30   34.95   46.70\n\n\nCode\n# Pearson's residuals\ne &lt;- chisq$residuals\n# Contribution in percentage of each cell (%)\ncontrib &lt;- 100*chisq$residuals^2/chisq$statistic\n\n\nThe sign of the standardized residuals is also very important to interpret the association between rows and columns. Positive values in cells specify an attraction (positive association) between the corresponding row and column variables, such as, for instance, between the column “Wife” and the row “Laundry”. There is a strong positive association between the column “Husband” and the row “Repairs”. A negative residual implies a repulsion (negative association) between the corresponding row and column variables. For example the column “Wife” is negatively associated (i.e., “not associated”) with the row “Repairs”. There is a repulsion between the column “Husband” and the rows “Laundry” and “Main_meal”.\nThe contribution (in %) of a given cell to the total Chi-square score can be calculated as follows:\n\\[\n\\text{contrib}\\,(\\%)=100\\,\\dfrac{r^2}{\\chi^2}\n\\]\nThe relative contribution of each cell to the total Chi-square score give some indication of the nature of the dependency between rows and columns of the contingency table. It can be seen that:\n\nThe column “Wife” is strongly associated with the rows “Laundry”\nThe column “Husband” is strongly associated with the row “Repairs”\nThe column “Jointly” is frequently associated with the row “Holidays”\n\n\n\nCode\ne %&gt;% \n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    digits = 2,\n    caption = \"Pearson's residuals\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nPearson's residuals\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n12.27\n-2.30\n-5.88\n-6.61\n\n\nMain_meal\n9.84\n-0.48\n-4.92\n-6.08\n\n\nDinner\n6.54\n-1.19\n-3.42\n-3.30\n\n\nBreakfeast\n4.88\n3.46\n-2.82\n-5.30\n\n\nTidying\n1.70\n-1.61\n-4.97\n3.59\n\n\nDishes\n-1.10\n1.86\n-4.16\n3.49\n\n\nShopping\n-1.29\n1.32\n-3.36\n3.38\n\n\nOfficial\n-3.66\n8.56\n0.44\n-2.46\n\n\nDriving\n-5.47\n6.84\n8.10\n-5.90\n\n\nFinances\n-4.15\n-0.85\n-0.74\n5.75\n\n\nInsurance\n-5.76\n-4.28\n4.11\n5.72\n\n\nRepairs\n-7.53\n-4.29\n20.65\n-6.65\n\n\nHolidays\n-7.42\n-4.62\n-4.90\n15.56\n\n\n\n\n\n\n\nCode\ncontrib %&gt;% \n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    digits = 2,\n    caption = \"Percentage contribution to the total Chi square\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nPercentage contribution to the total Chi square\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n7.74\n0.27\n1.78\n2.25\n\n\nMain_meal\n4.98\n0.01\n1.24\n1.90\n\n\nDinner\n2.20\n0.07\n0.60\n0.56\n\n\nBreakfeast\n1.22\n0.61\n0.41\n1.44\n\n\nTidying\n0.15\n0.13\n1.27\n0.66\n\n\nDishes\n0.06\n0.18\n0.89\n0.63\n\n\nShopping\n0.09\n0.09\n0.58\n0.59\n\n\nOfficial\n0.69\n3.77\n0.01\n0.31\n\n\nDriving\n1.54\n2.40\n3.37\n1.79\n\n\nFinances\n0.89\n0.04\n0.03\n1.70\n\n\nInsurance\n1.71\n0.94\n0.87\n1.68\n\n\nRepairs\n2.92\n0.95\n21.92\n2.28\n\n\nHolidays\n2.83\n1.10\n1.23\n12.45\n\n\n\n\n\n\n\n\nSCA is just the singular value decomposition of the standardized residuals. How to perform this analysis will be explained in the next post, together with a thorough discussion of the implications that the results of the analysis have."
  },
  {
    "objectID": "posts/random_incidence/index.html",
    "href": "posts/random_incidence/index.html",
    "title": "Random incidence",
    "section": "",
    "text": "Consider the situation when the continuous time axis of our observations is partitioned into a sequence of interarrival intervals. With the term arrival, we designate the occurrence of everything we are interested to observe, for instance, a particle emitted from radioactive material and captured by a radiation counter, a message reaching its destination queue, maybe the bus that we are waiting for at the bus stop. Usually, probabilistic models that attempt to describe these different type of arrivals share the same assumption: namely the interarrival times (i.e., the times between successive arrivals) are modeled as independent random variables. For instance, the continuous-time Poisson process (hopefully, this is not the right model to predict the next bus arrival!) is the case where the interarrival times are modeled as independent identically exponentially distributed random variables.\n\n\n\n\n\n\nExponential distribution\n\n\n\nA continuous random variable \\(X\\) is said to be exponential, or exponentially distributed with parameter \\(\\lambda\\), when its cumulative distribution function (CDF) is written\n\\[\nF_X(x)=\\text{Pr}(X\\leq x)=1-e^{-\\lambda x},\\;x\\geq0\n\\]\nThe probability density function (PDF) is then given by:\n\\[\np_X(x)=\\frac{d}{dx}F_X(x)=\\lambda\\,e^{-\\lambda x},\\;x\\geq0\n\\]\nThe mean value, the mean square value and the variance of \\(X\\) are:\n\\[\n\\left\\{\n\\begin{split}\nE[X]&=\\int_0^{\\infty}xp_X(x)\\,dx=\\frac{1}{\\lambda}\\\\\nE[X^2]&=\\int_0^{\\infty}x^2p_X(x)\\,dx=\\frac{2}{\\lambda^2}\\\\\n\\text{Var}(X)&=E[X^2]-E[X]^2=\\frac{1}{\\lambda^2}\n\\end{split}\n\\right.\n\\]\nIntegration by parts can be used to calculate the two expectations \\(E[X]\\) and \\(E[X^2]\\).\n\n\n\n\n\n\n\n\nPoisson process\n\n\n\nConsider a sequence of independent exponential random variables \\(T_1,T_2,T_3\\cdots,\\) with common parameter \\(\\lambda\\), and let these stand for the interarrival times. The arrivals are then recorded at times \\(T_1,T_1+T_2,T_1+T_2+T_3,\\cdots\\) and so forth, to define a continuous-time Poisson process.\nA Poisson process is endowed with the following important properties:\n\nIndependence of non-overlapping sets of times. This is a direct consequence of the assumed independence of the interarrival times.\nFresh-start property. The part of the Poisson process that starts at any particular time \\(t&gt;0\\) is a probabilistic replica of the Poisson process starting at time 0, and is independent of the part of the process prior to time \\(t\\). This can be seen as a special case of point 1.\nMemoryless interarrival time distribution. If \\(T\\) is the time of the first arrival and if we know that \\(T&gt;t\\), then the remaining time \\(T−t\\) is exponentially distributed, with the same parameter \\(\\lambda\\):\n\n\\[\n\\begin{split}\n\\text{Pr}(T&gt;t+s\\,\\vert\\,T&gt;t)&=\\frac{\\text{Pr}(T&gt;t+s,T&gt;t)}{\\text{Pr}(T&gt;t)}\\\\\n&=\\frac{\\text{Pr}(T&gt;t+s)}{\\text{Pr}(T&gt;t)}=\\frac{1-F_T(t+s)}{1-F_T(t)}\\\\\n&=\\frac{e^{-\\lambda(t+s)}}{e^{-\\lambda t}}=e^{-\\lambda s}=\\text{Pr}(T&gt;s)\n\\end{split}\n\\]\nProperties 2.-3. can be rephrased saying that the ones whose life is modeled by an exponential distribution, well they should remain forever young: no matter how long they have lived so far, the remaining time to their death is predicted as if they are just born!\n\n\nThe term random incidence denotes the arrival of an observer at an arbitrary time \\(t^*\\) into a gap between two consecutive arrivals in an arrival-type process that is not necessarily described by a Poisson model, Figure 1.\n\n\n\n\n\n\nFigure 1: Illustration of the random incidence phenomenon.\n\n\n\nWe are not saying that \\(t^*\\) is random; in this regard, perhaps, using the term random incidence may appear misleading. However, the interval between the time of the previous arrival \\(t_p\\) and \\(t^*\\), and the interval between \\(t^*\\) and the time of the next arrival \\(t_n\\) are random. All we need to state to start our discussion is to assume that the observer enters the arrival-type process in a situation when a previous arrival has surely occurred: probabilistically, the gap \\(t_n-t_p\\) is then a well-defined quantity.\nSuppose that we know the probability law of the interarrival times \\(Y\\). Let us denote by \\(W\\) the random variable that describes the duration of the gap entered by random incidence, \\(W=T_n-T_p\\). Finally, we denote by \\(T\\) the random variable that describes the waiting time for the next arrival from when the gap is entered by random incidence, \\(T=T_n-T^*\\).\nIt is argued that the probability that \\(W\\) assumes a value between \\(w\\) and \\(w+dw\\) is proportional to both the the duration of the gap \\(w\\) and the relative frequency of occurrence of such gaps \\(p_Y(w)dw\\):\n\\[\n\\text{Pr}(w\\leq W\\leq w+dw)=p_W(w)dw\\propto w\\,p_Y(w)dw\n\\]\nTherefore:\n\\[\np_W(w)=\\frac{w\\,p_Y(w)}{E[Y]}\n\\tag{1}\\]\nwhere the constant of proportionality is calculated to enforce the constraint of normalization for the PDF \\(p_W(w)\\) of \\(W\\) (its integral from 0 to infinity must be 1).\nNow, given that a gap of length \\(w\\) is entered by random incidence, we are equally likely to be anywhere within the gap. More precisely, given \\(w\\), the time until the next arrival \\(T\\) has a uniform PDF:\n\\[\np_{T\\vert W}(t\\,\\vert\\,w)=\\frac{1}{w},\\;0\\leq t\\leq w\n\\tag{2}\\]\nwhere \\(p_{T\\vert W}(t\\,\\vert\\,w)\\) is the conditional PDF of \\(T\\) given \\(W\\). Using Equation 1, Equation 2 the joint PDF \\(p_{TW}(t,w)\\) can be written:\n\\[\np_{TW}(t,w)=p_{T\\vert W}(t\\,\\vert\\,w)\\,p_W(w)=\\frac{p_Y(w)}{E[Y]},\\;0\\leq t\\leq w&lt;\\infty\n\\]\nFinally, the marginal PDF \\(p_T(t)\\) of the waiting time for the next arrival from when the gap is entered by random incidence can be formed simply by integrating out \\(W\\):\n\\[\n\\boxed{p_T(t)=\\int_{t}^{\\infty}\\frac{p_Y(w)}{E[Y]}dw=\\frac{1-F_Y(t)}{E[Y]},\\;t\\geq0}\n\\tag{3}\\]\n\nExample 1 Consider a bus passenger arriving at a bus stop. The probabilistic law of the bus headways \\(F_Y(y)\\) will determine the probability law for the waiting time until the next bus arrives, \\(p_T(t)\\) via Equation 3, if we ignore interactions between successive buses and assume that the arrivals are identically distributed and independent.\nSuppose that buses maintain perfect headways, being always spaced \\(T_0\\) minutes apart:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad y&lt;T_0\\\\1&\\quad y\\geq T_0\\end{split}\\right.\\rightarrow p_Y(y)=\\delta(y-T_0)\\rightarrow E[Y]=T_0\n\\]\nwhere the PDF is expressed in terms of a delta function located at time \\(T_0\\). The PDF of \\(T\\) can be written:\n\\[\np_T(t)=\\left\\{\\begin{split}&\\frac{1}{T_0}&\\quad 0\\leq t\\leq T_0\\\\&0&\\quad t&gt;T_0\\end{split}\\right.\\rightarrow E[T]=\\frac{T_0}{2}\n\\]\nAs expected intuitively, the time until the next arrival, given random incidence, is uniformly distributed between \\(0\\) and \\(T_0\\), with mean value \\(E[T]=T_0/2\\): if \\(T_0=60\\) min the average waiting time is 30 min.\nNow, suppose that the bus headways are on the hour, and fifteen minutes after the hour. Thus, the interarrival times alternate between 15 and 45 minutes. If the bus passenger shows up at the bus stop at any time uniformly distributed within a hour, she/he has to wait for an average time of 15/2 min (with probability 1/4) and 45/2 min (with probability 3/4):\n\\[\nE[T]=\\frac{15}{2}\\cdot\\frac{1}{4}+\\frac{45}{2}\\cdot\\frac{3}{4}=18.75\\;\\text{min}\n\\]\nMore formally:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad0\\leq y&lt;15\\\\1/2&\\quad15\\leq y&lt;45\\\\1&\\quad y\\geq45\\end{split}\\right.\\rightarrow E[Y]=30\\;\\text{min}\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\left\\{\\begin{split}1/30&\\quad0\\leq t&lt;15\\\\1/60&\\quad15\\leq t&lt;45\\\\0&\\quad t\\geq45\\end{split}\\right.\\rightarrow E[T]=18.75\\,\\text{min}\n\\]\nOn average, the bus passenger has to wait longer than it might be expected taking into account \\(E[Y]\\) only, namely \\(E[Y]/2=15\\) min.\n\nThis is because an observer who arrives at an arbitrary time, the bus passenger in this example, is more likely to fall in a large rather than a small interarrival interval: large interarrival intervals tend therefore to determine longer waiting times!\n\n\n\nExample 2 Consider the (unrealistic) case that the bus headways are Poissonian. What does this mean? Basically, we state that the interarrival times are modeled by independent exponentially distributed random variables with rate \\(\\lambda\\) (the rate denotes the number of arrivals per unit time):\n\\[\nF_Y(y)=1-e^{-\\lambda t},\\;t\\geq 0\\rightarrow p_Y(y)=\\frac{F_Y(y)}{dy}=\\lambda e^{-\\lambda t},\\;t\\geq0\\rightarrow E[Y]=1/\\lambda\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\lambda e^{-\\lambda t},\\;t\\geq0\n\\]\nHence \\(T\\) is exponentially distributed with rate \\(\\lambda\\). The average time of waiting is \\(E[T]=1/\\lambda\\). No matter when the event occurred, the observer who arrives at an arbitrary time \\(t^*\\) (the bus passenger in this example) sees the Poisson process to start fresh at time \\(t^*\\), Figure 1. It is worth noting that, according to the properties of independence, start-fresh and memorylessness stated above, we can run a Poisson process either forwards or backwards in time, without any modification in its properties. Hence, not only \\(T_n-T^*\\), but also \\(T^*-T_p\\) is exponentially distributed with parameter \\(\\lambda\\). Moreover, \\(T_n-T^*\\) and \\(T_n-T^*\\) are independent. We have therefore established that the gap \\(W\\) entered by random incidence is the sum of two independent exponential random variables with parameter \\(\\lambda\\) and mean value \\(2/\\lambda\\). More formally, using Equation 1 we get:\n\\[\np_W(w)=\\lambda w\\,e^{-\\lambda w},\\;w\\geq0\n\\]\nWe recall that the random variable \\(X\\) Erlang of order \\(k\\) has PDF:\n\\[\np(x)=\\frac{\\lambda^kx^{k-1}e^{-\\lambda x}}{(k-1)!}\n\\]\nIn conclusion, the gap duration is a random variable Erlang of order 2.\n\nIn a similar fashion to Example 1, when landing into a Poisson process at an arbitrary time, we are more likely to fall in a large interarrival interval; therefore, the length of what we perceive as a typical interarrival interval is greater than it is in reality."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "I am an associate professor at the BioRobotics Institute of the Scuola Superiore Sant’Anna, in Pisa, Italy. In my posts, I attempt to explain various concepts that I studied, in my double role of teacher and researcher.\nThere have been two main reasons for me to start this blog: first, learning - since writing is a very effective tool against shallow understanding (mine!); second, I wish to help others who may be struggling with similar problems.\nMy teaching is in the general area of instrumentation & measurement, statistics, signal processing and data analysis. My research focuses on developing algorithms for wearable inertial-sensor-based systems applied to motion analysis and assessment of human performance.\nRecently, I started studying the theory of diffusive processes for the modeling of time series of human motion.\nFor a record of my publications, please visit this site. Feel free to reach out to me via mail.\n\n\nThis blog’s content is under the Creative Commons Attribution-ShareAlike 4.0 International License:\n\nAngelo Maria Sabatini’s blog by Angelo Maria Sabatini is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBased on a work at https://amsabatini.netlify.app"
  },
  {
    "objectID": "index.html#copyrightpermissions",
    "href": "index.html#copyrightpermissions",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "This blog’s content is under the Creative Commons Attribution-ShareAlike 4.0 International License:\n\nAngelo Maria Sabatini’s blog by Angelo Maria Sabatini is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBased on a work at https://amsabatini.netlify.app"
  }
]