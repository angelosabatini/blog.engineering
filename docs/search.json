[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Computational methods in engineering",
    "section": "",
    "text": "The Milstein method\n\n\n\n\n\n\n\n\n\n\n\nApril 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Lamperti transform\n\n\n\n\n\n\n\n\n\n\n\nApril 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic integration by parts\n\n\n\n\n\n\n\n\n\n\n\nApril 9, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn transforming random variables\n\n\n\n\n\n\n\n\n\n\n\nApril 8, 2024\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\n\n\n\n\n\nApril 4, 2024\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/stochastic_integration/index.html",
    "href": "posts/stochastic_integration/index.html",
    "title": "Stochastic integration by parts",
    "section": "",
    "text": "Consider a process \\(X(t)\\) whose increments satisfy the stochastic differential equation \\(dX(t)=f(t,W(t))\\,dW(t)\\). Putting this in the integral form:\n\\[\nX(t)=X(0)+\\int_0^tf(s,W(s))\\,dW(s)\n\\tag{1}\\]\nwhere \\(W(t)\\) is a Wiener process, whose initial value \\(W(0)\\) is zero with probability one: its mean value and covariance function are\n\\[\n\\left\\{\n\\begin{split}\n\\langle W(t)\\rangle&=0\\\\\n\\langle W(t_1)W(t_2)&=\\min(t_1,t_2)\n\\end{split}\n\\right.\n\\]\nIf we consider the partition \\(\\Pi_n=\\{0=t_0&lt;t_1&lt;\\cdots&lt;t_n=t\\}\\), then the right-hand side of Equation 1 can be written (Itô stochastic integral):\n\\[\n\\int_0^tf(s,W(s))\\,dW(s)=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}f(t_i,W(t_i))(W(t_{i+1})-W(t_i))\n\\]\nwhere the limit is taken when the size of the partition, namely \\(\\vert\\Pi_n\\vert=\\max(t_{i+1}-t_i)\\) goes to zero. Henceforth, we assume that the partition is uniform, namely \\(t_{i+1}-t_i=\\Delta t, i=1,2,\\cdots,n\\). The convergence of the limit is assumed in the mean-square sense.\n\n\n\n\n\n\nItô lemma\n\n\n\nOne important result can be obtained by considering the Itô integral:\n\\[\nI(T)=\\int_0^T(dW(t))^2=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}(\\Delta W(t_i))^2\n\\]\n\\(I(T)\\) is easily seen to be a Gaussian random variable, with mean value \\(T\\) (because of the property of independent increments in a Wiener process):\n\\[\n\\langle I(T)\\rangle=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}\\langle(\\Delta W(t_i))^2\\rangle=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}\\Delta t=\\lim_{n\\rightarrow\\infty}n(T/n)=T\n\\]\nand zero variance:\n\\[\n\\text{Var}(I(T))=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}\\langle(\\Delta W(t_i))^4\\rangle=\\lim_{n\\rightarrow\\infty}2n(T/n)^2=0\n\\]\nTherefore, the integral of all the \\((dW(t))^2\\) is deterministic. Formally we can write this as follows:\n\\[\n\\boxed{(dW)^2=dt}\n\\tag{2}\\]\n\n\nThe weird property of the Wiener process to have increments \\(\\Delta W\\) that scale down by \\(\\sqrt{\\Delta t}\\), forces to get rid of the ordinary chain rule of differential calculus. This has implications also in the way integration by parts can be carried out.\nConsider the process \\(F(t)=f(t)g(W(t))\\), and assume that \\(f(\\cdot)\\) and \\(g(\\cdot)\\) are differentiable. Using the product rule:\n\\[\n\\begin{split}\ndF(t)&=df(t)g(W(t))+f(t)dg(W(t))\\\\\n&=f^{\\prime}(t)g(W(t))dt+f(t)\\left[g^{\\prime}(W(t))dW(t)+\\frac{1}{2}g^{\\prime\\prime}(W(t))(dW(t))^2\\right]\\\\\n&=\\left[f^{\\prime}(t)g(W(t))+\\frac{1}{2}f(t)g^{\\prime\\prime}(W(t))\\right]dt+f(t)g^{\\prime}(W(t))dW(t)\n\\end{split}\n\\]\nWriting the relation in the integral form, we obtain a formula for stochastic integration by parts:\n\\[\n\\boxed{\\int_0^tf(s)g^{\\prime}(W(s))dW(s)=f(t)g(W(t))\\bigg\\vert^t_0-\\int_0^tf^{\\prime}(s)g(W(s))ds-\\frac{1}{2}\\int_0^tf(s)g^{\\prime\\prime}(W(s))ds}\n\\tag{3}\\]\nLet us consider the case when \\(f(t)=1\\). Equation 3 becomes:\n\\[\n\\int_0^tg^{\\prime}(W(s))dW(s)=g(W(t))\\bigg\\vert^t_0-\\frac{1}{2}\\int_0^tg^{\\prime\\prime}(W(s))ds\n\\tag{4}\\]\n\nExample 1 Calculate the Itô integral:\n\\[\n\\int_0^tW(s)dW(s)\n\\]\nIn this case, \\(f(t)=1\\) and \\(g(W(t))=W^2(t)/2\\). Equation 4 simplifies to:\n\\[\n\\int_0^tW(s)dW(s)=\\frac{1}{2}W^2(s)\\bigg\\vert^t_0-\\frac{1}{2}\\int_0^tds=\\frac{W^2(t)-t}{2}\n\\]\nIt is noted that the result differs from the one expected by applying the ordinary rule of calculus:\n\\[\n\\int_0^tWdW=\\frac{1}{2}W^2\\bigg\\vert^t_0=\\frac{W^2(t)}{2}\n\\]\n\nAnother interesting case is when \\(g(W(t))=W(t)\\). Equation 3 becomes:\n\\[\n\\int_0^tf(s)dW(s)=f(t)W(t)\\bigg\\vert^t_0-\\int_0^tf^{\\prime}(s)W(s)ds\n\\tag{5}\\]\n\nExample 2 Calculate the Itô integral \\(I(t)\\) using the definition in terms of Riemann-Stieltjes sums:\n\\[\nI(t)=\\int_0^tsdW(s)=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}t_i\\Delta W(t_i)\n\\]\nThis is a Gaussian random variable, whose mean value and variance are:\n\\[\n\\left\\{\n\\begin{split}\n\\langle I(t)\\rangle&=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}t_i\\langle\\Delta W(t_i)\\rangle=0\\\\\n\\text{Var}(I(t))&=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1}t_it_j\\langle\\Delta W(t_i)\\Delta W(t_j)\\rangle=\\int_0^ts^2ds=\\frac{t^3}{3}\n\\end{split}\n\\right.\n\\tag{6}\\]\n\n\nExample 3 Calculate the Itô integral \\(I(t)\\) using the formula for stochastic integration by parts:\n\\[\nI(t)=\\int_0^tsdW(s)\n\\]\nI.e., \\(f(t)=t\\) in Equation 5, which simplifies to:\n\\[\nI(t)=\\int_0^tsdW(s)=sW(s)\\bigg\\vert^t_0-\\int_0^tW(s)ds=tW(t)-\\underbrace{\\int_0^tW(s)ds}_{\\substack{\\text{integrated}\\\\\\text{Wiener process}}}\n\\]\nDenote the integrated Wiener process with \\(Z(t)\\):\n\\[\nI(t)+Z(t)=tW(t)\n\\tag{7}\\]\n\\(Z(t)\\) is a Gaussian random variable, whose mean value and variance are:\n\\[\n\\left\\{\n\\begin{split}\n\\langle Z(t)\\rangle&=\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^{n-1}\\langle W(t_i)\\rangle\\Delta t=0\\\\\n\\text{Var}(Z(t))&=\\int_0^tdu\\int_0^t\\langle W(u)W(v)\\rangle dv=2\\int_0^tdu\\int_0^uvdv=\\int_0^tu^2du=\\frac{t^3}{3}\n\\end{split}\n\\right.\n\\tag{8}\\]\n\n\nExample 4 Calculate the covariance between \\(I(t)=\\int_0^tsdW(s)\\) and \\(Z(t)=\\int_0^tW(s)ds\\).\nWe know that \\(I(t)+Z(t)=tW(t)\\), see Equation 7:\n\\[\n\\text{Cov}(I(t)+Z(t),I(t)+Z(t))=\\text{Var}(tW(t))=t^3\n\\]\nOn the other hand:\n\\[\n\\text{Var}(I(t))+\\text{Var}(Z(t))+2\\text{Cov}(I(t),Z(t))=t^3\n\\]\nUsing Equation 6 and Equation 8 we conclude:\n\\[\n\\text{Cov}(I(t),Z(t))=\\frac{t^3}{6}\n\\]\nThe processes \\(I(t)\\) and \\(Z(t)\\) are thus not independent."
  },
  {
    "objectID": "posts/resolution/index.html",
    "href": "posts/resolution/index.html",
    "title": "Frequency resolution of spectral analysis",
    "section": "",
    "text": "Frequency resolution is the size of the smallest frequency for which details in the frequency response and the spectrum can be resolved by the estimate. For example, a resolution of 0.1 Hz means that the frequency response variations at frequency intervals at or below 0.1 Hz cannot be resolved.\nConsider an analog band-limited signal \\(x(t)\\) with bandwidth \\(B\\) Hz; \\(x(t)\\) is observed over a sample period of \\(T_r\\) s and sampled at a sampling frequency of \\(f_s\\) Hz (\\(T_s=1/f_s\\) denotes the sampling interval). The total number of samples of \\(x(t)\\) is then \\(N=\\lfloor T_r/T_s\\rfloor\\), where \\(\\lfloor\\cdot\\rfloor\\) denotes the floor function (or greatest integer function), namely the function that takes as input a real number \\(r\\) and gives as output the greatest integer less than or equal to \\(r\\).\nAccording to the Shannon-Nyquist sampling theorem, if the sampling frequency \\(f_s\\) is chosen to be \\(2B\\), the maximum resolvable frequency is:\n\\[\nf_{\\text{max}}=\\frac{f_s}{2}\n\\]\nOn the other hand, the minimum resolvable frequency is inversely related to the sample period:\n\\[\nf_{\\text{min}}=\\frac{1}{T_r}=\\frac{1}{NT_s}\n\\tag{1}\\]\nand hence the number of frequencies that can be resolved from \\(f_{\\text{min}}\\) to \\(f_{\\text{max}}\\) is\n\\[\nN_f=\\frac{f_{\\text{max}}-f_{\\text{min}}}{\\Delta f}\n\\]\nwhere \\(\\Delta f\\) is the frequency resolution. Since \\(\\Delta f=f_{\\text{min}}\\), we also have:\n\\[\nN_f=\\dfrac{\\dfrac{f_s}{2}-\\dfrac{f_s}{N}}{\\dfrac{f_s}{N}}=\\frac{N}{2}-1\n\\]\nThis implies that will be \\(N/2\\) discrete frequencies from \\(0\\) to \\(f_{\\text{max}}\\).\nThe accurate detection of frequency components in the spectrum \\(X(f)\\) of the signal \\(x(t)\\) is challenged by the phenomenon known as amplitude ambiguity; it consists of ambiguous and false amplitudes occurring in \\(X(f)\\) whenever the sample period \\(T_r\\) is not an integer multiple of all of the contributory periods in \\(x(t)\\). That is, false or ambiguous amplitudes will occur at frequencies that are immediately adjacent to the actual frequency.\nFor aperiodic signals, \\(T_r\\) theoretically must be infinite. For periodic signals, \\(T_r\\) must be equal to the least common integer multiple of all the periods contained in the signal. When this condition cannot be fulfilled, windowing is the elective method to minimize the effect of amplitude ambiguity (not used herein). Application of the DFT (Discrete Fourier Transform) or FFT (Fast Fourier Transform) to an aperiodic signal implicitly assumes that the signal is infinite in length and formed by repeating the signal of length \\(T_r\\) an infinite number of times. This leads to discontinuities in the amplitude that occur at each integer multiple of \\(T_r\\). These discontinuities are step-like, which introduce false amplitudes that decrease around the main frequencies.\n\n\n\n\n\n\nCan discrete-time sinusoids be non-periodic?\n\n\n\nThe normalized frequency \\(f\\) in cycles per sample of a discrete-time sinusoid:\n\\[\nx[n]=\\cos(2\\pi fn)\n\\]\nis restricted to values in the interval \\(-1/2\\leq f\\leq1/2\\). This is because, for any discrete-time sinusoid with frequency \\(\\vert f\\vert&gt;1/2\\), an integer \\(m\\) exists such that \\(f=f_0+m\\) and \\(\\vert f_0\\vert\\leq1/2\\):\n\\[\n\\cos 2\\pi fn=\\cos[2\\pi (f_0+m)\\,n]=\\cos(2\\pi f_0n)+\\cos(2\\pi mn)=\\cos(2\\pi f_0n)\n\\]\nIt is worth noting that the highest rate of oscillation of discrete-time sinusoids is when, at every time instant, the output sample flips with respect to the previous one:\n\\[\nf_0=\\frac{1}{2}\\rightarrow\\cos(2\\pi f_0n)=\\cos(\\pi n)=(-1)^n\n\\]\nMoreover, a discrete-time sinusoid is periodic if its frequency can be expressed in terms of a rational number; an additive mixture of periodical discrete-time sinusoids (called here sinusoidal mixture) is periodic with period equal to the least common integer multiple of their periods.\n\n\nAs outlined above, a spectrum line at frequency \\(f\\) will be accurately represented by the DFT when \\(f_s&gt;2f_{\\text{max}}\\) and \\(T_r = mT\\), where \\(m=1,2,\\cdots\\), and \\(T=1/f\\). This implies that \\(N=mf_s/f\\). If \\(T_r\\) is not an integer multiple of \\(T\\), leakage will occur in the DFT. This appears as amplitudes at \\(f\\) spilling onto adjacent frequencies.\nAs an example, consider a sinusoidal mixture with three unit-amplitude components at 2.875 Hz, 3 Hz, 3.125 Hz. The sampling frequency is set to \\(f_s=16\\) Hz and the sample period to \\(T=8\\) s (\\(N=128\\)): the component at 2.875 Hz is observed for 23 periods, the component at 3 Hz for 24 periods, and the component at 3.125 Hz for 25 periods. The resolution calculated according to Equation 1 is 0.125 Hz.\nA chunk of code written in MATLAB (Natick, Massachusetts: The MathWorks, Inc., https://www.mathworks.com) shows how to simulate the sinusoidal mixture and perform spectral analysis.\n% *****************************\n% sinusoidal mixture generation\n% *****************************\n\nfs = 16;   % sampling frequency, Hz\nTs = 1/fs; % sampling interval, s\nT  = 8;    % sample period, s\n\nt  = (0:Ts:T-Ts); % time domain, s\nN  = length(t);   % number of samples\n\nf1 = 2.9375;      % frequency component 1, Hz\nf2 = 3;           % frequency component 2, Hz\nf3 = 3.0625;      % frequency component 3, Hz\n\nx1 = cos(t.*(2*pi*f1)); % sinusoid 1\nx2 = cos(t.*(2*pi*f2)); % sinusoid 2\nx3 = cos(t.*(2*pi*f3)); % sinusoid 3\nX  = x1 + x2 + x3;      % sinusoidal mixture\n\n% ********************************************************\n% calculation of single-side spectrum (see comments below)\n% ********************************************************\n\nP              = fft(X, N);         % FFT calculation\nP2             = abs(P/N);          % double-side spectrum, rescaled by N     \nP1             = P2(1:N/2+1);  \nP1(:, 2:end-1) = 2*P1(2:end-1); \nY              = P1(1:end-1);       % single-side spectrum\nf              = (0:N/2-1).*(fs/N); % frequency domain, Hz\n\n\n\n\n\n\nComments\n\n\n\n\nConvert the FFT spectrum P of the signal X to the single-sided amplitude spectrum. Because the FFT function includes a scaling factor N between the original and the transformed signals, rescale P by dividing by N.\nTake the complex magnitude of the FFT spectrum. The two-sided amplitude spectrum P2, where the spectrum in the positive frequencies is the complex conjugate of the spectrum in the negative frequencies, has half the peak amplitudes of the time-domain signal.\nTo convert to the single-sided spectrum, take the first half of the two-sided spectrum P2. Multiply the spectrum in the positive frequencies by 2. P1(1) and P1(end) are not multiplied by 2 because these amplitudes correspond to the zero and Nyquist frequencies, respectively, and they do not have the complex conjugate pairs in the negative frequencies.\nDefine the frequency domain f for the single-side spectrum Y.\n\n\n\nThe single-sided spectrum of the sinusoidal mixture is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Single-sided spectrum of the sinusoidal mixture composed of three components at 2.875 Hz, 3 Hz, 3.125 Hz; sampling frequency: 16 Hz; sample period: 8 s. For the sake of visualization the frequency interval shown is limited to 2.5-3.5 Hz.\n\n\n\nConsider now the case that the three components of the sinusoidal mixture have frequencies 2.9375 Hz, 3 Hz, 3.0625 Hz. To resolve them, we would need to double the frequency resolution of the FFT, with respect to the scenario of Figure 1. Erroneously, I may try to double the sampling frequency, Figure 2: this expedient leaves unaltered the frequency resolution.\n\n\n\n\n\n\nFigure 2: Single-sided spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 32 Hz; sample period: 8 s.\n\n\n\nTo resolve the components of the sinusoidal mixture and avoid amplitude ambiguity, the frequency resolution of 0.0625 Hz can be achieved by extending the time period to \\(T_r=16\\) s, which corresponds to \\(N=256\\) samples at the original sampling frequency \\(f_s=16\\) Hz, see Figure 3.\n\n\n\n\n\n\nFigure 3: Single-sided spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 16 Hz; sample period: 16 s.\n\n\n\n\nTo conclude: whereas the sampling frequency sets the time resolution, the sample period occupies a central place in setting the frequency resolution when spectral analysis is performed. The sample period being the same, just increasing the sampling frequency simply increases the number of samples by the same ratio, leaving their ratio unaltered. The only way to increase the frequency resolution is to increase the sample period."
  },
  {
    "objectID": "posts/change_of_variables/index.html",
    "href": "posts/change_of_variables/index.html",
    "title": "On transforming random variables",
    "section": "",
    "text": "Consider a random vector \\(\\mathbf{X}\\) in the \\(d\\)-dimensional space, whose components \\(\\{X_i,i=1,\\cdots,d\\}\\) are independent normal random variables. To start with, the components are assumed to follow standard normal distributions, namely their common variance is one. The probability density function (PDF) of \\(\\mathbf{X}\\) is\n\\[\np_\\mathbf{X}(x_1,\\cdots,x_d)=\\frac{1}{(2\\pi)^{d/2}}\\exp\\left(-\\frac{1}{2}\\sum_{i=1}^dx^2_i\\right)\n\\]\nWhat is the PDF of the modulus squared of \\(\\mathbf{X}\\)? We are searching for the CDF of the random variable \\(Q\\):\n\\[\nQ=\\sum_{i=1}^dX_i^2\n\\]\nWe indicate with \\(F_Q(q)\\) the cumulative distribution function (CDF):\n\\[\nF_Q(q)=\\text{Pr}(Q\\leq q)=\\text{Pr}\\left(\\sum_{i=1}^dX_i^2\\leq q\\right)\n\\tag{1}\\]\nThe probability in Equation 1 can be computed as follows:\n\\[\nF_Q(q)=\\int_{\\sum_ix_i^2\\leq q}\\cdots\\int p_\\mathbf{R}(x_1,x_2,\\cdots,x_d)\\,dx_1\\,dx_2\\cdots dx_d\n\\]\nWe can pass to spherical coordinates by setting:\n\\[\n\\sum_{i=1}^dx_i^2=r\n\\]\nBecause of the spherical symmetry of both the multivariate Gaussian PDF and the domain of integration (a hypersphere), the integrand is angle-independent and we need to integrate just over the radius of the hypersphere. It is known from differential geometry that an element of volume \\(dV\\) integrated over the angles in the \\(d\\)-dimensional space is equal to:\n\\[\ndV=Dr^{d-1}dr\n\\tag{2}\\]\nwhere \\(D\\) is a constant. Therefore:\n\\[\nF_Q(q)=F_0\\int_0^{\\sqrt{q}}\\exp(-r^2/2)\\,r^{d-1}dr\n\\]\nIf we change variable in the integral:\n\\[\nr=\\sqrt{q^\\prime},\\quad dr=\\frac{1}{2\\sqrt{q^\\prime}}dq^\\prime\n\\]\nand collect all the constants into \\(F_0^{\\prime}\\), we obtain:\n\\[\nF_Q(q)=F_0^\\prime\\int_0^q\\exp(-q^\\prime/2)\\,(q^{\\prime})^{\\,d/2-1}dq^\\prime\n\\]\nThe PDF of \\(Q\\) is then\n\\[\np_Q(q)=F_0^\\prime\\exp(-q/2)\\,q^{\\,d/2-1}\n\\]\nThe constant \\(F_0^{\\prime}\\) can be derived from the condition of normalization:\n\\[\nF_0^\\prime\\int_0^{\\infty}\\exp(-q/2)\\,q^{\\,d/2-1}dq=1\n\\]\nRecall the definition of the gamma function:\n\\[\n\\Gamma(p)=\\int_0^{\\infty}\\exp(-x)x^{p-1}dx\n\\]\n\n\n\n\n\n\nGamma function and normalization factor\n\n\n\nSome useful properties of the gamma function to remember are the following:\n\\[\n\\begin{split}\n\\Gamma(1)&=1\\\\\n\\Gamma(1/2)&=\\sqrt{\\pi}\\\\\n\\Gamma(p+1)&=p\\,\\Gamma(p)\\\\\n\\Gamma(p+1)&=p!\\quad p\\in\\mathbb{N}\\\\\n\\Gamma(p+1)&=p(p-1)(p-2)\\cdots\\frac{3}{2}\\cdot\\frac{1}{2}\\sqrt{\\pi}\\quad p\\;\\text{half-integer}\n\\end{split}\n\\]\nFor the calculations to follow, related to the cases when \\(d=1,2,3\\), the normalization factor \\(1/(2^{d/2}\\Gamma(d/2))\\) in Equation 3 is equal to\n\\[\n\\begin{split}\nd=1\\rightarrow&\\;F_0^\\prime=\\frac{1}{\\sqrt{2\\pi}}\\\\\nd=2\\rightarrow&\\;F_0^\\prime=\\frac{1}{2}\\\\\nd=3\\rightarrow&\\;F_0^\\prime=\\frac{1}{\\sqrt{2\\pi}}\n\\end{split}\n\\]\n\n\nIn conclusion:\n\\[\n\\boxed{\\,p_Q(q)=\\frac{1^{\\phantom{'}}}{2^{d/2}\\Gamma(d/2)}\\exp(-q/2)\\,q^{d/2-1},\\quad q\\geq 0\\,_{\\phantom{'}}}\n\\tag{3}\\]\nThis is the PDF of the \\(\\chi\\)-squared random variable with \\(d\\) degrees of freedom. The mean value and the variance of \\(Q\\) are:\n\\[\n\\left\\{\n\\begin{split}\nE[Q]&=d\\\\\n\\text{Var}(Q)&=2d\n\\end{split}\n\\right.\n\\tag{4}\\]\n\n\n\n\n\n\nFunctions of a random variable\n\n\n\nLet \\(X\\) be a continuous random variable with PDF \\(p_X(x)\\) and let \\(Z\\) be the random variable depending on \\(X\\) through the functional relation:\n\\[\nY=f(X)\n\\]\nFor each \\(z\\in\\mathbb{R}\\), find the values of \\(x\\) that solve the equation \\(x=f^{-1}(z)\\) (we assume that \\(f^{-1}(\\cdot)\\) exists and that it is differentiable):\n\\[\nx_i=f^{-1}(z), i=1,\\cdots,n\n\\]\nIt can be proven that:\n\\[\n\\boxed{p_Z(z)=\\sum_{i=1}^n\\dfrac{p_X(x_i)}{\\vert f^{'}(x_i)\\vert}}\n\\tag{5}\\]\nThe analogous rule in the multivariate case \\(\\mathbf{Z}=\\mathbf{f}(\\mathbf{X})\\) (\\(\\mathbf{Z}\\in\\mathbb{R}^{m},\\mathbf{X}\\in\\mathbb{R}^{m}\\)) is written:\n\\[\n\\boxed{p_\\mathbf{Z}(\\mathbf{z})=\\sum_{i=1}^n p_\\mathbf{X}(\\mathbf{x}_i)\\big\\vert\\det\\mathbf{J}(\\mathbf{x}_i)\\big\\vert}\n\\tag{6}\\]\nwhere \\(\\mathbf{x}_i=\\mathbf{f}^{-1}(\\mathbf{z}),i=1,\\cdots n\\). If all functions \\(f_i(\\cdot)\\) are assumed to be differentiable, the \\(ij\\)-element of the \\(m\\times m\\) Jacobian matrix \\(\\mathbf{J}\\) is given by:\n\\[\nJ_{ij}=\\frac{\\partial f_i^{-1}}{\\partial z_j}\n\\tag{7}\\]\nThe Jacobian matrix is calculated for each \\(\\mathbf{x}_i=\\mathbf{f}^{-1}(\\mathbf{z})\\).\nIt is worth noting that the calculation of the integrals over the angular coordinates in Equation 2 can be managed by passing from Cartesian coordinates to polar coordinates (\\(d=2\\)) or spherical coordinates (\\(d=3\\)), which requires to consider, in place of the differential element \\(dx\\,dy\\) in \\(\\mathbb{R}^2\\), the differential element \\(\\vert\\det\\mathbf{J}(r,\\theta)\\vert dr\\,d\\theta\\) and, in place of the differential element \\(dx\\,dy\\,dz\\) in \\(\\mathbb{R}^3\\), the differential element \\(\\vert\\det\\mathbf{J}(r,\\phi,\\theta)\\vert dr\\,d\\phi\\,d\\theta\\) (see below for details about the calculations).\n\n\nNow, we want to consider specifically the following three cases.\nUnidimensional case (\\(d=1\\))\nApparently, we do not have any angle over which to integrate. However, we have:\n\\[\ndV=2r\\,dr\n\\]\nNamely in Equation 2, we have \\(D=2\\):\n\\[\nq=r^2\\rightarrow dq=2r\\,dr\n\\]\nThe reason for the factor \\(D=2\\) is that, in geometrical terms, for any vector aligned in one direction, we always have another vector, with the same modulus, oriented in the opposite direction.\nWe can take another route for the calculation. For \\(d=1\\), the density of the modulus squared \\(Q\\) can be obtained from the density of the standard normal variable by using Equation 5 for the case when \\(Q=X^2\\). Inverting \\(f(\\cdot)\\) gives rise to two values for \\(x\\), namely \\(x_1=\\sqrt{q}\\) and \\(x_2=-\\sqrt{q}\\):\n\\[\np_Q(q)=\\dfrac{p_X(\\sqrt{q})}{\\vert f^{'}(\\sqrt{q})\\vert}+\\dfrac{p_X(-\\sqrt{q})}{\\vert f^{'}(-\\sqrt{q})\\vert}\n\\]\nIn conclusion:\n\\[\n\\boxed{p_Q(q)=\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\sqrt{q}}\\exp(-q/2),\\quad q\\geq 0}\n\\]\nThis is the same as Equation 3, with \\(d=1\\) (\\(F_0^{\\prime}=1/\\sqrt{2\\pi}\\)). The mean value and the variance of the random variable \\(Q\\) are, see Equation 4:\n\\[\n\\left\\{\n\\begin{split}\nE[Q]&=1\\\\\n\\text{Var}(Q)&=2\n\\end{split}\n\\right.\n\\]\nBidimensional case (\\(d=2\\))\nWhen \\(d=2\\), the Cartesian coordinates \\(x=x_1,y=x_2\\) can be expressed in terms of the polar coordinates \\((r,\\theta)\\) using the algorithm, see Figure 1:\n\\[\n\\left\\{\n\\begin{split}\nx&=r\\cos\\theta\\\\\ny&=r\\sin\\theta\n\\end{split}\n\\right.\n\\tag{8}\\]\n\n\n\n\n\n\nFigure 1: The generic point \\(P\\) is identified by the two coordinates \\(r,\\theta\\).\n\n\n\nThe Jacobian associated to the transformation is written:\n\\[\nJ(r,\\theta)=\\begin{bmatrix}\\dfrac{\\partial x}{\\partial r}&\\dfrac{\\partial x}{\\partial\\theta}\\\\\\dfrac{\\partial y}{\\partial r}&\\dfrac{\\partial y}{\\partial\\theta}\\end{bmatrix}=\\begin{bmatrix}\\cos\\theta&-r\\sin\\theta\\\\\\sin\\theta&r\\cos\\theta\\end{bmatrix}\\rightarrow\\vert\\det J(r,\\theta)\\vert=r\n\\]\nThe integral Equation 2 is then:\n\\[\ndV=\\int_0^{2\\pi}r\\,d\\theta\\,dr=2\\pi r\\,dr\\rightarrow D=2\\pi\n\\]\nThe \\(\\chi\\) squared density with two degrees of freedom is:\n\\[\n\\boxed{p_Q(q)=\\frac{1}{2}\\exp(-q/2),\\quad q\\geq 0}\n\\]\nThe mean value and the variance of the random variable \\(Q\\) are, see Equation 4:\n\\[\n\\left\\{\n\\begin{split}\nE[Q]&=2\\\\\n\\text{Var}(Q)&=4\n\\end{split}\n\\right.\n\\]\nTridimensional case (\\(d=3\\))\nWhen \\(d=3\\), the Cartesian coordinates \\(x=x_1,y=x_2,z=x_3\\) can be expressed in terms of the spherical coordinates \\((r,\\phi,\\theta)\\) (\\(r\\geq0,\\phi\\in[0,2\\pi],\\theta\\in[0,\\pi]\\)) using the algorithm, see Figure 2.\n\n\n\n\n\n\nFigure 2: The generic point \\(P\\) is identified by the three coordinates \\(r,\\phi,\\theta\\).\n\n\n\n\\[\n\\left\\{\n\\begin{split}\nx&=r\\sin\\theta\\cos\\phi\\\\\ny&=r\\sin\\theta\\sin\\phi\\\\\nz&=r\\cos\\theta\n\\end{split}\n\\right.\n\\tag{9}\\]\nThe Jacobian associated to the transformation is written:\n\\[\nJ(r,\\theta)=\\begin{bmatrix}\\dfrac{\\partial x}{\\partial r}&\\dfrac{\\partial x}{\\partial\\phi}&\\dfrac{\\partial x}{\\partial \\theta}\\\\\\dfrac{\\partial y}{\\partial r}&\\dfrac{\\partial y}{\\partial\\phi}&\\dfrac{\\partial y}{\\partial \\theta}\\\\\\dfrac{\\partial z}{\\partial r}&\\dfrac{\\partial z}{\\partial\\phi}&\\dfrac{\\partial z}{\\partial \\theta}\\end{bmatrix}=\\begin{bmatrix}\\sin\\theta \\cos\\phi&-r\\sin\\theta \\sin\\phi&r \\cos\\theta \\cos\\phi\\\\\\sin\\theta \\sin\\phi&r\\sin\\theta \\cos\\phi&r \\cos\\theta \\sin\\phi\\\\\\cos\\theta&0&-r \\sin\\theta\n\\end{bmatrix}\\rightarrow\\vert\\det J(r,\\theta)\\vert=r^2\\sin\\theta\n\\]\nThe integral Equation 2 is then:\n\\[\ndV=\\int_0^{2\\pi}d\\phi\\int_0^{\\pi}d\\theta \\sin\\theta\\,r^2\\,dr=4\\pi r^2\\,dr\\rightarrow D=4\\pi\n\\]\nThe \\(\\chi\\) squared density with three degrees of freedom is:\n\\[\n\\boxed{p_Q(q)=\\sqrt{\\dfrac{q}{2\\pi}}\\exp(-q/2),\\quad q\\geq 0}\n\\]\nThe mean value and the variance of the random variable \\(Q\\) are, see Equation 4:\n\\[\n\\left\\{\n\\begin{split}\nE[Q]&=3\\\\\n\\text{Var}(Q)&=6\n\\end{split}\n\\right.\n\\]\nRather than the modulus squared \\(Q\\) of \\(\\mathbf{R}\\), we can be interested in determining the PDF of the modulus \\(R\\). We consider the transformation \\(R=\\sqrt{Q}\\) and the rule exposed in Equation 5\n\\[\n\\boxed{p_R(r)=\\dfrac{p_Q(r^2)}{1/2r}=\\frac{2}{2^{d/2}\\Gamma(d/2)}\\exp(-r^2/2)\\,r^{d-1},\\quad r\\geq 0\\,_{\\phantom{'}}}\n\\]\nIn Table 1 we report the expression of the PDF of \\(R\\) in the three spaces \\(\\mathbb{R},\\mathbb{R}^2,\\mathbb{R}^3\\) examined so far. For the sake of broader generality, the PDFs are written for the case when the components of \\(\\mathbf{R}\\) are non-standard, namely their common variance is not one, but \\(\\sigma^2\\): in the calculation, we need to change \\(r^{\\prime}\\) in \\(r^{\\prime}=r/\\sigma\\), with a factor \\(1/\\sigma\\) appearing in the expression of the PDF (this is due to \\(dr^{\\prime}=dr/\\sigma\\)).\n\n\n\nTable 1: PDF of the modulus of random vectors whose components are independent, normally distributed with common variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\n\ndimension\nname\nPDF\n\n\n\n\n\\(d=1\\)\n1D-Maxwell\n\\(p_R(r)=\\dfrac{2}{\\sqrt{2\\pi}\\sigma}\\exp(-r^2/2\\sigma^2)\\)\n\n\n\\(d=2\\)\nRayleigh\n\\(p_R(r)=\\dfrac{r}{\\sigma^2}\\exp(-r^2/2\\sigma^2)\\)\n\n\n\\(d=3\\)\n3D-Maxwell\n\\(p_R(r)=\\dfrac{2}{\\sqrt{2\\pi}}\\dfrac{r^2}{\\sigma^3}\\exp(-r^2/2\\sigma^2)\\)\n\n\n\n\n\n\n\nExercise 1 Find the energy density of the molecules of an ideal gas at the absolute temperature \\(T\\).\nIn an ideal gas the velocity components \\(V_x,V_y,V_z\\) of the molecules are random variables that satisfy the condition of the central limit theorem. Hence, thee components are independent normal. Their common variance \\(\\sigma^2\\) is:\n\\[\n\\sigma^2=\\dfrac{k_BT}{m}\n\\]\nwhere \\(m\\) is the mass of the molecules. This relationship comes from the thermodynamics equation that links the variance and temperature.\nWe can further consider the relationship existing between kinetic energy and velocity (equipartition law):\n\\[\nE=\\dfrac{1}{2}mv^2,\\quad v=\\sqrt{\\dfrac{2E}{m}},\\quad mv\\,dv=dE\n\\]\nThe energy density is then:\n\\[\np_E(E)=\\dfrac{p_V(\\sqrt{2E/m})}{m\\sqrt{2E/m}}\n\\]\nWe also know that the modulus of the velocity follows the Maxwellian distribution. Therefore:\n\\[\n\\boxed{p_E(E)=\\dfrac{2}{\\sqrt{\\pi}}\\dfrac{1}{k_BT}\\sqrt{\\dfrac{E}{k_BT}}\\exp(-E/k_BT)}\n\\]\nThis is the density of the Boltzmann distribution.\n\n\nExercise 2 Randomly generate points uniformly distributed in a circle of radius \\(R\\), with constant density \\(\\rho\\). The circle is assumed to be centered in the origin of the Cartesian coordinate system.\nTo define the position of a generic point \\(P\\) within a circle, we use the polar coordinates \\((r,\\theta)\\):\n\\[\n\\left\\{\n\\begin{split}\n&0\\leq r\\leq R\\\\\n&0\\leq\\theta\\leq 2\\pi\n\\end{split}\n\\right.\n\\]\nTo determine the PDFs \\(p(r)\\) and \\(p(\\theta)\\), we first observe that \\(p(\\theta)\\,d\\theta\\) is given by the ratio between the number of points contained in the circular sector with central angle \\(d\\theta\\) and the total number of points within the circle. On the other hand, \\(p(r)\\,dr\\) is given by the ratio between the number of points within the annulus of width \\(dr\\) and radius \\(r\\) and the total number of points within the circle, see Figure 3:\n\\[\n\\left\\{\n\\begin{split}\np(\\theta)d\\theta&=\\dfrac{\\rho(d\\theta/2)R^2}{\\rho\\pi R^2}=\\dfrac{d\\theta}{2\\pi}\\\\\np(r)dr&=\\dfrac{\\rho(2\\pi rdr)}{\\rho\\pi R^2}=\\dfrac{2r}{R^2}dr\n\\end{split}\n\\right.\n\\]\n\n\n\n\n\n\nFigure 3: When points are uniformly distributed in the circle, \\(p(\\theta)\\,d\\theta\\) and \\(p(r)\\,dr\\) can be easily derived by geometrical arguments.\n\n\n\nThe corresponding CDFs are:\n\\[\n\\left\\{\n\\begin{split}\n&F(\\theta)=\\int_0^{\\theta}p(\\theta)\\,d\\theta=\\dfrac{\\theta}{2\\pi}\\quad 0\\leq\\theta\\leq 2\\pi\\\\\n&F(r)=\\int_0^rp(r)\\,dr=\\dfrac{r^2}{R^2}\\quad 0\\leq r\\leq R\n\\end{split}\n\\right.\n\\]\nIf \\(\\xi_1\\) and \\(\\xi_2\\) are independent draws from a standard uniform pseudo-number generators:\n\\[\n\\left\\{\n\\begin{split}\n\\theta&=2\\pi\\xi_1\\\\\nr&=R\\sqrt{\\xi_2^{\\phantom{'}}}\n\\end{split}\n\\right.\n\\]\nThe points in Cartesian coordinates are calculated using Equation 8. Figure 4 shows the cloud of points generated from 5000 draws.\n\n\n\n\n\n\nFigure 4: Cloud of points distributed uniformly in a circle of radius $R$ ($R=1$).\n\n\n\nIt is worth noting that an isotropic point distribution in the circle implies uniformity in \\(\\theta\\) but not in \\(r\\). Uniformity in \\(r\\) would imply to have the same number of points for two circular sectors with different radii and then a higher density for the one closer to the center of the sphere.\n\n\nExercise 3 Randomly generate isotropically distributed on a spherical surface of radius \\(R\\) and uniformly within the spherical volume. The sphere is assumed to be centered in the origin of the Cartesian coordinate system.\nTo define the position of a generic point \\(P\\) located on a spherical surface, we use the spherical coordinates \\((r,\\phi,\\theta)\\):\n\\[\n\\left\\{\n\\begin{split}\n&0\\leq r\\leq R\\\\\n&0\\leq\\phi\\leq 2\\pi\\\\\n&0\\leq\\theta\\leq\\pi\n\\end{split}\n\\right.\n\\]\nIf we denote by \\(n_t\\) and \\(dn\\) the total number of points on the sphere and in the elemental volume \\(dV=r^2\\sin\\theta\\,dr\\,d\\phi\\,d\\theta\\), respectively, we have:\n\\[\np(r,\\phi,\\theta)=\\dfrac{3r^2\\sin\\theta}{4\\pi R^3}\n\\]\nThe marginal densities are given by:\n\\[\n\\left\\{\n\\begin{split}\np_R(r)&=\\dfrac{3r^2}{4\\pi R^3}\\int_0^{2\\pi}d\\phi\\int_0^\\pi\\sin\\theta\\,d\\theta=\\frac{3r^2}{R^3},&\\quad0\\leq r\\leq R\\\\\np_\\Phi(\\phi)&=\\dfrac{3}{4\\pi R^3}\\int_0^\\pi\\sin\\theta\\,d\\theta\\int_0^Rr^2dr=\\frac{1}{2\\pi},&\\quad 0\\leq\\phi\\leq 2\\pi\\\\\np_\\Theta(\\theta)&=\\dfrac{3\\sin\\theta}{4\\pi R^3}\\int_0^{2\\pi}d\\phi\\int_0^Rr^2dr=\\dfrac{\\sin\\theta}{2},&\\quad0\\leq\\theta\\leq\\pi\n\\end{split}\n\\right.\n\\]\nThe corresponding CDFs are:\n\\[\n\\left\\{\n\\begin{split}\nF_R(r)&=\\frac{r^3}{R^3},&\\quad0\\leq r\\leq R\\\\\nF_\\Phi(\\phi)&=\\dfrac{\\phi}{2\\pi},&\\quad 0\\leq\\phi\\leq 2\\pi\\\\\nF_\\Theta(\\theta)&=\\dfrac{1-\\cos\\theta}{2},&\\quad 0\\leq\\theta\\leq\\pi\n\\end{split}\n\\right.\n\\]\nThe formulae for the random generation of \\((r,\\phi,\\theta)\\) follow from three independent standard uniform draws \\(\\xi_1,\\xi_2\\) and \\(\\xi_3\\):\n\\[\n\\left\\{\n\\begin{split}\nr&=R^3\\xi_1^{1/3}\\\\\n\\phi&=2\\pi\\,\\xi_2\\\\\n\\theta&=\\arccos(1-2\\xi_3)\n\\end{split}\n\\right.\n\\]\nThe points in Cartesian coordinates are calculated using Equation 9. Figure 5 shows the cloud of points generated from 5000 draws.\n\n\n\n\n\n\nFigure 5: Cloud of points distributed uniformly on the spherical surface and uniformly within the spherical volume of radius \\(R\\) (\\(R=1\\))."
  },
  {
    "objectID": "posts/lamperti_transform/index.html",
    "href": "posts/lamperti_transform/index.html",
    "title": "The Lamperti transform",
    "section": "",
    "text": "Consider the stochastic differential equation (SDE):\n\\[\ndX_t=a(t,X_t)\\,dt+b(X_t)\\,dW_t\n\\tag{1}\\]\nwhere the diffusion coefficient \\(b(X_t)\\) depends only on the state variable. The Lamperti transform is defined as follows:\n\\[\nY_t=F(X_t)=\\int_{x^{\\prime}}^{X_t}\\frac{1}{b(u)}\\,du\n\\tag{2}\\]\nwhere \\(x^{\\prime}\\) is any value of the state variable \\(X\\) in its state space. We assume that the function \\(F(\\cdot)\\) defines a one-to-one mapping from the state space of \\(X_t\\) to \\(\\mathbb{R}\\).\nThe SDE solved by the process \\(Y_t\\) can be formulated by applying the Itô formula to the function:\n\\[\nf(t,x)=\\int_{x^{\\prime}}^x\\frac{1}{b(u)}du\n\\]\nWe calculate the derivatives of \\(f(t,x)\\) that are needed in the Itô formula:\n\\[\n\\left\\{\n\\begin{split}\nf_t(t,x)&=\\frac{\\partial}{\\partial t}f(t,x)=0\\\\\nf_x(t,x)&=\\frac{\\partial}{\\partial x}f(t,x)=\\frac{1}{b(x)}\\\\\nf_{xx}(t,x)&=\\frac{\\partial^2}{\\partial x^2}f(t,x)=-\\dfrac{\\dfrac{d}{dx}b(x)}{b^2(x)}=-\\frac{b_x(x)}{b^2(x)}\n\\end{split}\n\\right.\n\\tag{3}\\]\nThe Itô formula tells us that:\n\\[\n\\begin{split}\ndf(t,X_t)&=f_t(t,X_t)dt+f_x(t,X_t)dX_t+\\frac{1}{2}f_{xx}(t,X_t)(dX_t)^2\\\\\n&=\\frac{a(t,X_t)dt+b(X_t)dW_t}{b(X_t)}-\\frac{1}{2}\\frac{b_{x}(X_t)}{b^2(X_t)}(a(t,X_t)dt+b(X_t)dW_t)^2\\\\\n&=\\left[\\frac{a(t,X_t)}{b(X_t)}-\\frac{1}{2}b_x(X_t)\\right]dt+dW_t\n\\end{split}\n\\tag{4}\\]\nRecall that \\(Y_t=F(X_t)=f(t,X_t)\\) and \\(X_t=F^{-1}(Y_t)\\). Equation 4 can be written:\n\\[\ndY_t=a_Y(t,Y_t))dt+dW_t\n\\tag{5}\\]\nwhere:\n\\[\na_Y(t,y)=\\left[\\frac{a(t,F^{-1}(Y_t))}{b(F^{-1}(Y_t))}-\\frac{1}{2}b_x(F^{-1}(Y_t))\\right]\n\\]\n\nThe Lamperti transform changes the generic SDE Equation 1 into another SDE with a unitary diffusion coefficient, Equation 5.\n\n\nExample 1 Consider the SDE:\n\\[\ndX_t=-\\theta_1X_t\\,dt+\\theta_2\\sqrt{1+X_t^2}\\,dW_t\\\\\n\\]\nwhere \\(\\theta_1+\\theta_2^2/2&gt;0\\). It can be shown that, setting \\(\\nu=1+2\\theta_1/\\theta_2^2\\):\n\\[\nX_t\\propto t(\\nu)/\\sqrt{\\nu}\n\\]\nI.e., the stationary distribution of \\(X_t\\) is Student \\(t\\) with \\(\\nu\\) degrees of freedom. Applying the Lamperti transform:\n\\[\nF(x)=\\int_0^x\\frac{1}{\\theta_2\\sqrt{1+u^2}}du=\\frac{\\text{arcsinh}}{\\theta_2}\n\\]\nUsing Equation 4 we have:\n\\[\ndF(X_t)=-\\left[\\frac{\\theta_1}{\\theta_2}+\\frac{\\theta_2}{2}\\right]\\frac{X_t}{\\sqrt{1+X_t^2}}dt+dW_t\n\\]\nIn terms of the \\(Y_t\\) process we have:\n\\[\ndY_t=-\\left[\\frac{\\theta_1}{\\theta_2}+\\frac{\\theta_2}{2}\\right]\\text{tanh}(\\theta_2Y_t)+dW_t\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "I am associate professor at the BioRobotics Institute, Scuola Superiore Sant’Anna, Pisa, Italy. In my posts, I attempt to explain various concepts that I have learned while studying, teaching or researching - and I intend to do that without having any specific order or plan in mind.\nCurrently, the courses I teach are: “Methods and techniques of measurement and data analysis”, M.Sc. Bionics Engineering - a program jointly offered by University of Pisa and Scuola Superiore Sant’Anna; and “Introduction to statistics and data analysis”, a service course aimed at students of the Ph.D. School of BioRobotics - established at the BioRobotics Institute, Scuola Superiore Sant’Anna.\nMy research mainly focuses on the development of algorithms for wearable inertial-sensor-based systems to be used in motion analysis applied to assessment of human performance, robotics and the like. Recently, I started studying the theory of diffusive processes, with the aim to use related mathematical frameworks to model experimental time series of interest in the study of human motion.\nFor a record of my publications, please visit the following website: https://scholar.google.com/citations?user=cz_S3UkAAAAJ&hl=en"
  }
]