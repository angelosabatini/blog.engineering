[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStatistics and Machine Learning: A shared landscape\n\n\n\n\n\n\nstatistics\n\n\nmachine learning\n\n\n\nWhat does it mean to shift from statistical inference to prediction? This post uses a minimal but complete example to explore the shared terrain between statistics and machine learning — from how models are trained, to how decisions are made and evaluated in real-world contexts.\n\n\n\n\n\nMay 5, 2025\n\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge Deep Learning in R with a hybrid R–Colab workflow\n\n\n\n\n\n\nmachine learning\n\n\n\nTraining deep learning models in R is powerful—but it can be painfully slow on a single-CPU machine. This post shows how to blend the flexibility of R for data prep, visualization, and evaluation with the raw GPU power of Google Colab for fast model training. I show how to create a seamless workflow allowing continued use of familiar R tools, while letting Python handle the heavy lifting when it counts.\n\n\n\n\n\nApr 23, 2025\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the power of Apple Silicon for R: Parallel processing on M1/M2\n\n\n\n\n\n\nmachine learning\n\n\n\nParallel computation is a big deal in machine learning (ML), especially when working with large datasets, complex models (such as deep neural networks), or computationally intensive tasks like hyperparameter tuning. At its core, parallel computation means executing multiple calculations or processes simultaneously. In ML, this can significantly speed up training and inference by leveraging multiple processing units, such as CPU cores, GPUs, TPUs, or computing clusters. In this post, I’ll review some of the most popular methods available in R for running computations in parallel.\n\n\n\n\n\nApr 22, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Long Short Term Memory (LSTM) in R for time series forecasting\n\n\n\n\n\n\ntime series\n\n\n\nIn mid 2017, R launched Keras in R, a comprehensive library which runs on top of powerful numerical platforms, such as TensorFlow and Theano. This package helped R not to lag behind Python anymore when managing Deep Learning (DL) frameworks and libraries. Models supported by the R Keras package include Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Convoluted Neural Network (CNN), Multilayer Perceptron (MLP), among others. In this post, I discuss an example of using LSTM for time series forecasting using code written in R.\n\n\n\n\n\nMar 11, 2025\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency spectrum of a sine-wave tone burst\n\n\n\n\n\n\nsignal processing\n\n\n\nIn this post, I discuss the spectral properties of a well-known test signal, namely the sine-wave tone burst. The underlying theory is reviewed to explain the pattern of spectral lines that are produced when a sinusoidal signal at a given frequency is turned ‘on and off’ at a slower pace. Examples are presented to show the agreement between theoretical predictions and results of the FFT-based spectrum analysis.\n\n\n\n\n\nJan 16, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpell checking using hunspell\n\n\n\n\n\n\ntext mining\n\n\n\nThe hunspell package is a high-performance stemmer, tokenizer, and spell checker for R. LibreOffice, OpenOffice, Mozilla Firefox, Google Chrome, Mac OS-X, InDesign, Opera, RStudio and many others use this spell checker library, with support being provided for several languages, including Italian. Hunspell uses a special dictionary format that defines which characters, words and conjugations are valid in the specified language. In this post I will illustrate how to use the spell checker.\n\n\n\n\n\nJan 2, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCorrespondence analysis: Part II\n\n\n\n\n\n\nmultivariate statistics\n\n\n\nCorrespondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. Conceptually, CA is similar to principal component analysis, but applies to categorical rather than continuous data. In this post, I will briefly illustrate how simple CA (the method used when data of only two categorical variables are analyzed) can be computed using the R programming software (a brief overview of the underlying theory has been sketched in a previous post.)\n\n\n\n\n\nDec 10, 2024\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nCorrespondence analysis: Part I\n\n\n\n\n\n\nmultivariate statistics\n\n\n\nCorrespondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. Conceptually, CA is similar to principal component analysis, but applies to categorical rather than continuous data. In this post, I will briefly present the theory behind simple CA (the method used when data of only two categorical variables are analyzed), leaving details of how to carry the analysis using the R programming software to a future post.\n\n\n\n\n\nDec 9, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nAudio features for free\n\n\n\n\n\n\ndata mining\n\n\n\nSpotify is the leader in the audio streaming market, with its several million subscribers, including myself, and many more listeners who use the app for free. Although not necessarily known to ordinary users, each track in the Spotify library comes accompanied by an extensive list of numerical scores - the outcome of the audio analysis each track is submitted to using advanced algorithms developed by Spotify. No matter what the use is for them, these data, which are usually hidden to the user, can be retrieved using the Web API’s offered by Spotify. In this post I explain how this can be done.\n\n\n\n\n\nNov 12, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nNumerical simulation for stochastic differential equations\n\n\n\n\n\n\nstochastic calculus\n\n\n\nA down-to-the-bone post, where few issues concerning the numerical simulation of stochastic differential equations are discussed with just a limited amount of technical detail.\n\n\n\n\n\nJun 6, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nSignificant figures\n\n\n\n\n\n\nmeasurement\n\n\n\nThe use of calculators and computers leads to lab reports with far too many digits in every number produced. Assessing the correct number of significant figures is essential in reporting either experimental or computed results together with their stated uncertainties.\n\n\n\n\n\nMay 14, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nAugmented Dickey-Fuller test\n\n\n\n\n\n\ntime series\n\n\n\nIn statistics, an augmented Dickey–Fuller test (ADF) tests the null hypothesis of a unit root in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. This post explains how to use the ADF test in R, with an attempt to make the different test statistics clear and easily interpretable.\n\n\n\n\n\nMay 13, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGambler’s ruin\n\n\n\n\n\n\nprobability\n\n\n\nThe gambler’s ruin problem is often applied to gamblers with finite wealth playing against a bookie or casino assumed to have a much larger amount of wealth available, in principle infinite. It can then be proven that the probability of the gambler’s eventual ruin tends to 1 even in the scenario where the game is fair.\n\n\n\n\n\nMay 5, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate probability regions\n\n\n\n\n\n\nstatistics\n\n\n\nComputing probability regions in the space where sample data are assumed to live relates to the determination of regions that we are confident that the underlying population will occupy with the prescribed value of probability. After reviewing the theory for multivariate normal distributions, I present an example of application from the field of posturographic research.\n\n\n\n\n\nMay 2, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence intervals for proportions\n\n\n\n\n\n\nstatistics\n\n\n\nUsually confidence intervals for the estimation of proportions are based on methods that exploit the normal approximation to the binomial distribution. By simulation, two of these methods (Wilson and Wald) are tested for their ability to provide the stated coverage for small-to-large samples.\n\n\n\n\n\nApr 27, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\nprobability\n\n\n\nAlthough experimentalists are well familiar with the topic, nonetheless I believe it might be helpful to spend a few minutes for a review of basic formulae of combinatorial analysis.\n\n\n\n\n\nApr 24, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\nprobability\n\n\n\nIn this post, I briefly discuss the random incidence phenomenon, using the classical example of a person arriving at a bus stop at a random time, and waiting for the arrival of the next bus.\n\n\n\n\n\nApr 22, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\nsignal processing\n\n\n\nSpectral leakage and length of data record hamper our ability to resolve spectral lines by DFT/FFT analysis. In this post, I briefly discuss this problem, with examples using sinusoidal mixtures.\n\n\n\n\n\nApr 13, 2024\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Statistics and Machine Learning: A shared landscape\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\n\n\n\n\n\nSupercharge Deep Learning in R with a hybrid R–Colab workflow\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\nUnleashing the power of Apple Silicon for R: Parallel processing on M1/M2\n\n\n\n\n\n\n\n\nApr 22, 2025\n\n\n\n\n\n\n\nUsing Long Short Term Memory (LSTM) in R for time series forecasting\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\nFrequency spectrum of a sine-wave tone burst\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\n\n\n\n\n\nSpell checking using hunspell\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nCorrespondence analysis: Part II\n\n\n\n\n\n\n\n\nDec 10, 2024\n\n\n\n\n\n\n\nCorrespondence analysis: Part I\n\n\n\n\n\n\n\n\nDec 9, 2024\n\n\n\n\n\n\n\nAudio features for free\n\n\n\n\n\n\n\n\nNov 12, 2024\n\n\n\n\n\n\n\nNumerical simulation for stochastic differential equations\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\nSignificant figures\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\nAugmented Dickey-Fuller test\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\n\n\n\n\n\nGambler’s ruin\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nMultivariate probability regions\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\nConfidence intervals for proportions\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\n\n\nApr 13, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/random_incidence/index.html",
    "href": "posts/random_incidence/index.html",
    "title": "Random incidence",
    "section": "",
    "text": "Consider the situation when the continuous time axis of our observations is partitioned into a sequence of interarrival intervals. With the term arrival, we designate the occurrence of everything we are interested to observe, for instance, a particle emitted from radioactive material and captured by a radiation counter, a message reaching its destination queue, maybe the bus that we are waiting for at the bus stop. Usually, probabilistic models that attempt to describe these different type of arrivals share the same assumption: namely the interarrival times (i.e., the times between successive arrivals) are modeled as independent random variables. For instance, the continuous-time Poisson process (hopefully, this is not the right model to predict the next bus arrival!) is the case where the interarrival times are modeled as independent identically exponentially distributed random variables.\n\n\n\n\n\n\nExponential distribution\n\n\n\nA continuous random variable \\(X\\) is said to be exponential, or exponentially distributed with parameter \\(\\lambda\\), when its cumulative distribution function (CDF) is written\n\\[\nF_X(x)=\\text{Pr}(X\\leq x)=1-e^{-\\lambda x},\\;x\\geq0\n\\]\nThe probability density function (PDF) is then given by:\n\\[\np_X(x)=\\frac{d}{dx}F_X(x)=\\lambda\\,e^{-\\lambda x},\\;x\\geq0\n\\]\nThe mean value, the mean square value and the variance of \\(X\\) are:\n\\[\n\\left\\{\n\\begin{split}\nE[X]&=\\int_0^{\\infty}xp_X(x)\\,dx=\\frac{1}{\\lambda}\\\\\nE[X^2]&=\\int_0^{\\infty}x^2p_X(x)\\,dx=\\frac{2}{\\lambda^2}\\\\\n\\text{Var}(X)&=E[X^2]-E[X]^2=\\frac{1}{\\lambda^2}\n\\end{split}\n\\right.\n\\]\nIntegration by parts can be used to calculate the two expectations \\(E[X]\\) and \\(E[X^2]\\).\n\n\n\n\n\n\n\n\nPoisson process\n\n\n\nConsider a sequence of independent exponential random variables \\(T_1,T_2,T_3\\cdots,\\) with common parameter \\(\\lambda\\), and let these stand for the interarrival times. The arrivals are then recorded at times \\(T_1,T_1+T_2,T_1+T_2+T_3,\\cdots\\) and so forth, to define a continuous-time Poisson process.\nA Poisson process is endowed with the following important properties:\n\nIndependence of non-overlapping sets of times. This is a direct consequence of the assumed independence of the interarrival times.\nFresh-start property. The part of the Poisson process that starts at any particular time \\(t&gt;0\\) is a probabilistic replica of the Poisson process starting at time 0, and is independent of the part of the process prior to time \\(t\\). This can be seen as a special case of point 1.\nMemoryless interarrival time distribution. If \\(T\\) is the time of the first arrival and if we know that \\(T&gt;t\\), then the remaining time \\(T−t\\) is exponentially distributed, with the same parameter \\(\\lambda\\):\n\n\\[\n\\begin{split}\n\\text{Pr}(T&gt;t+s\\,\\vert\\,T&gt;t)&=\\frac{\\text{Pr}(T&gt;t+s,T&gt;t)}{\\text{Pr}(T&gt;t)}\\\\\n&=\\frac{\\text{Pr}(T&gt;t+s)}{\\text{Pr}(T&gt;t)}=\\frac{1-F_T(t+s)}{1-F_T(t)}\\\\\n&=\\frac{e^{-\\lambda(t+s)}}{e^{-\\lambda t}}=e^{-\\lambda s}=\\text{Pr}(T&gt;s)\n\\end{split}\n\\]\nProperties 2.-3. can be rephrased saying that the ones whose life is modeled by an exponential distribution, well they should remain forever young: no matter how long they have lived so far, the remaining time to their death is predicted as if they are just born!\n\n\nThe term random incidence denotes the arrival of an observer at an arbitrary time \\(t^*\\) into a gap between two consecutive arrivals in an arrival-type process that is not necessarily described by a Poisson model, Figure 1.\n\n\n\n\n\n\nFigure 1: Illustration of the random incidence phenomenon.\n\n\n\nWe are not saying that \\(t^*\\) is random; in this regard, perhaps, using the term random incidence may appear misleading. However, the interval between the time of the previous arrival \\(t_p\\) and \\(t^*\\), and the interval between \\(t^*\\) and the time of the next arrival \\(t_n\\) are random. All we need to state to start our discussion is to assume that the observer enters the arrival-type process in a situation when a previous arrival has surely occurred: probabilistically, the gap \\(t_n-t_p\\) is then a well-defined quantity.\nSuppose that we know the probability law of the interarrival times \\(Y\\). Let us denote by \\(W\\) the random variable that describes the duration of the gap entered by random incidence, \\(W=T_n-T_p\\). Finally, we denote by \\(T\\) the random variable that describes the waiting time for the next arrival from when the gap is entered by random incidence, \\(T=T_n-T^*\\).\nIt is argued that the probability that \\(W\\) assumes a value between \\(w\\) and \\(w+dw\\) is proportional to both the the duration of the gap \\(w\\) and the relative frequency of occurrence of such gaps \\(p_Y(w)dw\\):\n\\[\n\\text{Pr}(w\\leq W\\leq w+dw)=p_W(w)dw\\propto w\\,p_Y(w)dw\n\\]\nTherefore:\n\\[\np_W(w)=\\frac{w\\,p_Y(w)}{E[Y]}\n\\tag{1}\\]\nwhere the constant of proportionality is calculated to enforce the constraint of normalization for the PDF \\(p_W(w)\\) of \\(W\\) (its integral from 0 to infinity must be 1).\nNow, given that a gap of length \\(w\\) is entered by random incidence, we are equally likely to be anywhere within the gap. More precisely, given \\(w\\), the time until the next arrival \\(T\\) has a uniform PDF:\n\\[\np_{T\\vert W}(t\\,\\vert\\,w)=\\frac{1}{w},\\;0\\leq t\\leq w\n\\tag{2}\\]\nwhere \\(p_{T\\vert W}(t\\,\\vert\\,w)\\) is the conditional PDF of \\(T\\) given \\(W\\). Using Equation 1, Equation 2 the joint PDF \\(p_{TW}(t,w)\\) can be written:\n\\[\np_{TW}(t,w)=p_{T\\vert W}(t\\,\\vert\\,w)\\,p_W(w)=\\frac{p_Y(w)}{E[Y]},\\;0\\leq t\\leq w&lt;\\infty\n\\]\nFinally, the marginal PDF \\(p_T(t)\\) of the waiting time for the next arrival from when the gap is entered by random incidence can be formed simply by integrating out \\(W\\):\n\\[\n\\boxed{p_T(t)=\\int_{t}^{\\infty}\\frac{p_Y(w)}{E[Y]}dw=\\frac{1-F_Y(t)}{E[Y]},\\;t\\geq0}\n\\tag{3}\\]\n\nExample 1 Consider a bus passenger arriving at a bus stop. The probabilistic law of the bus headways \\(F_Y(y)\\) will determine the probability law for the waiting time until the next bus arrives, \\(p_T(t)\\) via Equation 3, if we ignore interactions between successive buses and assume that the arrivals are identically distributed and independent.\nSuppose that buses maintain perfect headways, being always spaced \\(T_0\\) minutes apart:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad y&lt;T_0\\\\1&\\quad y\\geq T_0\\end{split}\\right.\\rightarrow p_Y(y)=\\delta(y-T_0)\\rightarrow E[Y]=T_0\n\\]\nwhere the PDF is expressed in terms of a delta function located at time \\(T_0\\). The PDF of \\(T\\) can be written:\n\\[\np_T(t)=\\left\\{\\begin{split}&\\frac{1}{T_0}&\\quad 0\\leq t\\leq T_0\\\\&0&\\quad t&gt;T_0\\end{split}\\right.\\rightarrow E[T]=\\frac{T_0}{2}\n\\]\nAs expected intuitively, the time until the next arrival, given random incidence, is uniformly distributed between \\(0\\) and \\(T_0\\), with mean value \\(E[T]=T_0/2\\): if \\(T_0=60\\) min the average waiting time is 30 min.\nNow, suppose that the bus headways are on the hour, and fifteen minutes after the hour. Thus, the interarrival times alternate between 15 and 45 minutes. If the bus passenger shows up at the bus stop at any time uniformly distributed within a hour, she/he has to wait for an average time of 15/2 min (with probability 1/4) and 45/2 min (with probability 3/4):\n\\[\nE[T]=\\frac{15}{2}\\cdot\\frac{1}{4}+\\frac{45}{2}\\cdot\\frac{3}{4}=18.75\\;\\text{min}\n\\]\nMore formally:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad0\\leq y&lt;15\\\\1/2&\\quad15\\leq y&lt;45\\\\1&\\quad y\\geq45\\end{split}\\right.\\rightarrow E[Y]=30\\;\\text{min}\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\left\\{\\begin{split}1/30&\\quad0\\leq t&lt;15\\\\1/60&\\quad15\\leq t&lt;45\\\\0&\\quad t\\geq45\\end{split}\\right.\\rightarrow E[T]=18.75\\,\\text{min}\n\\]\nOn average, the bus passenger has to wait longer than it might be expected taking into account \\(E[Y]\\) only, namely \\(E[Y]/2=15\\) min.\n\nThis is because an observer who arrives at an arbitrary time, the bus passenger in this example, is more likely to fall in a large rather than a small interarrival interval: large interarrival intervals tend therefore to determine longer waiting times!\n\n\n\nExample 2 Consider the (unrealistic) case that the bus headways are Poissonian. What does this mean? Basically, we state that the interarrival times are modeled by independent exponentially distributed random variables with rate \\(\\lambda\\) (the rate denotes the number of arrivals per unit time):\n\\[\nF_Y(y)=1-e^{-\\lambda t},\\;t\\geq 0\\rightarrow p_Y(y)=\\frac{F_Y(y)}{dy}=\\lambda e^{-\\lambda t},\\;t\\geq0\\rightarrow E[Y]=1/\\lambda\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\lambda e^{-\\lambda t},\\;t\\geq0\n\\]\nHence \\(T\\) is exponentially distributed with rate \\(\\lambda\\). The average time of waiting is \\(E[T]=1/\\lambda\\). No matter when the event occurred, the observer who arrives at an arbitrary time \\(t^*\\) (the bus passenger in this example) sees the Poisson process to start fresh at time \\(t^*\\), Figure 1. It is worth noting that, according to the properties of independence, start-fresh and memorylessness stated above, we can run a Poisson process either forwards or backwards in time, without any modification in its properties. Hence, not only \\(T_n-T^*\\), but also \\(T^*-T_p\\) is exponentially distributed with parameter \\(\\lambda\\). Moreover, \\(T_n-T^*\\) and \\(T_n-T^*\\) are independent. We have therefore established that the gap \\(W\\) entered by random incidence is the sum of two independent exponential random variables with parameter \\(\\lambda\\) and mean value \\(2/\\lambda\\). More formally, using Equation 1 we get:\n\\[\np_W(w)=\\lambda w\\,e^{-\\lambda w},\\;w\\geq0\n\\]\nWe recall that the random variable \\(X\\) Erlang of order \\(k\\) has PDF:\n\\[\np(x)=\\frac{\\lambda^kx^{k-1}e^{-\\lambda x}}{(k-1)!}\n\\]\nIn conclusion, the gap duration is a random variable Erlang of order 2.\n\nIn a similar fashion to Example 1, when landing into a Poisson process at an arbitrary time, we are more likely to fall in a large interarrival interval; therefore, the length of what we perceive as a typical interarrival interval is greater than it is in reality."
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html",
    "href": "posts/correspondence_analysis_1/index.html",
    "title": "Correspondence analysis: Part I",
    "section": "",
    "text": "Correspondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. If data of only two variables are involved, the method is usually called Simple Correspondence Analysis (SCA); if data of more than two variables are considered in the dataset, then the method is usually called Multiple Correspondence Analysis (MCA).\nSCA is often used in combination with a standard chi-squared test of independence for two categorical variables that form a contingency table. We recall that a contingency table displays frequencies for combinations of two categorical variables; analysts also refer to contingency tables as crosstabulation and two-way tables. Contingency tables classify outcomes for one variable in rows and the other in columns. The values at the row and column intersections are frequencies for each unique combination of the two variables. These values can suggest whether or not the two variables are correlated.\n\n\nCode\n# Libraries \n\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(corrplot)\nlibrary(factoextra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#introduction",
    "href": "posts/correspondence_analysis_1/index.html#introduction",
    "title": "Correspondence analysis: Part I",
    "section": "",
    "text": "Correspondence Analysis (CA) is a type of multidimensional scaling, one of several methods that are available for developing spatial models that reveal associations between two or more categorical variables. If data of only two variables are involved, the method is usually called Simple Correspondence Analysis (SCA); if data of more than two variables are considered in the dataset, then the method is usually called Multiple Correspondence Analysis (MCA).\nSCA is often used in combination with a standard chi-squared test of independence for two categorical variables that form a contingency table. We recall that a contingency table displays frequencies for combinations of two categorical variables; analysts also refer to contingency tables as crosstabulation and two-way tables. Contingency tables classify outcomes for one variable in rows and the other in columns. The values at the row and column intersections are frequencies for each unique combination of the two variables. These values can suggest whether or not the two variables are correlated.\n\n\nCode\n# Libraries \n\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(corrplot)\nlibrary(factoextra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#exemplary-dataset",
    "href": "posts/correspondence_analysis_1/index.html#exemplary-dataset",
    "title": "Correspondence analysis: Part I",
    "section": "Exemplary dataset",
    "text": "Exemplary dataset\nhousetasks, available in the package factoextra, is a data frame that contains the frequency of execution of 13 house tasks performed by the couple in four different ways: a) the wife only; b) alternatively; c) the husband only; d) jointly.\n\n\nCode\ndata(housetasks)\nhousetasks %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    caption = \"House tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks contingency table\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n156\n14\n2\n4\n\n\nMain_meal\n124\n20\n5\n4\n\n\nDinner\n77\n11\n7\n13\n\n\nBreakfeast\n82\n36\n15\n7\n\n\nTidying\n53\n11\n1\n57\n\n\nDishes\n32\n24\n4\n53\n\n\nShopping\n33\n23\n9\n55\n\n\nOfficial\n12\n46\n23\n15\n\n\nDriving\n10\n51\n75\n3\n\n\nFinances\n13\n13\n21\n66\n\n\nInsurance\n8\n1\n53\n77\n\n\nRepairs\n0\n3\n160\n2\n\n\nHolidays\n0\n1\n6\n153\n\n\n\n\n\n\n\nTo easily interpret the contingency table, a graphical matrix can be drawn using the function balloonplot() from the gplots package. In this graph, each cell contains a dot whose size reflects the relative magnitude of the value it contains.\n\n\nCode\n# Convert the data frame to a table\ndt &lt;- as.table(as.matrix(housetasks))\n# Graph\nballoonplot(t(dt), main = \"House tasks contingency table\", xlab = \"\", ylab = \"\",\n            label = FALSE, show.margins = FALSE)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#theory-an-overview",
    "href": "posts/correspondence_analysis_1/index.html#theory-an-overview",
    "title": "Correspondence analysis: Part I",
    "section": "Theory: An overview",
    "text": "Theory: An overview\nLet us consider a generic two-dimensional contingency table \\(\\bf{T}\\), having \\(R\\) rows (corresponding to the levels of the categorical variable \\(X\\)) and \\(C\\) columns (corresponding to the levels of the categorical variable \\(Y\\)). The table is built based on a dataset of \\(n\\) observations, which are distributed across the \\(R\\cdot C\\) cells of \\(\\bf{T}\\). Each cell shows the count \\(n_{ij},\\,i=1,\\cdots,R;\\,j=1,\\cdots,C\\).\n\nKey terms\nRow marginals\nThe row marginals (or row margins) can be computed by taking the row sums of the counts \\(n_{ij}\\):\n\\[\nn_{i+}=\\sum_{j=1}^C n_{ij},\\;i=1,\\dots,R\n\\]\nColumn marginals\nThe column marginals (or col margins) can be computed by taking the column sums of the counts \\(n_{ij}\\):\n\\[\nn_{+j}=\\sum_{i=1}^R n_{ij}\\;j=1,\\dots,C\n\\]\nGrand total\nThe gran total is the complete number after everything has been added up:\n\\[\nn_{++}=\\sum_{i=1}^R n_{i+}=\\sum_{j=1}^Cn_{+j}=\\sum_{i=1}^R\\sum_{j=1}^Cn_{ij}=n\n\\]\n\n\nCode\n# Row marginals\nrow.sum &lt;- apply(housetasks, 1, sum)\n# Column marginals\ncol.sum &lt;- apply(housetasks, 2, sum)\n# Gran total\nn &lt;- sum(housetasks)\nhousetasks %&gt;%\n  mutate(total = row.sum) %&gt;%\n  rbind(\"Total\" = c(col.sum, n)) %&gt;%\n  kable(\n    align = \"rrrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\", \"Total\"),\n    caption = \"House task contingency table with row, column margins and gran total\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse task contingency table with row, column margins and gran total\n\n\n\nWife\nAlternating\nHusband\nJointly\nTotal\n\n\n\n\nLaundry\n156\n14\n2\n4\n176\n\n\nMain_meal\n124\n20\n5\n4\n153\n\n\nDinner\n77\n11\n7\n13\n108\n\n\nBreakfeast\n82\n36\n15\n7\n140\n\n\nTidying\n53\n11\n1\n57\n122\n\n\nDishes\n32\n24\n4\n53\n113\n\n\nShopping\n33\n23\n9\n55\n120\n\n\nOfficial\n12\n46\n23\n15\n96\n\n\nDriving\n10\n51\n75\n3\n139\n\n\nFinances\n13\n13\n21\n66\n113\n\n\nInsurance\n8\n1\n53\n77\n139\n\n\nRepairs\n0\n3\n160\n2\n165\n\n\nHolidays\n0\n1\n6\n153\n160\n\n\nTotal\n600\n254\n381\n509\n1744\n\n\n\n\n\n\n\nThe gran total is obtained by summing the values of the row margin or the values of the col margin (\\(n=\\) 1744).\nTo compare rows (or columns), their profiles can be analyzed, in the search for similar rows (or columns).\nRow profile\nThe profile of a row (or row profile) is calculated by taking each row point and dividing by the corresponding value of the row margin:\n\\[\n\\underline{a}_i,\\;i=1,\\dots,R=\\{\\underline{a}_{i_j},\\;j=1,\\dots,C\\}=\\{n_{ij}/n_{i+},\\;j=1,\\dots,C\\}\n\\]\nAverage row profile\nThe average row profile can be computed from the col margin after dividing it by the grand total:\n\\[\n\\underline{r}=\\{r_j=n_{+j}/n,\\;j=1,\\dots,C\\}\n\\]\n\n\nCode\n# Row profile\nrow.profile &lt;- housetasks/row.sum\n# Average row profile\naverage.row.profile &lt;- col.sum/n\n# Row margins\nrow.profile.sum &lt;- apply(row.profile, 1, sum)\n\nrow.profile %&gt;%\n  mutate(TOTAL = row.profile.sum) %&gt;%\n  rbind(`Average row profile` = c(average.row.profile, sum(average.row.profile))) %&gt;%\n  kable(\n    align = \"rrrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\", \"Total\"),\n    digits = 4,\n    caption = \"House tasks data frame with row profiles and average row profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks data frame with row profiles and average row profile\n\n\n\nWife\nAlternating\nHusband\nJointly\nTotal\n\n\n\n\nLaundry\n0.8864\n0.0795\n0.0114\n0.0227\n1\n\n\nMain_meal\n0.8105\n0.1307\n0.0327\n0.0261\n1\n\n\nDinner\n0.7130\n0.1019\n0.0648\n0.1204\n1\n\n\nBreakfeast\n0.5857\n0.2571\n0.1071\n0.0500\n1\n\n\nTidying\n0.4344\n0.0902\n0.0082\n0.4672\n1\n\n\nDishes\n0.2832\n0.2124\n0.0354\n0.4690\n1\n\n\nShopping\n0.2750\n0.1917\n0.0750\n0.4583\n1\n\n\nOfficial\n0.1250\n0.4792\n0.2396\n0.1562\n1\n\n\nDriving\n0.0719\n0.3669\n0.5396\n0.0216\n1\n\n\nFinances\n0.1150\n0.1150\n0.1858\n0.5841\n1\n\n\nInsurance\n0.0576\n0.0072\n0.3813\n0.5540\n1\n\n\nRepairs\n0.0000\n0.0182\n0.9697\n0.0121\n1\n\n\nHolidays\n0.0000\n0.0063\n0.0375\n0.9562\n1\n\n\nAverage row profile\n0.3440\n0.1456\n0.2185\n0.2919\n1\n\n\n\n\n\n\n\nColumn profile\nThe profile of a column (or col profile) is calculated by taking each column point and dividing by the corresponding value of the col margin:\n\\[\n\\underline{b}_j,\\;j=1,\\dots,C=\\{\\underline{b}_{j_i},\\;i=1,\\dots,R\\}=\\{n_{ij}/n_{+j},\\;i=1,\\dots,R\\}\n\\]\nAverage column profile\nThe average column profile (or average col profile) can be computed from the row marginal after dividing it by the grand total:\n\\[\n\\underline{c}=\\{c_i=n_{i+}/n,\\;i=1,\\dots,R\\}\n\\]\n\n\nCode\n# Column profile\ncol.profile &lt;- t(housetasks)/col.sum\ncol.profile &lt;- as.data.frame(t(col.profile))\n# Average column profile\naverage.col.profile &lt;- row.sum/n\n# Column margins\ncol.profile.sum &lt;- apply(col.profile, 2, sum)\n\ncol.profile %&gt;%\n  mutate(`Average col profile` = average.col.profile) %&gt;%\n  rbind(Total = c(col.profile.sum, sum(average.col.profile))) %&gt;%\n  kable(\n    align = \"rrrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\", \"Average col profile\"),\n    digits = 4,\n    caption = \"House tasks data frame with col profiles and average col profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks data frame with col profiles and average col profile\n\n\n\nWife\nAlternating\nHusband\nJointly\nAverage col profile\n\n\n\n\nLaundry\n0.2600\n0.0551\n0.0052\n0.0079\n0.1009\n\n\nMain_meal\n0.2067\n0.0787\n0.0131\n0.0079\n0.0877\n\n\nDinner\n0.1283\n0.0433\n0.0184\n0.0255\n0.0619\n\n\nBreakfeast\n0.1367\n0.1417\n0.0394\n0.0138\n0.0803\n\n\nTidying\n0.0883\n0.0433\n0.0026\n0.1120\n0.0700\n\n\nDishes\n0.0533\n0.0945\n0.0105\n0.1041\n0.0648\n\n\nShopping\n0.0550\n0.0906\n0.0236\n0.1081\n0.0688\n\n\nOfficial\n0.0200\n0.1811\n0.0604\n0.0295\n0.0550\n\n\nDriving\n0.0167\n0.2008\n0.1969\n0.0059\n0.0797\n\n\nFinances\n0.0217\n0.0512\n0.0551\n0.1297\n0.0648\n\n\nInsurance\n0.0133\n0.0039\n0.1391\n0.1513\n0.0797\n\n\nRepairs\n0.0000\n0.0118\n0.4199\n0.0039\n0.0946\n\n\nHolidays\n0.0000\n0.0039\n0.0157\n0.3006\n0.0917\n\n\nTotal\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n\n\n\n\n\nDistance (or similarity) between row profiles\nThis metrics is useful to compare how similar to each other are two rows of the contingency table:\n\\[\nd^2_{kl},\\;k,l=1,\\dots,R=\\Vert \\underline{a}_k-\\underline{a}_{l}\\Vert^2_\\underline{r}=\\large\\sum_{j=1}^C\\dfrac{(\\underline{a}_{k_j}-\\underline{a}_{l_j})^2}{r_j}\n\\]\nRecall that we need to normalize each component within the sum to the corresponding element of the average row profile.\n\n\nCode\n# \"Dinner\" and \"Driving\" profiles\ndinner.p &lt;- row.profile[\"Dinner\", ]\ndriving.p &lt;- row.profile[\"Driving\", ]\n# Distance between \"Dinner\" and \"Driving\"\nd2_1 &lt;- sum(((dinner.p - driving.p)^2)/average.row.profile)\n# \"Breakfeast\" profile\nbreakfast.p &lt;- row.profile[\"Breakfeast\", ]\n# Distance between \"Dinner\" and \"Breakfeast\"\nd2_2 &lt;- sum(((dinner.p - breakfast.p)^2)/average.row.profile)\n\n\nExample - The similarity between “Dinner” and “Driving” profiles is 2.742, whereas the similarity between “Dinner” and “Breakfeast” is 0.238.\nDistance (or similarity) between col profiles\nThis metrics is useful to compare how similar to each other are two columns of the contingency table:\n\\[\nd^2_{kl},\\;k,l=1,\\dots,C=\\Vert \\underline{b}_k-\\underline{b}_l\\Vert^2_\\underline{c}=\\large\\sum_{i=1}^R\\dfrac{(\\underline{b}_{k_i}-\\underline{b}_{l_i})^2}{c_i}\n\\]\nRecall that we need to normalize each component within the sum to the corresponding element of the average col profile.\n\n\nCode\n# \"Wife\" and \"Husband\" profiles\nwife.p &lt;- col.profile[, \"Wife\"]\nhusband.p &lt;- col.profile[, \"Husband\"]\n# Distance between \"Wife\" and \"Husband\"\nd2_1 &lt;- sum(((wife.p - husband.p)^2) / average.col.profile)\n# \"Jointly\" profile\njointly.p &lt;- col.profile[, \"Jointly\"]\n# Distance between \"Wife\" and \"Jointly\"\nd2_2 &lt;- sum(((wife.p - jointly.p)^2) / average.col.profile)\n\n\nExample - The similarity between “Wife” and “Husband” profiles is 4.05, whereas the similarity between “Wife” and “Jointly” is 2.935.\nDistance (or similarity) between each row profile and the average row profile\nThis metrics is useful to compare how similar to the average row profile is any row profile.\n\\[\nd^2_{k},\\;k=1,\\dots,R=\\Vert\\underline{a}_k-\\underline{r}\\Vert^2_\\underline{r}=\\large\\sum_{j=1}^C\\dfrac{(\\underline{a}_{k_j}-r_{j})^2}{r_j}\n\\]\n\n\nCode\nd2.row &lt;- apply(row.profile, 1, \n        function(row.p, av.p){sum(((row.p - av.p)^2)/av.p)}, \n        average.row.profile)\n\nas.matrix(round(d2.row,3)) %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Task\", \"Similarity\"),\n    digits = 3,\n    caption = \"Similarity between each row profile and the average row profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSimilarity between each row profile and the average row profile\n\n\nTask\nSimilarity\n\n\n\n\nLaundry\n1.329\n\n\nMain_meal\n1.034\n\n\nDinner\n0.618\n\n\nBreakfeast\n0.512\n\n\nTidying\n0.353\n\n\nDishes\n0.302\n\n\nShopping\n0.218\n\n\nOfficial\n0.968\n\n\nDriving\n1.274\n\n\nFinances\n0.456\n\n\nInsurance\n0.727\n\n\nRepairs\n3.307\n\n\nHolidays\n2.140\n\n\n\n\n\n\n\nDistance (or similarity) between each col profile and the average col profile\nThis metrics is useful to compare how similar to the average col profile is any col profile.\n\\[\nd^2_{l},\\;l=1,\\dots,C=\\Vert\\underline{b}_l-\\underline{c}\\Vert^2_\\underline{c}=\\large\\sum_{i=1}^R\\dfrac{(\\underline{b}_{l_i}-c_i)^2}{c_i}\n\\]\n\n\nCode\nd2.col &lt;- apply(col.profile, 2, \n        function(col.p, av.p){sum(((col.p - av.p)^2)/av.p)}, \n        average.col.profile)\n\nas.matrix(round(d2.col,3)) %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Couple\", \"Similarity\"),\n    digits = 3,\n    caption = \"Similarity between each col profile and the average col profile\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSimilarity between each col profile and the average col profile\n\n\nCouple\nSimilarity\n\n\n\n\nWife\n0.875\n\n\nAlternating\n0.809\n\n\nHusband\n1.746\n\n\nJointly\n1.078\n\n\n\n\n\n\n\nDistance matrix\nThe similarity can be computed between each row (col) profile and the other row (col) profiles in the contingency table, using, for example, the function dist-matrix() (here).\n\n\nCode\n# data: a data frame or matrix \n# average.profile: average profile\ndist.matrix &lt;- function(data, average.profile) {\n   mat &lt;- as.matrix(t(data))\n   n &lt;- ncol(mat)\n   dist.mat &lt;- matrix(NA, n, n)\n   diag(dist.mat) &lt;- 0\n    for (i in 1:(n - 1)) {\n        for (j in (i + 1):n) {\n            d2 &lt;- sum(((mat[, i] - mat[, j])^2) / average.profile)\n            dist.mat[i, j] &lt;- dist.mat[j, i] &lt;- d2\n        }\n    }\n  colnames(dist.mat) &lt;- rownames(dist.mat) &lt;- colnames(mat)\n  dist.mat\n}\n\n\nThe result is a distance matrix (a kind of correlation or dissimilarity matrix), either orientated per rows or columns. The distance between row (col) profiles can be computed using the package corrplot for visualization.\nRow distance matrix\n\n\nCode\n# Distance matrix (row)\ndist.mat &lt;- dist.matrix(row.profile, average.row.profile)\ndist.mat &lt;- round(dist.mat, 2)\n# Visualize the matrix (row)\ncorrplot(dist.mat, type = \"upper\", is.corr = FALSE, col = colorRampPalette(c(\"white\", \"steelblue\"))(128))\n\n\n\n\n\n\n\n\n\nCol distance matrix\n\n\nCode\n# Distance matrix (column)\ndist.mat &lt;- dist.matrix(t(col.profile), average.col.profile)\ndist.mat &lt;- round(dist.mat, 2)\n# Visualize the matrix (column)\ncorrplot(dist.mat, type = \"upper\", is.corr = FALSE, col = colorRampPalette(c(\"white\", \"steelblue\"))(128))\n\n\n\n\n\n\n\n\n\nRow mass and inertia\nThe row mass can be defined as the total frequency of each row, obtained by dividing its row sum by the gran total. The row inertia is calculated as the row mass multiplied by the squared distance between the row and the average row profile. The inertia is a measure of the information contained in each row.\n\n\nCode\n# Row mass\nrow.sum &lt;- apply(housetasks, 1, sum)\ngrand.total &lt;- sum(housetasks)\nrow.mass &lt;- row.sum/grand.total\n# Row inertia\nrow.inertia &lt;- row.mass*d2.row\n\nrow.inertia %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Task\", \"Inertia\"),\n    digits = 3,\n    caption = \"Row inertia of the house tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nRow inertia of the house tasks contingency table\n\n\nTask\nInertia\n\n\n\n\nLaundry\n0.134\n\n\nMain_meal\n0.091\n\n\nDinner\n0.038\n\n\nBreakfeast\n0.041\n\n\nTidying\n0.025\n\n\nDishes\n0.020\n\n\nShopping\n0.015\n\n\nOfficial\n0.053\n\n\nDriving\n0.102\n\n\nFinances\n0.030\n\n\nInsurance\n0.058\n\n\nRepairs\n0.313\n\n\nHolidays\n0.196\n\n\n\n\n\n\n\nColumn mass and inertia\nThe column mass can be defined as the total frequency of each column, obtained by dividing its col sum by the gran total. The column inertia is calculated as the column mass multiplied by the squared distance between the column and the average col profile. The inertia is a measure of the information contained in each column.\n\n\nCode\n# Column mass\ncol.sum &lt;- apply(housetasks, 2, sum)\ngrand.total &lt;- sum(housetasks)\ncol.mass &lt;- col.sum/grand.total\n# Column inertia\ncol.inertia &lt;- col.mass*d2.col\n\ncol.inertia %&gt;%\n  kable(\n    align = \"rc\",\n    col.names = c(\"Couple\", \"Inertia\"),\n    digits = 3,\n    caption = \"Column inertia of the house tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nColumn inertia of the house tasks contingency table\n\n\nCouple\nInertia\n\n\n\n\nWife\n0.301\n\n\nAlternating\n0.118\n\n\nHusband\n0.381\n\n\nJointly\n0.315\n\n\n\n\n\n\n\nTotal inertia\nThe total inertia is the total information contained in the contingency table.\n\n\nCode\n# Total inertia\ninertia_total_row &lt;- sum(row.inertia)\ninertia_total_col &lt;- sum(col.inertia)\n\n\nThe total inertia is computed as the sum of row inertias (or equivalently, as the sum of column inertias), in the present case we get the value 1.115.\n\n\nCode\n# Row mass\nrow.sum &lt;- apply(housetasks, 1, sum)\ngrand.total &lt;- sum(housetasks)\nrow.mass &lt;- row.sum/grand.total\n# Row inertia\nrow.inertia &lt;- row.mass*d2.row\n# Total inertia\ninertia_total_row &lt;- sum(row.inertia)\n\n# Column mass\ncol.sum &lt;- apply(housetasks, 2, sum)\ncol.mass &lt;- col.sum/grand.total\n# Column inertia\ncol.inertia &lt;- col.mass*d2.col\n# Total inertia\ninertia_total_col &lt;- sum(col.inertia)"
  },
  {
    "objectID": "posts/correspondence_analysis_1/index.html#results",
    "href": "posts/correspondence_analysis_1/index.html#results",
    "title": "Correspondence analysis: Part I",
    "section": "Results",
    "text": "Results\nThe result for rows can be summarized in terms of the squared distance between row profiles and the average row profile (i.e., d2.row), the row mass (i.e., row mass), and the row inertia (i.e., row.inertia). Similarly, the results for columns can be summarized using d2.col, col.mass and col.inertia.\n\n\nCode\nrow &lt;- cbind.data.frame(d2 = d2.row, mass = row.mass, inertia = row.inertia)\ncol &lt;- cbind.data.frame(d2 = d2.col, mass = col.mass, inertia = col.inertia)\n\nrow %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Task\", \"Squared distance\", \"Mass\", \"Inertia\"),\n    digits = 3,\n    caption = \"Summary statistics for the rows of the contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSummary statistics for the rows of the contingency table\n\n\nTask\nSquared distance\nMass\nInertia\n\n\n\n\nLaundry\n1.329\n0.101\n0.134\n\n\nMain_meal\n1.034\n0.088\n0.091\n\n\nDinner\n0.618\n0.062\n0.038\n\n\nBreakfeast\n0.512\n0.080\n0.041\n\n\nTidying\n0.353\n0.070\n0.025\n\n\nDishes\n0.302\n0.065\n0.020\n\n\nShopping\n0.218\n0.069\n0.015\n\n\nOfficial\n0.968\n0.055\n0.053\n\n\nDriving\n1.274\n0.080\n0.102\n\n\nFinances\n0.456\n0.065\n0.030\n\n\nInsurance\n0.727\n0.080\n0.058\n\n\nRepairs\n3.307\n0.095\n0.313\n\n\nHolidays\n2.140\n0.092\n0.196\n\n\n\n\n\n\n\nCode\ncol %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"couple\", \"squared distance\", \"mass\", \"inertia\"),\n    digits = 3,\n    caption = \"Summary statistics for the columns of the contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nSummary statistics for the columns of the contingency table\n\n\ncouple\nsquared distance\nmass\ninertia\n\n\n\n\nWife\n0.875\n0.344\n0.301\n\n\nAlternating\n0.809\n0.146\n0.118\n\n\nHusband\n1.746\n0.218\n0.381\n\n\nJointly\n1.078\n0.292\n0.315\n\n\n\n\n\n\n\n\nChi-square test\nThe chi-square statistic (\\(\\chi^2\\)) is an overall measure of the difference between the frequencies observed in a contingency table and the expected frequencies calculated under the assumption of homogeneity of row profiles (or column profiles). Geometrically, the inertia measures how far away row profiles (or column profiles) are from their average profiles. The average profile can be thought of as the representative of the hypothesis of homogeneity (i.e., equality) of profiles. The distances between profiles are measured using the distance of the chi-square (distance \\(\\chi^2\\)). This distance is similar in its formulation to the Euclidean distance between points in a physical space, except that any quadratic difference between coordinates is divided by the corresponding element of the mean profile.\nIt can be shown that the total inertia \\(\\Phi\\) of a contingency table is the statistic (\\(\\chi^2\\)) divided by the grand total \\(n\\):\n\\[\n\\Phi=\\dfrac{\\chi^2}{n}=\\sum_{i}^Rc_i\\Vert\\underline{a}_i-\\underline{r}\\Vert^2_{\\underline{r}}=\\sum_{j}^Cr_j\\Vert\\underline{b}_j-\\underline{c}\\Vert^2_{\\underline{c}}\n\\]\nThe Chi-squared test can be used to examine whether rows and columns of a contingency table are statistically significantly associated:\n\nNull hypothesis (H0): the row and the column variables of the contingency table are independent;\nAlternative hypothesis (H1): row and column variables are dependent\n\nFor each cell of the table, we have to calculate the expected value under H0. For a given cell \\((i,j)\\), the expected value is calculated by taking the product of the \\(i\\)-th element of the row margin (sum of the elements in the \\(i\\)-th row of the contingency table) and the \\(j\\)-th element of the col margin (sum of the elements in the \\(j\\)-th column of the contingency table), divided by the gran total.\n\n\n\n\n\n\nStatistical testing of homogeneity/independence\n\n\n\nSuppose that the population is divided into \\(R\\) groups and each group (or the entire population) is divided into \\(C\\) categories. We would like to test whether the distribution of categories in each group is the same (homogeneity test). If observations \\(X_1,\\dots,X_n\\) are sampled independently from the entire population then homogeneity over groups is the same as independence of groups and categories. If we have homogeneity:\n\\[\n\\text{Pr}(\\text{Category}_j\\vert\\text{Group}_i)=\\text{Pr}(\\text{Category}_j)\n\\]\nthen we have independence:\n\\[\n\\text{Pr}(\\text{Category}_j,\\text{Group}_i)=\\text{Pr}(\\text{Category}_j\\vert\\text{Group}_i)\\text{Pr}(\\text{Group}_i)=\\text{Pr}(\\text{Category}_j)\\text{Pr}(\\text{Group}_i)\n\\]\nIt is also possible to move the other way around (i.e., start from independence to obtain homogeneity). In other words, to test homogeneity, we can use the test of independence. The computation of the expected cell counts under the null hypothesis H0 (the row and the column variables of the contingency table are independent) is done according to the scheme of Figure 1.\n\n\n\n\n\n\nFigure 1: The expected counts are involved in testing either homegeneity or independence\n\n\n\n\n\nFor example, in the case of the house tasks contingency table, the scheme of computation for the cell “Shopping”-“Alternating” is shown in Figure 2.\n\n\n\n\n\n\nFigure 2: How to calculate the expected counts\n\n\n\n\n\nCode\n# Table of expected counts\nE = (row.sum %*% t(col.sum))/n\n\nrow.names(E) &lt;- row.names(housetasks)\nE %&gt;% \n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    digits = 2,\n    caption = \"Table of expected counts\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nTable of expected counts\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n60.55\n25.63\n38.45\n51.37\n\n\nMain_meal\n52.64\n22.28\n33.42\n44.65\n\n\nDinner\n37.16\n15.73\n23.59\n31.52\n\n\nBreakfeast\n48.17\n20.39\n30.58\n40.86\n\n\nTidying\n41.97\n17.77\n26.65\n35.61\n\n\nDishes\n38.88\n16.46\n24.69\n32.98\n\n\nShopping\n41.28\n17.48\n26.22\n35.02\n\n\nOfficial\n33.03\n13.98\n20.97\n28.02\n\n\nDriving\n47.82\n20.24\n30.37\n40.57\n\n\nFinances\n38.88\n16.46\n24.69\n32.98\n\n\nInsurance\n47.82\n20.24\n30.37\n40.57\n\n\nRepairs\n56.77\n24.03\n36.05\n48.16\n\n\nHolidays\n55.05\n23.30\n34.95\n46.70\n\n\n\n\n\n\n\nThe calculated Chi-square statistic \\(\\chi^2\\) is compared to the critical value (obtained from statistical tables) with \\(df=(R-1)(C-1)\\) degrees of freedom and \\(p=0.05\\). Recall that \\(R, C\\) are the number of rows and columns in the contingency table, respectively. If the calculated Chi-square statistic is greater than the critical value, then we must conclude that the row and the column variables are not independent of each other. This implies that they are significantly associated. The function chisq.test() can be used to implement the test.\nThe result of chisq.test() function is a list containing the following components:\n\nstatistic: the value the chi-squared test statistic.\nparameter: the degrees of freedom\np.value: the p-value of the test\nobserved: the observed count\nexpected: the expected count\nresiduals: Pearson residuals\n\nIt is worth noting that the total inertia is equal to the total Chi-square score divided by the grand total. The square root of the total inertia is called trace and may be interpreted as a correlation coefficient. Any value of the trace &gt; 0.2 indicates a significant dependency between rows and columns. In the present example 1.056.\n\n\nPearson residuals\nIf we want to know the most contributing cells to the total Chi-square score, we just have to calculate the Chi-square statistic for each cell:\n\\[\nr_{ij} = \\dfrac{o_{ij}-e_{ij}}{\\sqrt{e_{ij}}},\\,i=1,\\dots,R;\\,j=1,\\dots,C\n\\]\nThe above formula returns the so-called Pearson residuals (\\(r\\)) for each cell (or standardized residuals). Cells with the highest absolute standardized residuals contribute the most to the total Chi-square score. Pearson residuals can be easily extracted from the output of the function chisq.test().\n\n\nCode\nchisq &lt;- chisq.test(housetasks)\nchisq\n\n\n\n    Pearson's Chi-squared test\n\ndata:  housetasks\nX-squared = 1944.5, df = 36, p-value &lt; 2.2e-16\n\n\nCode\n# Observed counts\nchisq$observed\n\n\n           Wife Alternating Husband Jointly\nLaundry     156          14       2       4\nMain_meal   124          20       5       4\nDinner       77          11       7      13\nBreakfeast   82          36      15       7\nTidying      53          11       1      57\nDishes       32          24       4      53\nShopping     33          23       9      55\nOfficial     12          46      23      15\nDriving      10          51      75       3\nFinances     13          13      21      66\nInsurance     8           1      53      77\nRepairs       0           3     160       2\nHolidays      0           1       6     153\n\n\nCode\n# Expected counts\nround(chisq$expected, 2)\n\n\n            Wife Alternating Husband Jointly\nLaundry    60.55       25.63   38.45   51.37\nMain_meal  52.64       22.28   33.42   44.65\nDinner     37.16       15.73   23.59   31.52\nBreakfeast 48.17       20.39   30.58   40.86\nTidying    41.97       17.77   26.65   35.61\nDishes     38.88       16.46   24.69   32.98\nShopping   41.28       17.48   26.22   35.02\nOfficial   33.03       13.98   20.97   28.02\nDriving    47.82       20.24   30.37   40.57\nFinances   38.88       16.46   24.69   32.98\nInsurance  47.82       20.24   30.37   40.57\nRepairs    56.77       24.03   36.05   48.16\nHolidays   55.05       23.30   34.95   46.70\n\n\nCode\n# Pearson's residuals\ne &lt;- chisq$residuals\n# Contribution in percentage of each cell (%)\ncontrib &lt;- 100*chisq$residuals^2/chisq$statistic\n\n\nThe sign of the standardized residuals is also very important to interpret the association between rows and columns. Positive values in cells specify an attraction (positive association) between the corresponding row and column variables, such as, for instance, between the column “Wife” and the row “Laundry”. There is a strong positive association between the column “Husband” and the row “Repairs”. A negative residual implies a repulsion (negative association) between the corresponding row and column variables. For example the column “Wife” is negatively associated (i.e., “not associated”) with the row “Repairs”. There is a repulsion between the column “Husband” and the rows “Laundry” and “Main_meal”.\nThe contribution (in %) of a given cell to the total Chi-square score can be calculated as follows:\n\\[\n\\text{contrib}\\,(\\%)=100\\,\\dfrac{r^2}{\\chi^2}\n\\]\nThe relative contribution of each cell to the total Chi-square score give some indication of the nature of the dependency between rows and columns of the contingency table. It can be seen that:\n\nThe column “Wife” is strongly associated with the rows “Laundry”\nThe column “Husband” is strongly associated with the row “Repairs”\nThe column “Jointly” is frequently associated with the row “Holidays”\n\n\n\nCode\ne %&gt;% \n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    digits = 2,\n    caption = \"Pearson's residuals\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nPearson's residuals\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n12.27\n-2.30\n-5.88\n-6.61\n\n\nMain_meal\n9.84\n-0.48\n-4.92\n-6.08\n\n\nDinner\n6.54\n-1.19\n-3.42\n-3.30\n\n\nBreakfeast\n4.88\n3.46\n-2.82\n-5.30\n\n\nTidying\n1.70\n-1.61\n-4.97\n3.59\n\n\nDishes\n-1.10\n1.86\n-4.16\n3.49\n\n\nShopping\n-1.29\n1.32\n-3.36\n3.38\n\n\nOfficial\n-3.66\n8.56\n0.44\n-2.46\n\n\nDriving\n-5.47\n6.84\n8.10\n-5.90\n\n\nFinances\n-4.15\n-0.85\n-0.74\n5.75\n\n\nInsurance\n-5.76\n-4.28\n4.11\n5.72\n\n\nRepairs\n-7.53\n-4.29\n20.65\n-6.65\n\n\nHolidays\n-7.42\n-4.62\n-4.90\n15.56\n\n\n\n\n\n\n\nCode\ncontrib %&gt;% \n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    digits = 2,\n    caption = \"Percentage contribution to the total Chi square\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nPercentage contribution to the total Chi square\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n7.74\n0.27\n1.78\n2.25\n\n\nMain_meal\n4.98\n0.01\n1.24\n1.90\n\n\nDinner\n2.20\n0.07\n0.60\n0.56\n\n\nBreakfeast\n1.22\n0.61\n0.41\n1.44\n\n\nTidying\n0.15\n0.13\n1.27\n0.66\n\n\nDishes\n0.06\n0.18\n0.89\n0.63\n\n\nShopping\n0.09\n0.09\n0.58\n0.59\n\n\nOfficial\n0.69\n3.77\n0.01\n0.31\n\n\nDriving\n1.54\n2.40\n3.37\n1.79\n\n\nFinances\n0.89\n0.04\n0.03\n1.70\n\n\nInsurance\n1.71\n0.94\n0.87\n1.68\n\n\nRepairs\n2.92\n0.95\n21.92\n2.28\n\n\nHolidays\n2.83\n1.10\n1.23\n12.45\n\n\n\n\n\n\n\n\nSCA is just the singular value decomposition of the standardized residuals. How to perform this analysis using the R programming software will be explained in a future post (here), together with some discussion of the implications that the results of the analysis may have."
  },
  {
    "objectID": "posts/combinatorics/index.html",
    "href": "posts/combinatorics/index.html",
    "title": "Pills of combinatorics",
    "section": "",
    "text": "At least in the elementary case of finite sample spaces (i.e., a finite number of outcomes can be generated by our experiment), we often find useful to consider the intuitive notion that the outcomes are equally likely; this allows to introduce what is known as the classical definition of probability. For any set \\(A\\) made of some collection of outcomes from the sample space, we define the probability of \\(A\\) as:\n\\[\n\\text{Pr}(A)=\\frac{\\text{number of cases in}\\;A}{\\text{number of cases in the sample space}}\n\\]\nAfter the sample space has been defined and its size determined, we calculate probabilities of sets by counting the number of possible cases.\nThe art of counting is a relevant part of a field in mathematics known as combinatorics. In this post, I present the basic principle of counting and apply it to a number of situations that are often encountered in probabilistic models."
  },
  {
    "objectID": "posts/combinatorics/index.html#the-counting-principle",
    "href": "posts/combinatorics/index.html#the-counting-principle",
    "title": "Pills of combinatorics",
    "section": "The counting principle",
    "text": "The counting principle\nConsider an experiment that consists of two consecutive stages. The possible outcomes of the first stage are \\(a_1,\\cdots,a_n\\) and, for each outcome from the first stage, \\(b_1,\\cdots,b_m\\) outcomes are possible for the second stage. The outcomes of the two-stage experiment are thus the ordered pairs \\((a_i,b_j),i=1,...,m;j=1,\\cdots,m\\), whose number is \\(nm\\). An obvious generalization is for an \\(r\\)-stage experiment, with \\(n_i,i=1,2,\\cdots,r\\) outcomes each. The total number of outcomes is\n\\[\nN=\\prod_{i=1}^rn_i\n\\]\n\nExample 1 (Number of subsets of an \\(n\\)-element set.) Consider an \\(n\\)-element set \\(\\{a_1,a_2,\\cdots,a_n\\}\\). The construction of a subset can be seen as an \\(n\\)-stage process, where, at the \\(i\\)th stage, the \\(i\\)th element is offered the opportunity to be a member of the subset or not (binary choice). Therefore the number of subsets is \\(N=2^n\\), inclusive of either the empty set (no elements are in one subset) or the sample space (all the elements are in another subset).\n\nConsider now the situation where we have \\(n\\) distinct objects in a box and we want to form groups by sequentially selecting \\(k\\) objects from it, without repetitions being allowed (sampling without replacement) or with repetitions being allowed (sampling with replacement). In the former case, of course, we cannot select more objects than they are in the box, namely \\(k\\leq n\\). In the latter case, the restriction \\(k\\leq n\\) does not apply and, although unlikely, a group can even consist of \\(k\\) replicas of the same object.\nMoreover, the selection process can differ in regard to wthether we are interested in the order of selection or not. If the order of selection matters, two groups that are made by the same objects, each one with its own number of occurrences, are considered distinct, whereas, if the order of selection does not matter, they should count as one case only in the probability calculation.\nBased on these properties, four different arrangements of \\(k\\) out of a collection of \\(n\\) objects have to be considered, namely (without/with) repetition-order of selection (does/does not) matter. Each arrangement has its own name and related formula of counting, as outlined in the following."
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-without-replacement",
    "href": "posts/combinatorics/index.html#sampling-without-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling without replacement",
    "text": "Sampling without replacement\n\nOrder of selection matters\nThe number of arrangements, called permutations, can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at the second stage, the choices are reduced by one unit, i.e., \\(n-1\\); when we reach the \\(k\\)-stage, we are left with \\(n-k+1\\) choices. Applying the principle of counting, we have:\n\\[\nD(n,k)=n(n-1)\\cdots (n-k+1)=\\frac{n!}{(n-k)!}\n\\tag{1}\\]\nwhere the factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\), i.e., \\(n!=1\\cdot2\\cdots n\\). Recall that, when \\(k=n\\), \\(D(n,n)=n!\\) (\\(0!=1!=1\\)).\n\n\nOrder of selection does not matter\nThe number of arrangements, called combinations, can be calculated by noting that there exist \\(P(k,k)=k!\\) sequences of length \\(k\\) that differ from one another just in terms of the order of the presentation of their elements. They contribute only one arrangement to the total number of them:\n\\[\nC(n,k)=\\frac{D(n,k)}{D(k,k)}=\\frac{n!}{(n-k)!k!}={n\\choose k}\n\\tag{2}\\]\nwhere the binomial coefficient \\({n\\choose k}\\), indexed by the pair of integers \\(n\\geq k\\geq0\\), is considered.\n\nExample 2 (Newton’s binomial theorem) For any real number \\(n\\) that is not a non-negative integer, the theorem states that:\n\\[\n\\sum_{k=0}^n{n\\choose k}a^kb^{n-k}=(a+b)^n\n\\]\nwhen \\(a,b\\in\\mathbb{R}\\).\nSince \\({n\\choose k}\\) is the number of \\(k\\)-element sequences of a given \\(n\\)-element set, the sum over \\(k\\) of \\({n\\choose k}\\) counts the number of subsets of all possible sizes. Using the result of Example 1:\n\\[\n\\sum_{k=0}^{n}{n\\choose k}=2^n\n\\]\nThis result is also a simple application of the Newton’s binomial theorem for \\(a=b=1\\).\n\nRecall that a combination is a choice of \\(k\\) elements out of an \\(n\\)-element set without regard to order. This is the same as partitioning the set in two: one part contains \\(k\\) elements and the other contains the remaining \\(n−k\\). We can generalize by considering partitions in more than two subsets. We have \\(n\\) distinct objects and we are given non-negative integers \\(n_1,n_2,\\cdots,n_r\\), whose sum is equal to \\(n\\). The \\(n\\) items are to be divided into \\(r\\) disjoint groups, with the \\(i\\)th group containing exactly \\(n_i\\) items. The total number of groups is given by the multinomial coefficient:\n\\[\nN_r={n\\choose n_1n_2\\cdots n_r}=\\frac{n!}{n_1!n_2!\\cdots n_r!}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-with-replacement",
    "href": "posts/combinatorics/index.html#sampling-with-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling with replacement",
    "text": "Sampling with replacement\n\nOrder of selection matters\nThe number of arrangements, called dispositions can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at all the other stages, the choices are still \\(n\\). Applying the principle of counting, we have:\n\\[\nD^*(n,k)=n^k\n\\tag{3}\\]\n\n\nOrder of selection does not matter\nThe number of arrangements, called partitions is given by:\n\\[\nC^*(n,k)=\\frac{(n+k-1)!}{k!(n-1)!}={n+k-1\\choose k}\n\\tag{4}\\]\nTo understand the formula of Equation 4, think of writing \\(C^*(n,k)\\) when, for instance, \\(n=6,k=4\\) as, say, \\(a_1a_2a_3a_2\\) or equivalently \\(a_1*a_2**a_3*a_4\\), where any element \\(a_i,i=1,2,\\cdots,n\\) is followed by a number of asterisks equal to the number of its occurrences in the sequence (the total number of asterisks in the equivalent representation needs to be equal to \\(k\\)). Another example: \\(a_2a_3a_3a_3\\) and \\(a_1a_2*a_3***a_4\\). It is observed that a one-to-one correspondence exists between the original arrangements and all possible permutations in the alignment of elements and asterisks in the equivalent representation. Since each alignment starts with \\(a_1\\), we need to permute \\(n+k-1\\) elements, among which \\(k\\) (the asterisks) and \\(n-1\\) (the elements \\(a_i,i=2,\\cdots,n\\)) are equal."
  },
  {
    "objectID": "posts/combinatorics/index.html#formulae",
    "href": "posts/combinatorics/index.html#formulae",
    "title": "Pills of combinatorics",
    "section": "Formulae",
    "text": "Formulae\nIn conclusion, the four different arrangements of \\(k\\) out of a collection of \\(n\\) objects can be computed as follows:\n\\[\n\\begin{split}\n&\\quad\\quad\\textbf{without replacement}&\\quad\\quad\\textbf{with replacement}\\\\\n\\\\\n\\textbf{order matters}&\\quad\\quad\\text{permutations}&\\quad\\quad\\text{dispositions}\\\\\n&\\quad\\quad D(n,k)=\\dfrac{n!}{(n-k)!}&\\quad\\quad D^*(n,k)=n^k\\\\\n\\textbf{order does not matter}&\\quad\\quad\\text{combinations}&\\quad\\quad\\text{partitions}\\\\\n&\\quad\\quad C(n,k)={n\\choose k}&\\quad\\quad C^*(n,k)={n+k-1\\choose k}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#exercises",
    "href": "posts/combinatorics/index.html#exercises",
    "title": "Pills of combinatorics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Given a group of \\(n\\) individuals, count how many subgroups can be formed having one particular person as the leader, and a number (from 0 to \\(n-1\\)) of additional members.\nAnswer The counting principle can be applied as follows. First, we have \\(n\\) possible choices for the leader, and once the leader is chosen, \\(2^{n-1}\\) subsets can be considered, which may include from none to all the remaining individuals of the group. We have therefore:\n\\[\nN=n2^{n-1}\n\\]\n\n\nExercise 2 How many words that consists of four distinct letters can be formed?\nAnswer The sample space is composed of \\(n=26\\) elements. The sequences we are interested are formed by \\(k=4\\) elements, repetitions are not allowed. Two words are considered distinct based on the order of appearance of the four letters into them. The number of words is then given by Equation 1:\n\\[\nN=D(n,k)=\\frac{n!}{(n-k)!}=\\frac{26!}{22!}=26\\cdot25\\cdot24\\cdot23=358,800\n\\]\n\n\nExercise 3 How many combinations exist using two letters out of four letters?\nAnswer The sample is composed of \\(n=4\\) elements. The sequences we are interested are formed by \\(k=2\\) distinct elements, repetitions are not allowed. Two sequences are considered the same if they differ just by the order of the elements in them, namely \\(a_1a_2\\) is the same as \\(a_2a_1\\). The number of combinations is then given by Equation 2:\n\\[\nN=C(n,k)={n\\choose k}=\\frac{4!}{2!2!}=6\n\\]\n\n\nExercise 4 How many distinct combinations can be formed from the letters ATALANTA?\nAnswer There are \\(n=8\\) letters which may be arranged in \\(n!\\) ways, but this leads to double counting. If the \\(k_1=4\\) “A”s are permuted, then nothing is changed, and similarly for the \\(k_2=2\\) “T”s. The total number of distinct combinations is then given by the multinomial coefficient:\n\\[\nN=\\frac{n!}{k_1!k_2!}=\\frac{8!}{4!2!}=7\\cdot6\\cdot5\\cdot4=840\n\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html",
    "href": "posts/multivariate_probability_regions/index.html",
    "title": "Multivariate probability regions",
    "section": "",
    "text": "Consider the case of a multivariate random variable \\(\\mathbf{X}\\in\\mathbb{R}^d\\), whose distribution is normal with mean value \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{C}\\):\n\\[\n\\mathbf{C}=\\begin{bmatrix}\\sigma_1^2&\\cdots&\\sigma_{1d}\\\\\\vdots&\\ddots&\\vdots\\\\\\sigma_{d1}&\\cdots&\\sigma^2_d\\end{bmatrix}\n\\]\nfrom which we draw a sample of \\(n\\) data points:\n\\[\n\\mathbf{x}_i=\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}\n\\]\nWe suppose that either the mean value or the covariance matrix are unknown; they can be estimated using the sample mean, denoted by \\(\\bar{\\mathbf{x}}\\):\n\\[\n\\bar{\\mathbf{x}}=\\begin{bmatrix}\\bar{x}_1\\\\\\vdots\\\\\\bar{x}_d\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\n\\]\nand the sample covariance matrix, denoted by \\(\\mathbf{\\Sigma}\\):\n\\[\n\\mathbf{\\Sigma}=\\{\\,\\sigma_{jk},j,k=1,\\cdots d\\,\\}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n\\]\nwhose \\(jk\\)th element is written:\n\\[\n\\Sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)\n\\]\nThe \\(95\\%\\) prediction interval is a \\(d\\)-dimensional prediction hyperellipsoid, which, formally, is written as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{1}\\]\nwhere \\(\\mathbf{x}_{\\text{new}}\\) is the new observation for which the prediction interval is desired. \\(F_{0.95,n,n-d}\\) is the quantile with probability 0.95 of the Fisher’s \\(F\\)-distribution with \\(d\\) and \\((n-d)\\) degrees of freedom.\nEquation 1 is the formula for predicting the next single observation where we only have estimates of the mean value and the covariance matrix from the sample.\nAnother interesting formula can be elaborated as for the computation of the volume of the prediction hyperellipsoid. Recall that the prediction hyperellipsoid is the transformation of the hypersphere of radius\n\\[\nr=\\sqrt{F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{2}\\]\nby the linear transform of matrix \\(\\Sigma^{1/2}\\). Let \\(V\\) the volume of the hypersphere of radius \\(r\\) in an \\(d\\)-dimensional space. The volume of the hyperellipsoid can be obtained from \\(V\\) by multiplying with the determinant of the linear transform:\n\\[\nV_d=\\underbrace{\\frac{2}{d}\\frac{\\pi^{d/2}}{\\Gamma(d/2)}r^d}_{V}\\sqrt{\\det(\\Sigma)}\n\\tag{3}\\]\n\n\n\n\n\n\nUnivariate and bivariate random variables\n\n\n\nUnivariate random variable\nWhen \\(d=1\\), Equation 1 reads:\n\\[\n\\frac{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert}{s}\\leq\\sqrt{F_{0.95,1,n-1}^{\\phantom{'}}}\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{4}\\]\nwhere \\(s\\) is the sample standard deviation:\n\\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nIt is noted that\n\\[\nF_{0.95,1,n-1}=t_{0.975,n-1}^2\n\\tag{5}\\]\nwhere \\(t_{0.975,n-1}\\) is the quantile with probability 0.975 of the \\(t\\)-Student distribution with \\(n-1\\) degrees of freedom. Therefore Equation 4 can be written:\n\\[\n\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq t_{0.975,n-1}s\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{6}\\]\nFor large \\(n\\), when the central limit theorem can be applied, Equation 6 can be simplified:\n\\[\n\\boxed{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq z_{0.975}s}\n\\]\nwhere \\(z_{0.975}=1.96\\) is the quantile with probability 0.975 of the standard normal distribution.\nBivariate random variable\nWhen \\(d=2\\), Equation 1 reads:\n\\[\n(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq2F_{0.95,2,n-2}\\frac{(n-1)(n+1)}{n(n-2)}\n\\tag{7}\\]\nSince \\(2F_{0.95,2,\\infty}=\\chi^2(2)\\), for large \\(n\\) we can simplity Equation 7 as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq\\chi^2(0.95,2)}\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#prediction-hyperellipsoid",
    "href": "posts/multivariate_probability_regions/index.html#prediction-hyperellipsoid",
    "title": "Multivariate probability regions",
    "section": "",
    "text": "Consider the case of a multivariate random variable \\(\\mathbf{X}\\in\\mathbb{R}^d\\), whose distribution is normal with mean value \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{C}\\):\n\\[\n\\mathbf{C}=\\begin{bmatrix}\\sigma_1^2&\\cdots&\\sigma_{1d}\\\\\\vdots&\\ddots&\\vdots\\\\\\sigma_{d1}&\\cdots&\\sigma^2_d\\end{bmatrix}\n\\]\nfrom which we draw a sample of \\(n\\) data points:\n\\[\n\\mathbf{x}_i=\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}\n\\]\nWe suppose that either the mean value or the covariance matrix are unknown; they can be estimated using the sample mean, denoted by \\(\\bar{\\mathbf{x}}\\):\n\\[\n\\bar{\\mathbf{x}}=\\begin{bmatrix}\\bar{x}_1\\\\\\vdots\\\\\\bar{x}_d\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\n\\]\nand the sample covariance matrix, denoted by \\(\\mathbf{\\Sigma}\\):\n\\[\n\\mathbf{\\Sigma}=\\{\\,\\sigma_{jk},j,k=1,\\cdots d\\,\\}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n\\]\nwhose \\(jk\\)th element is written:\n\\[\n\\Sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)\n\\]\nThe \\(95\\%\\) prediction interval is a \\(d\\)-dimensional prediction hyperellipsoid, which, formally, is written as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{1}\\]\nwhere \\(\\mathbf{x}_{\\text{new}}\\) is the new observation for which the prediction interval is desired. \\(F_{0.95,n,n-d}\\) is the quantile with probability 0.95 of the Fisher’s \\(F\\)-distribution with \\(d\\) and \\((n-d)\\) degrees of freedom.\nEquation 1 is the formula for predicting the next single observation where we only have estimates of the mean value and the covariance matrix from the sample.\nAnother interesting formula can be elaborated as for the computation of the volume of the prediction hyperellipsoid. Recall that the prediction hyperellipsoid is the transformation of the hypersphere of radius\n\\[\nr=\\sqrt{F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{2}\\]\nby the linear transform of matrix \\(\\Sigma^{1/2}\\). Let \\(V\\) the volume of the hypersphere of radius \\(r\\) in an \\(d\\)-dimensional space. The volume of the hyperellipsoid can be obtained from \\(V\\) by multiplying with the determinant of the linear transform:\n\\[\nV_d=\\underbrace{\\frac{2}{d}\\frac{\\pi^{d/2}}{\\Gamma(d/2)}r^d}_{V}\\sqrt{\\det(\\Sigma)}\n\\tag{3}\\]\n\n\n\n\n\n\nUnivariate and bivariate random variables\n\n\n\nUnivariate random variable\nWhen \\(d=1\\), Equation 1 reads:\n\\[\n\\frac{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert}{s}\\leq\\sqrt{F_{0.95,1,n-1}^{\\phantom{'}}}\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{4}\\]\nwhere \\(s\\) is the sample standard deviation:\n\\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nIt is noted that\n\\[\nF_{0.95,1,n-1}=t_{0.975,n-1}^2\n\\tag{5}\\]\nwhere \\(t_{0.975,n-1}\\) is the quantile with probability 0.975 of the \\(t\\)-Student distribution with \\(n-1\\) degrees of freedom. Therefore Equation 4 can be written:\n\\[\n\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq t_{0.975,n-1}s\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{6}\\]\nFor large \\(n\\), when the central limit theorem can be applied, Equation 6 can be simplified:\n\\[\n\\boxed{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq z_{0.975}s}\n\\]\nwhere \\(z_{0.975}=1.96\\) is the quantile with probability 0.975 of the standard normal distribution.\nBivariate random variable\nWhen \\(d=2\\), Equation 1 reads:\n\\[\n(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq2F_{0.95,2,n-2}\\frac{(n-1)(n+1)}{n(n-2)}\n\\tag{7}\\]\nSince \\(2F_{0.95,2,\\infty}=\\chi^2(2)\\), for large \\(n\\) we can simplity Equation 7 as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq\\chi^2(0.95,2)}\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#concentration-ellipse",
    "href": "posts/multivariate_probability_regions/index.html#concentration-ellipse",
    "title": "Multivariate probability regions",
    "section": "Concentration ellipse",
    "text": "Concentration ellipse\nEquation 8 is the equation of the \\(95\\%\\) prediction ellipse for a bivariate normal distribution, whose sample mean value and sample covariance matrix are calculated from a large-size dataset. The ellipse is also called the concentration ellipse. Using Equation 5 the area \\(A=V_2\\) of the concentration ellipse can be computed from Equation 23 when \\(n\\) is large as follows:\n\\[\n\\boxed{A=\\pi\\sqrt{\\det(\\Sigma)}\\,\\chi^2_{0.95,2}}\n\\tag{9}\\]\n\nExample 1 (Simulation) The dataset is composed of 1000 points simulated from a bivariate normal distribution with mean value and covariance matrix:\n\\[\n\\mathbf{\\mu}=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\quad\\mathbf{C}=\\begin{bmatrix}5&2\\\\2&4\\end{bmatrix}\n\\]\nI used the function rmvnorm() from the package rvnorm for the simulation of the dataset, and the function stat_ellipse() of ggplot2 for drawing the ellipse, Figure 1. Using stat_ellipse() dispensed me from computing eigenvalues and eigenvectors of the sample covariance matrix, which are needed to evaluate the orientation and the eccentricity of the ellipse in the Cartesian plane. This computation can be based on a simple principal component analysis (PCA).\n\n\nCode\nlibrary(tidyverse)\nlibrary(mvtnorm)\n\nset.seed(1492)\nsigma &lt;- matrix(c(5, 2, 2, 4), ncol = 2)\nn &lt;- 1000\nx &lt;- rmvnorm(n = n, mean = c(1, 2), sigma = sigma)\n\ndist &lt;- qchisq(0.95, 2)                # to define the 95% probability contour\nm &lt;- matrix(data = c(0, 0), ncol = 1) \nfor (i in 1:2) m[i] &lt;- mean(x[, i])    # sample mean value\nC &lt;- cov(x)                            # sample covariance matrix\nS &lt;- solve(C)                          # inverse of C\nQ &lt;- numeric(n)                        \nfor (i in 1:n) Q[i] &lt;- t(x[i, ] - m) %*% S %*% (x[i, ] - m) # computation of the quadratic form\n\ndf &lt;- data.frame(x, Q)\ndf &lt;- df %&gt;%\n  mutate(inside = Q &lt;= dist)\naest &lt;- round(100*(1 - sum(!df$inside)/n), 2) # estimated coverage\narea &lt;- pi*dist*sqrt(det(C)) # area of the concentration ellipse \n\nmy_theme = theme(\n    axis.title.x = element_text(size = 16),\n    axis.text.x = element_text(size = 14),\n    axis.title.y = element_text(size = 16),\n    axis.text.y = element_text(size = 14),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14),\n    panel.border = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(colour = \"black\"),\n    aspect.ratio = 1,\n    legend.position = \"none\")\n\nggplot(df, aes(x = X1, y = X2, color = inside)) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  geom_point(size = 1) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", level = 0.95, \n               color = \"blue\", fill = \"blue\", alpha = 0.2) +\n  scale_x_continuous(limits = c(-10, 10)) + \n  scale_y_continuous(limits = c(-10, 10)) + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 1: The curve corresponding to the value \\(\\chi^2(0.95,2)=5.99\\) contains \\(95\\%\\) of the variable pairs; the points falling outside the concentration ellipse are plotted in red.\n\n\n\n\n\nThe coverage of the concentration ellipse is 94.9%. Moreover, the code calculates the area enclosed by the concentration ellipse, Equation 9: 74 (arbitrary units)."
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#an-application-to-posturography",
    "href": "posts/multivariate_probability_regions/index.html#an-application-to-posturography",
    "title": "Multivariate probability regions",
    "section": "An application to posturography",
    "text": "An application to posturography\nPostural control is often quantified by recording the trajectory of the center of pressure (CoP) during human quiet standing. This quantification has many important applications, including the assessment of balance disorders and the early detection of balance degradation to prevent falls in geriatric populations. The CoP trajectory in time, reported as the anteroposterior (AP) component (forward/backward) and the mediolateral (ML) component (left/right) paired together to form the so-called stabilogram, is typically acquired by means of a force platform, on top of which tested subjects are asked to stand in their upright posture for a while, typically 60 s, as still as possible. Manipulation of testing conditions, e.g., presence or absence of vision, different kind of surfaces for the feet to stay on are possible, so as to gain further insights about the postural control system performance.\nAmong the many CoP features that have been introduced in the literature, few of them have to do with the concentration ellipse of the CoP. E.g., the principal sway direction, to represent the relative contribution of the ML and AP components to the CoP fluctuations, in terms of the angle between the AP axis and the direction of the main eigenvector produced by the PCA. Another feature of interest, the one considered in this post, is the area of the concentration ellipse, also called the sway area (Schubert and Kirchner 2014). For instance, an increase in this feature value among elderly people has been associated with a higher risk of fall. I want to show here an example of calculation of the sway area using the code above, by taking an exemplary stabilogram from a recently published dataset (Santos et al. 2017), Figure 2.\n\n\nCode\nlibrary(ggpubr)\n\ndata &lt;- read.table(file = 'PDS01OR1grf.txt', header = TRUE, sep = \"\\t\")\nt &lt;- data$Time                  # 6000 samples acquired at a rate of 100 Hz\nCoP_AP &lt;- data$COPNET_X*1e2     # anteroposterior component (AP), cm\nCoP_ML &lt;- data$COPNET_Z*1e2     # mediolateral component (ML), cm\nCoP_AP &lt;- CoP_AP - mean(CoP_AP) # detrending\nCoP_ML &lt;- CoP_ML - mean(CoP_ML) # detrending\n\ndf &lt;- data.frame(time = t, x_ap = CoP_AP, x_ml = CoP_ML)\n\np_ap &lt;- ggplot(df, aes(x = time, y = x_ap)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"time, s\",\n       y = \"AP direction, cm\") + \n  scale_y_continuous(limits = c(-2, 2)) +\n  my_theme\n\np_ml &lt;- ggplot(df, aes(x = time, y = x_ml)) +\n  geom_line(color = \"red\") +\n  labs(x = \"time, s\",\n       y = \"ML direction, cm\") + \n  scale_y_continuous(limits = c(-2, 2)) +\n  my_theme\n\nggarrange(p_ap, p_ml)\n\n\n\n\n\n\n\n\nFigure 2: Anteroposterior and mediolateral components of the CoP for the subject with ID 1 from the Duarte’s dataset, tested Open-Eyes on a Rigid surface (first trial out of three).\n\n\n\n\n\nThe stabilogram with the concentration ellipse superimposed on it are shown in Figure 3.\n\n\nCode\nx &lt;- cbind(df$x_ml, df$x_ap) # detrending already done\nd &lt;- qchisq(0.95, 2)         # to define the 95% probability contour\nn &lt;- nrow(x)\nC &lt;- cov(x)                  # sample covariance matrix\nS &lt;- solve(C)                # inverse of C\nQ &lt;- numeric(n)                        \nfor (i in 1:n) Q[i] &lt;- t(x[i, ]) %*% S %*% x[i, ] # computation of the quadratic form\n\ndf_Q &lt;- data.frame(x, Q)\ndf_Q &lt;- df_Q %&gt;%\n  mutate(inside = Q &lt;= dist)\naest &lt;- round(100*(1 - sum(!df_Q$inside)/n), 2) # estimated coverage\narea &lt;- pi*dist*sqrt(det(C)) # area of the concentration ellipse \n\nggplot(df_Q, aes(x = X1, y = X2, color = inside)) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  geom_point(size = 0.5) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", level = 0.95, \n               color = \"blue\", fill = \"blue\", alpha = 0.2) +\n  labs(x = \"ML component, cm\",\n       y = \"AP component, cm\") +\n  scale_x_continuous(limits = c(-2, 2)) + \n  scale_y_continuous(limits = c(-2, 2)) + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 3: Exemplary stabilogram with superimposed the concentration ellipse - parts of the CoP trajectory outside the concentration ellipse are displayed in red.\n\n\n\n\n\nThe coverage of the concentration ellipse is 97%. The sway area is 3.09 cm\\(^2\\)."
  },
  {
    "objectID": "posts/parallel_processing_GPU/index.html",
    "href": "posts/parallel_processing_GPU/index.html",
    "title": "Supercharge Deep Learning in R with a hybrid R–Colab workflow",
    "section": "",
    "text": "Exploring deep learning in R with the Keras and tensorflow packages reveals an intuitive and powerful interface. With just a few lines of R code, deep neural networks can be built, trained, and evaluated for tasks such as image recognition, time series forecasting, or natural language processing—all while maintaining the efficiency and flexibility of a tidyverse-powered workflow.\nBut here’s the catch: training deep learning models on a typical laptop or single-CPU machine can be painfully slow. GPUs are built to handle the massive parallelism required for deep learning, but most R users don’t have one sitting idle in their local setup.\nEnter Google Colab—a free, cloud-based environment that provides access to powerful NVIDIA GPUs, without the need for installation or cost. It offers the experience of renting a high-performance machine for deep learning, directly from a browser. For many R users, this opens up a new realm of possibilities, enabling models that would take hours to train on a local machine to run in mere minutes.\nBut there’s one little twist: Colab speaks Python. The environment is built for the Python ecosystem, especially tools like TensorFlow, PyTorch, and Keras—all in their native Python form.\nThis doesn’t mean R users are excluded. Instead, it offers an opportunity to combine the strengths of both environments. All data preparation, exploration, and evaluation can be done in R using tools like dplyr, ggplot2, and yardstick, while the deep learning model training can be delegated to Colab. Colab serves as an outsourced training assistant—working seamlessly in Python.\nThis hybrid approach requires some initial setup, but once established, it becomes smooth, scalable, and remarkably efficient. R can be used for data wrangling, visualization, and model evaluation—leveraging the power of the tidyverse. When it’s time to train the model, the heavy lifting can be offloaded to Colab, with its GPU handling the computations. The two environments—R and Python—communicate through shared files, ensuring a flexible, fast, and efficient workflow."
  },
  {
    "objectID": "posts/parallel_processing_GPU/index.html#full-mnist-classification-pipeline-in-r",
    "href": "posts/parallel_processing_GPU/index.html#full-mnist-classification-pipeline-in-r",
    "title": "Supercharge Deep Learning in R with a hybrid R–Colab workflow",
    "section": "Full MNIST Classification Pipeline in R",
    "text": "Full MNIST Classification Pipeline in R\nThe MNIST (Modified National Institute of Standards and Technology) dataset is a widely used benchmark for testing machine learning algorithms, particularly in the field of deep learning. It consists of 70,000 handwritten digit images, each labeled with the corresponding digit (0-9). The images are 28x28 pixels in grayscale, making them a relatively simple task for modern machine learning models, though still an effective tool for evaluating model performance.\nThe following R code snippet presents the entire pipeline for the MNIST classification task using Keras for deep learning. This pipeline includes:\n\nData Preparation: Loading and preprocessing the MNIST dataset, including normalization and reshaping of image data to fit the model.\nModel Definition: Constructing a deep neural network (DNN) using Keras, with layers for dense processing, dropout regularization, and softmax output.\nModel Training: Compiling and fitting the model to the training data, while using a validation split for model evaluation.\nPerformance Evaluation: Timing the training process and calculating the overall duration.\nExporting Data: Saving both the training and test datasets, as well as predictions, for further analysis or use in external systems.\n\nThe implementation also ensures reproducibility by setting a fixed random seed across the entire workflow, and leverages the tidyverse for data manipulation and visualization, alongside the Keras library for deep learning.\nThis pipeline provides a streamlined approach to model development, training, and evaluation while integrating seamlessly with Python-based environments (such as Google Colab) for GPU-accelerated training.\nMinimal setup\nThis section outlines the essential libraries for the project and specifies the base directory path at which Google Drive is mounted, enabling file access within both R and Colab-based Python environments.\n\n\nCode\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(reticulate)\n\nlibrary(tidyverse)\nlibrary(yardstick)\nlibrary(readr)\n\n# Define Google Drive base path\ngdrive_base &lt;- \"~/Library/CloudStorage/GoogleDrive-&lt;your-mail&gt;@gmail.com/My Drive/&lt;your-project-folder&gt;\"\n\n\nLoad and preprocess the MNIST dataset\nThe dataset is reshaped to match the input requirements of a fully connected (dense) neural network architecture. Additionally, class labels are one-hot encoded to facilitate categorical classification during model training.\n\n\nCode\nmnist &lt;- dataset_mnist()\n\nx_train &lt;- mnist$train$x / 255\nx_test  &lt;- mnist$test$x / 255\ny_train &lt;- mnist$train$y\ny_test  &lt;- mnist$test$y\n\n# Reshape for dense model (flatten 28x28 images)\nx_train &lt;- array_reshape(x_train, c(nrow(x_train), 784))\nx_test  &lt;- array_reshape(x_test, c(nrow(x_test), 784))\n\n# One-hot encode labels\ny_train_cat &lt;- to_categorical(y_train, 10)\ny_test_cat  &lt;- to_categorical(y_test, 10)\n\n\nDefine model\nThe model is defined using the Keras API as a sequential architecture composed of fully connected layers. Following its construction, the model is compiled by specifying the loss function, optimization algorithm, and evaluation metrics, thereby preparing it for training.\nDropout is used to help the model generalize better and avoid overfitting. During training, it randomly turns off a portion of the neurons in the network. This forces the model to learn more robust features instead of relying too heavily on any one part of the network.\ntf$keras$optimizers$legacy$Adam() is used to maintain the original behavior of the Adam optimizer, which can be especially valuable on M1/M2 MacBook architectures where newer versions might encounter compatibility issues. Newer versions changed how optimizers work, so the legacy version ensures compatibility with older code and stable training results.\n\n\nCode\nmodel &lt;- keras_model_sequential(list(\n  layer_dense(units = 128, activation = \"relu\", input_shape = 784),\n  layer_dropout(rate = 0.4),\n  layer_dense(units = 64, activation = \"relu\"),\n  layer_dropout(rate = 0.3),\n  layer_dense(units = 10, activation = \"softmax\")\n))\n\nmodel %&gt;% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"adam\", #tf$keras$optimizers$legacy$Adam(),\n  metrics = \"accuracy\"\n)\n\n\nTraining and time the training\nSetting a seed ensures reproducibility of training results, while timing the training process helps assess the efficiency of the model’s learning and optimization.\nThe validation split refers to the portion of the training data set aside during training to evaluate model performance on unseen data, helping to detect overfitting (and tune hyperparameters, though not considered in this post).\nThe number of epochs is set to 10. For the purposes of this post, I will not address whether the training process adequately accounts for the issue of overfitting and generalization.\n\n\nCode\n# Set seed for reproducibility across R, TensorFlow, NumPy\nset.seed(123)\ntensorflow::set_random_seed(123)\n\n# Time the training\nstart_time &lt;- Sys.time()\n\nmodel %&gt;% fit(\n  x_train, y_train_cat,\n  epochs = 10,\n  batch_size = 256,\n  validation_split = 0.2,\n  verbose = FALSE\n)\n\nend_time &lt;- Sys.time()\n\nduration &lt;- c()\nx &lt;- end_time-start_time\nduration[1] &lt;- as.numeric(regmatches(x,regexpr(\"\\\\d*\\\\.?\\\\d+\",x)))\n\n\nExport training and testing sets to Drive\nThe training and testing sets saved to Drive will be used later in a Colab notebook for GPU-based training of the same model defined in Keras above.\n\n\nCode\n# Convert data to data frames with labels\ncolnames(x_train) &lt;- paste0(\"pixel_\", seq_len(ncol(x_train)))\ncolnames(x_test)  &lt;- paste0(\"pixel_\", seq_len(ncol(x_test)))\n\ntrain_df &lt;- as_tibble(cbind(label = y_train, x_train))\ntest_df  &lt;- as_tibble(cbind(label = y_test, x_test))\n\nwrite_csv(train_df, file.path(gdrive_base, \"data\", \"mnist_train.csv\"))\nwrite_csv(test_df, file.path(gdrive_base, \"data\", \"mnist_test.csv\"))\n\n\nPredict in R and export\nAfter making predictions in R, we can export the results and visualize the confusion matrix to evaluate the model’s performance. We also compute and export the accuracy, along with the training time, to track the model’s efficiency.\n\n\nCode\n# Only needed if you want to compare R vs Colab predictions.\n\npreds_r &lt;- model %&gt;% predict(x_test, verbose = FALSE) %&gt;% k_argmax()\npreds_df &lt;- tibble(\n  actual = y_test,\n  predicted = as.array(preds_r)\n)\n\n# Convert to factors\npreds_df &lt;- preds_df %&gt;%\n  mutate(\n    actual = factor(actual),\n    predicted = factor(predicted, levels = levels(actual))\n  )\n\nacc &lt;- c()\nacc[1] &lt;- accuracy(preds_df, truth = actual, estimate = predicted)$.estimate\nconf_mat(preds_df, truth = actual, estimate = predicted) %&gt;% \n    autoplot(type = \"heatmap\") +\n    ggtitle(\"R-based DNN confusion matrix for the MNIST dataset\")\n\n\n\n\n\n\n\n\n\nCode\nwrite_csv(preds_df, file.path(gdrive_base, \"data\", \"predictions_r.csv\"))\n\n\n(Later) Compare with Colab predictions\nThe predictions made in R can be compared with those generated in Colab to assess the consistency and performance of the model across different environments.\n\nOpen the notebook in Google Colab.\nAt the top menu bar, click “Runtime”\nSelect “Change runtime type”\nIn the “Hardware accelerator” dropdown menu, choose:\n\n\nGPU to enable GPU support\nTPU if you want to experiment with TPU acceleration (more advanced)\nNone to use CPU only\n\n\nClick “Save”\nColab will restart the kernel and launch an environment with the selected hardware.\n\n\n\nCode\n# After Colab finishes and saves 'predictions_colab.csv':\npreds_colab &lt;- read_csv(file.path(gdrive_base, \"data\", \"predictions_colab.csv\"))\n\n# Convert to factors\npreds_colab &lt;- preds_colab %&gt;%\n  mutate(\n    actual = factor(actual),\n    predicted = factor(predicted, levels = levels(actual))\n  )\n\nacc[4] &lt;- accuracy(preds_colab, truth = actual, estimate = predicted)$.estimate\nconf_mat(preds_colab, truth = actual, estimate = predicted) %&gt;% \n    autoplot(type = \"heatmap\") +\n    ggtitle(\"Python-based DNN confusion matrix for the MNIST dataset\")\n\n\n\n\n\n\n\n\n\nCode\n# Optionally print Colab training time (if saved)\nduration[4] &lt;- read_lines(file.path(gdrive_base, \"data\", \"colab_duration.txt\"))\n\n\nComparative analysis\nAfter running the model for 10 epochs—with all other elements of the pipeline held constant—I repeated the classification task using the DNN trained for 50 and 100 epochs. The results, in terms of accuracy and training time, are presented in the table below. Although not strictly necessary — or even correct — I used three decimal places to highlight subtle differences in accuracy observed in the experiments. Moreover, I chose to round the training time to the nearest integer.\n\n\n\nComparative analysis\n \n  \n    Method \n    Epoch \n    Accuracy, % \n    Time, s \n  \n \n\n  \n    R-based \n    10 \n    0.972 \n    5 \n  \n  \n    R-based \n    50 \n    0.977 \n    26 \n  \n  \n    R-based \n    100 \n    0.979 \n    54 \n  \n  \n    Python-based \n    10 \n    0.974 \n    33 \n  \n  \n    Python-based \n    50 \n    0.979 \n    72 \n  \n  \n    Python-based \n    100 \n    0.979 \n    128 \n  \n\n\n\n\n\nEven with matching seeds, data splits, architectures, and all the same training settings in R and Python, small differences in accuracy still appeared (compare the confusion matrices above). These are likely due to subtle differences in weight initialization, batch shuffling, and dropout masking between the two frameworks. Such variations are common in deep learning and typically don’t affect overall performance trends.\nWhy MacBook CPUs (M1/M2) can beat Colab GPUs for MNIST\n\nModel + Dataset are too small for GPU to shine: MNIST has only 60,000 grayscale 28×28 images, and the DNN models used here have a relatively limited number of parameters to learn (109386). With only 100k parameters, each training step is very fast on modern CPUs and the overhead of using a GPU — loading the data, launching kernels, moving data around — starts to outweigh the benefits of parallelism.\nApple Silicon (M1/M2) is extremely optimized: M1/M2 chips have Unified Memory (RAM + GPU share memory), fast memory access, and dedicated matrix multiplication engines (AMX). TensorFlow and PyTorch now have native Metal backends or use Accelerate.framework under the hood on macOS, enabling them to be blazingly fast even without explicit GPU use.\nColab GPUs aren’t top-tier GPUs: Colab’s free tier provides K80 or T4 GPUs, which are relatively old or mid-tier and often shared among users. Their performance can fluctuate depending on server load, whereas a MacBook runs everything locally without such interruptions.\nOverhead on the GPU pipeline: the GPU has to load the data, transfer it to GPU memory, perform the computation, and then transfer the results back. This overhead adds latency unless the batch size and model complexity are large enough to justify the transfer."
  },
  {
    "objectID": "posts/parallel_processing_GPU/index.html#mnist-classification-training-in-python",
    "href": "posts/parallel_processing_GPU/index.html#mnist-classification-training-in-python",
    "title": "Supercharge Deep Learning in R with a hybrid R–Colab workflow",
    "section": "MNIST Classification Training in Python",
    "text": "MNIST Classification Training in Python\nIn the snippet below, I present an example of a *.ipynb notebook that facilitates the sharing of input and output files with a local R project via a Google Drive synced folder.\n\n\nCode\n# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport time\n\n# Set seed\nnp.random.seed(123)\ntf.random.set_seed(123)\n\n# Load training and testing data from Drive\ngdrive_path = \"/content/drive/MyDrive/parallel_processing_colab/data\"\ntrain_df = pd.read_csv(f\"{gdrive_path}/mnist_train.csv\")\ntest_df = pd.read_csv(f\"{gdrive_path}/mnist_test.csv\")\n\nx_train = train_df.drop(columns = [\"label\"]).values.astype(\"float32\")\ny_train = to_categorical(train_df[\"label\"].values, num_classes = 10)\n\nx_test = test_df.drop(columns = [\"label\"]).values.astype(\"float32\")\ny_test = test_df[\"label\"].values\n\n# Define the same model as in R\nmodel = Sequential([\n    Dense(128, activation = 'relu', input_shape = (784, )),\n    Dropout(0.4),\n    Dense(64, activation = 'relu'),\n    Dropout(0.3),\n    Dense(10, activation = 'softmax')\n])\n\nmodel.compile(\n    loss = 'categorical_crossentropy',\n    optimizer = \"adam\",\n    metrics = ['accuracy']\n)\n\n# Time the training\nstart = time.time()\n\nmodel.fit(\n    x_train, y_train,\n    epochs = 10,\n    batch_size = 128,\n    validation_split = 0.2,\n    verbose = 2\n)\n\nend = time.time()\nduration = end - start\nprint(\"Training time (seconds):\", duration)\n\n# Save Colab training time to Google Drive\nwith open('/content/drive/MyDrive/parallel_processing_colab/data/colab_duration.txt', 'w') as f:\n    f.write(str(duration))\n    \n# Predict on test set and export predictions\npreds = model.predict(x_test)\npred_labels = np.argmax(preds, axis = 1)\n\n# Save predictions to Drive\npreds_df = pd.DataFrame({\n    \"actual\": y_test,\n    \"predicted\": pred_labels\n})\npreds_df.to_csv(f\"{gdrive_path}/predictions_colab.csv\", index = False)\n\n# Evaluate metrics (optional here)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_labels))\nprint(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred_labels))"
  },
  {
    "objectID": "posts/parallel_processing_GPU/index.html#summary",
    "href": "posts/parallel_processing_GPU/index.html#summary",
    "title": "Supercharge Deep Learning in R with a hybrid R–Colab workflow",
    "section": "Summary",
    "text": "Summary\nWhen working with deep learning tasks, timing considerations between R and Python (especially in Colab) can significantly impact performance.\n\nHardware acceleration\n\n\nR: limited GPU support unless utilizing packages like tensorflow, often relying on the CPU for training, leading to longer processing times for complex models.\nPython (Colab): seamless GPU support accelerates training dramatically, reducing training times from hours to minutes.\n\n\nExecution times\n\n\nR: training on a CPU can be slow for deep learning tasks. Execution time can be measured using system.time().\nPython (Colab): with GPU enabled, training times are drastically reduced. Timing can be monitored using Python’s time module.\n\n\nColab overheads\n\n\nInitialization time: setting up Colab (mounting Drive, loading data, GPU setup) adds some overhead, especially when starting a new session.\nSession limitations: free GPU access has time limits and potential disconnects, requiring reinitialization.\nFile access delays: reading and writing to Google Drive in Colab can slow down the workflow compared to local file systems.\nCold start for GPU: initializing GPU resources adds delay when starting a session.\n\n\nHybrid workflow: a hybrid R-Python workflow maximizes the strengths of both ecosystems. R handles data preprocessing, feature engineering, evaluation, and visualization, while Python in Colab handles heavy-duty model training with GPU support. This combination offers a flexible and efficient solution, especially for large models and datasets.\nTotal execution time: while Colab’s GPU accelerates training, overheads like initialization and session limitations can add to the overall time. For simpler models or smaller datasets, running everything locally may be more efficient, but for larger models, the speedup from Colab’s GPU outweighs the overheads.\n\nWhat I’ve shown here is how to leverage Colab’s GPU support for deep learning while staying within the comfort of R, taking full advantage of the tidyverse ecosystem for analysis and visualization. In a future post, I’ll circle back to this topic—probably with a hands-on example using LSTM for time series forecasting, where Colab’s GPU support will really get a chance to shine compared to single-CPU runs.\n\nIn summary, the hybrid R + Python approach offers a balance of computational power (via Colab’s GPUs) and flexibility (using R for preprocessing and evaluation), though the Colab overheads should be considered when designing the workflow.\n\nParallel icons created by juicy_fish - Flaticon"
  },
  {
    "objectID": "posts/spell_checking/index.html",
    "href": "posts/spell_checking/index.html",
    "title": "Spell checking using hunspell",
    "section": "",
    "text": "Spell checking text consists of the following steps:\n\nParse a document by extracting (tokenizing) words that we want to check\nAnalyze each word by breaking it down in its root (stemming) and conjugation affix\nLookup in a dictionary if the word+affix combination if valid for the stated language\n(optional) For incorrect words, suggest corrections by finding similar (correct) words in the dictionary\n\n\n\nLet us suppose that the text to be spell checked is composed of a char array, with each element of the array being a single word expressed in the specified language (Italian, here). A custom dictionary can be set in the dict parameter when functions hunspell_* are invoked.\n\nlibrary(hunspell)\n\n# check individual words\nwords   &lt;- c(\"amore\", \"ammore\", \"prof\", \"professsore\")\ncorrect &lt;- hunspell_check(words, dict = dictionary(\"it_IT\"))\nprint(correct)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n# incorrect words\nincorrect.words &lt;- words[!correct]\n# find suggestions for incorrect words\nsuggested.words &lt;- hunspell_suggest(incorrect.words, dict = dictionary(\"it_IT\"))\nsuggested.words &lt;- unlist(lapply(suggested.words, function(x) x[1]))\n\nNote that the outcome of the function hunspell_suggest() is a list. An array of corrected words can then be constructed for the corresponding misspelled words. Clearly, some words are not the best choice, and the first word is usually the best alternative. The quality of data can be improved by careful exploration of the list.\n\n\n\nSpell checking text\n \n  \n    word \n    best alternative \n  \n \n\n  \n    ammore \n    amore \n  \n  \n    professsore \n    professore"
  },
  {
    "objectID": "posts/spell_checking/index.html#spell-checking",
    "href": "posts/spell_checking/index.html#spell-checking",
    "title": "Spell checking using hunspell",
    "section": "",
    "text": "Spell checking text consists of the following steps:\n\nParse a document by extracting (tokenizing) words that we want to check\nAnalyze each word by breaking it down in its root (stemming) and conjugation affix\nLookup in a dictionary if the word+affix combination if valid for the stated language\n(optional) For incorrect words, suggest corrections by finding similar (correct) words in the dictionary\n\n\n\nLet us suppose that the text to be spell checked is composed of a char array, with each element of the array being a single word expressed in the specified language (Italian, here). A custom dictionary can be set in the dict parameter when functions hunspell_* are invoked.\n\nlibrary(hunspell)\n\n# check individual words\nwords   &lt;- c(\"amore\", \"ammore\", \"prof\", \"professsore\")\ncorrect &lt;- hunspell_check(words, dict = dictionary(\"it_IT\"))\nprint(correct)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n# incorrect words\nincorrect.words &lt;- words[!correct]\n# find suggestions for incorrect words\nsuggested.words &lt;- hunspell_suggest(incorrect.words, dict = dictionary(\"it_IT\"))\nsuggested.words &lt;- unlist(lapply(suggested.words, function(x) x[1]))\n\nNote that the outcome of the function hunspell_suggest() is a list. An array of corrected words can then be constructed for the corresponding misspelled words. Clearly, some words are not the best choice, and the first word is usually the best alternative. The quality of data can be improved by careful exploration of the list.\n\n\n\nSpell checking text\n \n  \n    word \n    best alternative \n  \n \n\n  \n    ammore \n    amore \n  \n  \n    professsore \n    professore"
  },
  {
    "objectID": "posts/spell_checking/index.html#an-example",
    "href": "posts/spell_checking/index.html#an-example",
    "title": "Spell checking using hunspell",
    "section": "An example",
    "text": "An example\nLet us suppose that we want to analyze the lyrics of the song titled “Il mio canto libero” (here). One single occurrence of the words “emozioni”, “nuda” and “pianto” was intentionally misspelled, namely, “emozzioni”, “nudda”, “painto” were transcripted, respectively.\nTo get the list of words used in the lyrics, I first split the text into individual words. To do so, I use unnest_tokens() from the tidytext package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringi)\nlibrary(tidytext)\n\n# data upload\nlyrics &lt;- stri_read_lines(\"Il mio canto libero.txt\")\ndf_pre &lt;- data.frame(text = lyrics) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  select(id, text)\n# tokenization\ntokenized_text_pre &lt;- df_pre %&gt;%\n  unnest_tokens(output = word, token = \"words\", input = text) %&gt;%\n  count(word, sort = TRUE)\n# check spelling\ncorrect &lt;- hunspell_check(tokenized_text_pre$word, dict = dictionary(\"it_IT\"))\n# incorrect words\nincorrect.words &lt;- tokenized_text_pre$word[!correct]\n# find suggestions for incorrect words\nsuggested.words &lt;- hunspell_suggest(incorrect.words, dict = dictionary(\"it_IT\"))\nsuggested.words &lt;- unlist(lapply(suggested.words, function(x) x[1]))\n# list of incorrect and suggested words\nword.list &lt;- as.data.frame(cbind(word = incorrect.words, `best alternative` = suggested.words))\n\n\n\n\n\nSpell checking text - 'Il mio canto libero'\n \n  \n    word \n    best alternative \n  \n \n\n  \n    emozzioni \n    emozioni \n  \n  \n    nudda \n    nuda \n  \n  \n    painto \n    pianto \n  \n\n\n\n\n\nThe misspelled words in the song lyrics can be replaced with the suggested alternative using the code below.\n\n\nCode\nincorrect.whole.words &lt;- paste0(\"\\\\b\", word.list$word, \"\\\\b\")\nlyrics &lt;- stri_replace_all_regex(lyrics, incorrect.whole.words, suggested.words, \n                                 vectorize_all = FALSE)\n\n\nThe count of the words that were misspelled is correctly updated after spell checking and replacement.\n\n\n\nBefore spell checking\n \n  \n    word \n    n \n  \n \n\n  \n    emozioni \n    1 \n  \n  \n    emozzioni \n    1 \n  \n  \n    nuda \n    1 \n  \n  \n    nudda \n    1 \n  \n  \n    painto \n    1 \n  \n\n\n\n\n\n\nAfter spell checking\n \n  \n    word \n    n \n  \n \n\n  \n    emozioni \n    2 \n  \n  \n    nuda \n    2 \n  \n  \n    pianto \n    1 \n  \n\n\n\n\n\nThe thumbnail image is credited to Text mining icons created by Freepik - Flaticon"
  },
  {
    "objectID": "posts/audio_features/index.html",
    "href": "posts/audio_features/index.html",
    "title": "Audio features for free",
    "section": "",
    "text": "In this post, I’ll be exploring how to pull back audio features and other information available for any track in the Spotify library. This information is usually hidden to users, however it can be easily retrieved. This will be done here using the spotifyr R wrapper package, whose description sounds as a quick and easy wrapper for pulling back audio features from Spotify’s Web API in bulk. Information about the various audio features offered by Spotify in the analysis of each track, as well as what all those features actually mean, is part of the discussion herein.\nThe first step in accessing Spotify data is to get an API key. This can be done by first logging into the Personal Dashboard available in the “Spotify for Developers” page (this requires that I signed on to a valid account, of course!) - see Figure 1.\nFrom within the ‘Basic information page’ of the app lyrics I have created in my Dashboard, I was directed to select “Create a Client ID”. After filling out the required questions, I received my Client ID and Client Secret. The following code chunk is then needed to get the Spotify access token:\nlibrary(spotifyr)\n\nid &lt;- 'my client ID'\nsecret &lt;- 'my client secret'\n\nSys.setenv(SPOTIFY_CLIENT_ID = id)\nSys.setenv(SPOTIFY_CLIENT_SECRET = secret)\n\naccess_token &lt;- get_spotify_access_token()\nFor one of the several playlists that I have created using my Spotify account, namely trial, I invoke the function get_user_playlists() to obtain the associated playlist_id. This is possible once I have retrieved the information about the desired playlist using, e.g., the verb filter from tidyverse, as in the code chunk below, where how to use the Spotify ID to complete the authentication process is also shown.\nlibrary(tidyverse)\n\nmy_id &lt;- 'my Spotify ID'\nplaylist_id &lt;- get_user_playlists(my_id) %&gt;%\n  filter(name == 'trial') %&gt;%\n  pull(id)\nAll is done. Now, a number of functions are available in spotifyr to get access to the audio features of all the tracks contained in the playlist trial.\ntracks &lt;- get_playlist_tracks(playlist_id = playlist_id, \n                              limit = 100, \n                              authorization = access_token)\nfeatures &lt;- get_track_audio_features(tracks,\n                                     authorization = access_token)\nThe call to get_playlist_tracks() with limit fixed to 100 (default maximum value) clarifies that only data of up 100 tracks can be retrieved from any Spotify playlist using just one call. The argument authorization requires a valid access token, as explained above. With the function get_track_audio_features() we finally get the values of the audio features. These numerical representations of various characteristics of each music track of interest are derived, behind the scenes, through the advanced audio analysis algorithms developed by Spotify as an important part of their business strategy. It is worth noting that tracks contains a wealth of other relevant information, including track title, author, album, release date and so forth."
  },
  {
    "objectID": "posts/audio_features/index.html#audio-features",
    "href": "posts/audio_features/index.html#audio-features",
    "title": "Audio features for free",
    "section": "Audio features",
    "text": "Audio features\nThe following table reports a list of the audio features, see here. Since my native language is Italian, I translate each English term (in bold) into a corresponding Italian term (in italic.) Next to the column Audio feature, the column Explanation provides information regarding what each audio feature can mean.\n\n\n\nAudio feature\nExplanation\n\n\n\n\nacousticness (acusticità)\nIndex from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence that the track is acoustic, i.e., with little or no electronic elements.\n\n\ndanceability (ballabilità)\nIndex from 0.0 to 1.0 describing how suitable a track is for dancing. The index is based on a combination of tempo, rhythm stability, beat strength and overall regularity. A higher danceability score indicates a more danceable track.\n\n\nduration_ms (durata)\nTrack duration, in ms.\n\n\nenergy (energia)\nIndex from 0.0 to 1.0, which helps representing the intensity and activity level of a track. Tracks with high energy values tend to be more upbeat and energetic (they sound faster, louder and noisier than tracks with lower energy). Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\ninstrumentalness (proporzione di parte strumentale su parte vocale)\nIndex from 0.0 to 1.0, which helps predicting whether a track contains no vocals. A higher instrumentalness values indicates a higher likelihood that the track is purely instrumental. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n\nkey (chiave) (*)\nThe estimated overall key of the track. Integers, from -1 to 11, map to pitches using standard Pitch Class notation. E.g., 0 = C (do), 1 = C♯/D♭ (do diesis/re bemolle), 2 = D (re) up to 11 = B (si). If no key is detected, the value is -1.\n\n\nliveness (vivacità)\nIndex from 0.0 to 1.0, which helps detecting the presence of an audience in the recording. A higher liveness values suggests the track was performed live.\n\n\nloudness (presenza)\nThis index represents the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Typical values range between -60 and 0 dB.\n\n\nmode (tonalità) (*)\nMode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor by 0.\n\n\nspeechiness (proporzione di parlato)\nIndex from 0.0 to 1.0, which helps identifying tracks that contain spoken words. Higher values indicate more speech-like sounds, distinguishing them from purely instrumental tracks. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech. Values below 0.33 most likely represent music and other non-speech-like tracks.\n\n\ntempo (tempo)\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\ntime signature (indicazione del tempo) (*)\nIt yiels an estimated overall time signature for a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of “3/4”, to “7/4”.\n\n\nvalence (positività emotiva)\nAlso known as the “positiveness” index, valence describes, in a range from 0.0 to 1.0, the musical positiveness conveyed by a track. Tracks with higher valence values tend to sound more positive, cheerful, and euphoric, while lower scores suggest more negative, sad, or depressing emotions.\n\n\n\n(*) Key, mode and time signature relate to the fundamental musical components of a track - these features are essential for understanding its harmonic structure and rhythmic pattern.\nAnother parameter that can be retrieved is named popularity. The popularity of a track will be between 0 and 100, with 100 being the most popular. The popularity is calculated by an algorithm and is mostly based on the total number of plays the track has had and how recent those plays are. Generally speaking, tracks that are being played a lot now will have a higher popularity than tracks that were played a lot in the past."
  },
  {
    "objectID": "posts/audio_features/index.html#example",
    "href": "posts/audio_features/index.html#example",
    "title": "Audio features for free",
    "section": "Example",
    "text": "Example\nAs a matter of example, my Spotify’s playlist trial contains just four tracks. They were extracted from those published by a beloved singer of mine, Sergio Caputo in his debut album “Un Sabato Italiano”, which was released in 1983. The magazine “Rolling Stone Italia” ranks the album “Un Sabato Italiano” at the 37th position in the list of the greatest 100 Italian albums of all time (here).\nThe songs in this album are a sort of diary of the life that the singer lived in the early Eighties. We can enjoy the chance to find ourselves immersed with him and his great friend Rino into smoky bars and complicated love stories, dealt with in noisy never-ending Saturday nights. Prevalent in all the tracks is the rhythm of swing and jazz. The high quality of the lyrics concurs, with the music, to evoke brilliantly those atmospheres, and even today the album can be considered a remarkable testimonianza of the mood of the early Eighties.\nFor the sake of better visualization, I grouped the results of the audio analysis of these tracks in the following three tables; moreover, I condensed the values of the three parameters key, mode and time signature into only one cell.\n\n\n\nKey, mode, time signature, tempo, duration and loudness\n\n\ntrack title\nkey-mode-time signature\ntempo, BPM\nduration, min\nloudness, dB\n\n\n\n\nMercy bocù\nF major in 3/4\n116.19\n4.06\n-14.09\n\n\nBimba se sapessi\nF major in 4/4\n153.76\n3.45\n-13.70\n\n\nMettimi giù\nG major in 4/4\n76.35\n4.35\n-18.34\n\n\nSpicchio di luna\nG major in 4/4\n77.73\n3.18\n-14.47\n\n\n\n\n\n\n\n\nAcousticness, instrumentalness, liveness and speechiness\n\n\ntrack title\nacousticness\ninstrumentalness\nliveness\nspeechiness\n\n\n\n\nMercy bocù\n0.40\n0\n0.15\n0.05\n\n\nBimba se sapessi\n0.30\n0\n0.18\n0.06\n\n\nMettimi giù\n0.12\n0\n0.09\n0.07\n\n\nSpicchio di luna\n0.47\n0\n0.11\n0.09\n\n\n\n\n\n\n\n\nDanceability, energy, valence and popularity\n\n\ntrack title\ndanceability\nenergy\nvalence\npopularity\n\n\n\n\nMercy bocù\n0.60\n0.40\n0.42\n26\n\n\nBimba se sapessi\n0.55\n0.48\n0.88\n32\n\n\nMettimi giù\n0.67\n0.46\n0.89\n19\n\n\nSpicchio di luna\n0.40\n0.36\n0.14\n26\n\n\n\n\n\n\n\n\nTranslating music in numbers as exemplified above, might be, and actually is, the starting point of some heavy data mining for doing interesting research at the intersection between several different domains, which may include data science, statistics, machine learning, sociology, psychology, neuroscience, just to mention a few of them. Something more about these opportunities will be discussed in future posts … up to now, that’s my two cents for the day!\n\nThe thumbnail image is credited to Spotify icons created by Creative - Flaticon"
  },
  {
    "objectID": "posts/tone_burst/index.html",
    "href": "posts/tone_burst/index.html",
    "title": "Frequency spectrum of a sine-wave tone burst",
    "section": "",
    "text": "A sine-wave tone burst signal is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Sine-wave tone burst of two cycles on, four cycles off.\n\n\n\nIt is formed by “turning on and off” (gating) a sinusoidal signal (here). The sinusoidal signal, with fundamental period \\(T_b\\) (frequency: \\(f_b = 1/T_b\\)) and initial phase \\(\\theta_b\\), is assumed here to have unit amplitude:\n\\[\nx_b(t)=\\sin\\left(2\\pi\\dfrac{t}{T_b}+\\theta_b\\right)\n\\]\nGating amounts to multiply it by \\(x_p(t)\\), which can be described as the time-delayed rectangular function \\(\\text{rect}(t/T_g)\\), periodized by the fundamental period \\(T_r\\):\n\\[\nx_p(t)=\\sum_{n=-\\infty}^{+\\infty}\\text{rect}\\left(\\dfrac{t-\\tau_r-nT_r}{T_g}\\right)\n\\]\nwhere \\(\\tau_r\\) is the time-shift of the rectangular function before submitting it to periodization. Henceforth, we assume that \\(T_r&gt;T_b\\), and a multiple integer of full cycles of \\(x_b(t)\\) is captured in either the time interval \\(T_g\\) or the time interval \\(T_r\\): in Figure 1, \\(T_g=2T_b\\) (\\(N=2\\)) and \\(T_r=6T_b\\) (\\(M=4\\), hence \\(N+M=6\\)).\n\n\n\n\n\n\nRectangular function\n\n\n\nThe rectangular function (also known as the rect function, gate function) is defined as:\n\\[\n\\text{rect}\\left(\\dfrac{t}{T}\\right)=\\left\\{\n\\begin{align}\n1,&\\quad\\vert t\\vert\\leq T/2\\\\\n1/2,&\\quad t=T/2\\\\\n0,&\\quad t&gt;T/2\n\\end{align}\n\\right.\n\\]\nThe Continuous Time Fourier Transform of the rectangular function \\(\\text{rect}(t/T)\\) is:\n\\[\nX(f)=T\\,\\text{sinc}(fT)\n\\]\n\n\n\n\n\n\n\n\nSinc function\n\n\n\nIn digital signal processing and information theory, the sinc function is commonly defined for \\(x\\neq0\\) by\n\\[\n\\text{sinc}(x)=\\dfrac{\\sin(\\pi x)}{\\pi x}\n\\]\n\n\nThe duty-ratio factor is defined as the ratio of time the tone is ON compared to the total time when it is either ON or OFF, namely \\(D=N/(N+M)=N/L\\). The duty-ratio factor approaches 0 for widely spaced, narrow bursts or 1 for closely spaced, wide bursts: when \\(D=1\\), \\(x_r(t)=1\\) and \\(x(t)=x_b(t)\\) (pure sine-wave).\nWhen the duty-ratio factor is a rational number, the sine-wave tone burst \\(x(t)\\) can be modeled as the product of two periodical signals, namely \\(x_b(t)\\) (with period \\(T_b\\)) and \\(x_r(t)\\) (with period \\(T_r\\)):\n\\[\nx(t)=x_r(t)\\cdot x_b(t)\n\\]\nThe fundamental period of \\(x(t)\\) is \\(T=T_r\\). We can appropriately choose the phase shift \\(\\theta_b\\) and the time delay \\(\\tau_r\\) so as to yield a signal \\(x(t)\\) without discontinuities and further endowed with odd symmetry (e.g., \\(\\theta_b=0\\), \\(\\tau_r=0\\)). Under these conditions, \\(x(t)\\) can be regarded as composed by infinitely many replicas (with period \\(T_r\\)) of the time-limited signal \\(x_p(t)\\):\n\\[\nx_p(t)=\\left\\{\n\\begin{align}\n\\sin\\left(2\\pi\\dfrac{t}{T_b}\\right),&\\quad\\vert t\\vert\\leq T_g/2\\\\\n0,&\\quad\\text{elsewhere}\n\\end{align}\n\\right.\n\\]"
  },
  {
    "objectID": "posts/tone_burst/index.html#sine-wave-tone-burst",
    "href": "posts/tone_burst/index.html#sine-wave-tone-burst",
    "title": "Frequency spectrum of a sine-wave tone burst",
    "section": "",
    "text": "A sine-wave tone burst signal is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Sine-wave tone burst of two cycles on, four cycles off.\n\n\n\nIt is formed by “turning on and off” (gating) a sinusoidal signal (here). The sinusoidal signal, with fundamental period \\(T_b\\) (frequency: \\(f_b = 1/T_b\\)) and initial phase \\(\\theta_b\\), is assumed here to have unit amplitude:\n\\[\nx_b(t)=\\sin\\left(2\\pi\\dfrac{t}{T_b}+\\theta_b\\right)\n\\]\nGating amounts to multiply it by \\(x_p(t)\\), which can be described as the time-delayed rectangular function \\(\\text{rect}(t/T_g)\\), periodized by the fundamental period \\(T_r\\):\n\\[\nx_p(t)=\\sum_{n=-\\infty}^{+\\infty}\\text{rect}\\left(\\dfrac{t-\\tau_r-nT_r}{T_g}\\right)\n\\]\nwhere \\(\\tau_r\\) is the time-shift of the rectangular function before submitting it to periodization. Henceforth, we assume that \\(T_r&gt;T_b\\), and a multiple integer of full cycles of \\(x_b(t)\\) is captured in either the time interval \\(T_g\\) or the time interval \\(T_r\\): in Figure 1, \\(T_g=2T_b\\) (\\(N=2\\)) and \\(T_r=6T_b\\) (\\(M=4\\), hence \\(N+M=6\\)).\n\n\n\n\n\n\nRectangular function\n\n\n\nThe rectangular function (also known as the rect function, gate function) is defined as:\n\\[\n\\text{rect}\\left(\\dfrac{t}{T}\\right)=\\left\\{\n\\begin{align}\n1,&\\quad\\vert t\\vert\\leq T/2\\\\\n1/2,&\\quad t=T/2\\\\\n0,&\\quad t&gt;T/2\n\\end{align}\n\\right.\n\\]\nThe Continuous Time Fourier Transform of the rectangular function \\(\\text{rect}(t/T)\\) is:\n\\[\nX(f)=T\\,\\text{sinc}(fT)\n\\]\n\n\n\n\n\n\n\n\nSinc function\n\n\n\nIn digital signal processing and information theory, the sinc function is commonly defined for \\(x\\neq0\\) by\n\\[\n\\text{sinc}(x)=\\dfrac{\\sin(\\pi x)}{\\pi x}\n\\]\n\n\nThe duty-ratio factor is defined as the ratio of time the tone is ON compared to the total time when it is either ON or OFF, namely \\(D=N/(N+M)=N/L\\). The duty-ratio factor approaches 0 for widely spaced, narrow bursts or 1 for closely spaced, wide bursts: when \\(D=1\\), \\(x_r(t)=1\\) and \\(x(t)=x_b(t)\\) (pure sine-wave).\nWhen the duty-ratio factor is a rational number, the sine-wave tone burst \\(x(t)\\) can be modeled as the product of two periodical signals, namely \\(x_b(t)\\) (with period \\(T_b\\)) and \\(x_r(t)\\) (with period \\(T_r\\)):\n\\[\nx(t)=x_r(t)\\cdot x_b(t)\n\\]\nThe fundamental period of \\(x(t)\\) is \\(T=T_r\\). We can appropriately choose the phase shift \\(\\theta_b\\) and the time delay \\(\\tau_r\\) so as to yield a signal \\(x(t)\\) without discontinuities and further endowed with odd symmetry (e.g., \\(\\theta_b=0\\), \\(\\tau_r=0\\)). Under these conditions, \\(x(t)\\) can be regarded as composed by infinitely many replicas (with period \\(T_r\\)) of the time-limited signal \\(x_p(t)\\):\n\\[\nx_p(t)=\\left\\{\n\\begin{align}\n\\sin\\left(2\\pi\\dfrac{t}{T_b}\\right),&\\quad\\vert t\\vert\\leq T_g/2\\\\\n0,&\\quad\\text{elsewhere}\n\\end{align}\n\\right.\n\\]"
  },
  {
    "objectID": "posts/tone_burst/index.html#fourier-analysis",
    "href": "posts/tone_burst/index.html#fourier-analysis",
    "title": "Frequency spectrum of a sine-wave tone burst",
    "section": "Fourier analysis",
    "text": "Fourier analysis\nThe Fourier series of \\(x(t)\\) is defined as a trigonometric series of the form:\n\\[\nx(t)=A_0+\\sum_{n=1}^{+\\infty}A_n\\cos\\left(2\\pi \\dfrac{n}{T_r}t\\right)+\\sum_{i=1}^{+\\infty}B_n\\sin\\left(2\\pi \\dfrac{n}{T_r}t\\right)\n\\]\nwhere the Fourier series coefficients are defined by the integrals:\n\\[\n\\left\\{\n\\begin{align}\nA_0&=\\dfrac{1}{T_r}\\int_{-T_{r}/2}^{T_{r}/2}x(t)\\,dt\\\\\nA_n&=\\dfrac{2}{T_r}\\int_{-T_{r}/2}^{T_{r}/2}x(t)\\cos\\left(2\\pi\\dfrac{nt}{T_r}\\right)\\,dt\\\\\nB_n&=\\dfrac{2}{T_r}\\int_{-T_{r}/2}^{T_{r}/2}x(t)\\sin\\left(2\\pi\\dfrac{nt}{T_r}\\right)\\,dt\n\\end{align}\n\\right.\n\\]\nSince \\(x(t)\\) is odd-symmetric, \\(A_0\\) is null (i.e., the sine-wave tone burst has no DC component) and \\(A_n=0,n=1,\\cdots,+\\infty\\). The harmonic coefficients \\(B_n\\) are given by:\n\\[\nB_n=\\dfrac{4}{T_r}\\int_0^{T_g/2}\\sin\\left(2\\pi\\dfrac{t}{T_b}\\right)\\sin\\left(2\\pi\\dfrac{nt}{T_r}\\right)dt\n\\]\nwhere \\(T_r=LT_b\\) (\\(L=N+M\\)) and \\(T_g=N T_b\\). To make the notation lighter, \\(T_b=T\\) in the following. The spectrum of the sine-wave tone burst requires the calculation of the Fourier integrals:\n\\[\nB_n=\\dfrac{4}{LT}\\int_{0}^{N T/2}\\sin\\left(\\dfrac{2\\pi}{T}t\\right)\\sin\\left(\\dfrac{2\\pi}{T}\\dfrac{n}{L}t\\right)dt\n\\]\n\n\n\n\n\n\nWerner formulas\n\n\n\nThe Werner formulas are the trigonometric product formulas:\n\\[\n\\begin{align}\n2\\sin\\alpha\\cos\\beta&=\\sin(\\alpha-\\beta)+\\sin(\\alpha+\\beta)\\\\\n2\\cos\\alpha\\cos\\beta&=\\cos(\\alpha-\\beta)+\\cos(\\alpha+\\beta)\\\\\n2\\cos\\alpha\\sin\\beta&=\\sin(\\alpha+\\beta)-\\sin(\\alpha-\\beta)\\\\\n2\\sin\\alpha\\sin\\beta&=\\cos(\\alpha-\\beta)-\\cos(\\alpha+\\beta)\n\\end{align}\n\\]\n\n\nApplying the Werner formulas, the integrand can be written:\n\\[\n\\sin\\left(\\dfrac{2\\pi}{T}t\\right)\\sin\\left(\\dfrac{2\\pi}{T}\\dfrac{n}{L}t\\right)=\\dfrac{1}{2}\\left[\\cos\\left(\\dfrac{2\\pi}{T}\\left(1-\\dfrac{n}{L}\\right)t\\right)-\\cos\\left(\\dfrac{2\\pi}{T}\\left(1+\\dfrac{n}{L}\\right)t\\right)\\right]\n\\]\nTherefore:\n\\[\n\\begin{align}\nB_n &= \\dfrac{2}{LT}\\dfrac{1}{\\dfrac{2\\pi}{T}\\left(1-\\dfrac{n}{L}\\right)}\\sin\\left(\\dfrac{2\\pi}{T}\\left(1-\\dfrac{n}{L}\\right)t\\right)\\biggr\\vert_0^{NT/2}\\\\\n&-\\dfrac{2}{LT}\\dfrac{1}{\\dfrac{2\\pi}{T}\\left(1+\\dfrac{n}{L}\\right)}\\sin\\left(\\dfrac{2\\pi}{T}\\left(1+\\dfrac{n}{L}\\right)t\\right)\\biggr\\vert_0^{NT/2}\n\\end{align}\n\\]\nand\n\\[\n\\begin{align}\nB_n &= \\dfrac{2}{\\pi L}\\dfrac{1}{\\left(1-\\dfrac{n}{L}\\right)}\\sin\\left(\\pi N\\left(1-\\dfrac{n}{L}\\right)\\right)-\\dfrac{2}{\\pi L}\\dfrac{1}{\\left(1+\\dfrac{n}{L}\\right)}\\sin\\left(\\pi N\\left(1+\\dfrac{n}{L}\\right)\\right)\\\\\n&=\\dfrac{N}{L}\\left[\\text{sinc}\\left(N\\left(1-\\dfrac{n}{L}\\right)\\right)-\\text{sinc}\\left(N\\left(1+\\dfrac{n}{L}\\right)\\right)\\right]\n\\end{align}\n\\]\nUsing the duty-ratio factor, we have:\n\\[\nB_n=D\\left[\\text{sinc}(D(L-n))-\\text{sinc}(D(L+n))\\right]\n\\]\nWhen \\(D=1\\) (i.e., \\(x_r(t)=1\\)), we get \\(B_n = 0\\) for all \\(n\\), but \\(B_L=1\\). Recall that the \\(L\\)th harmonic component in the spectrum of \\(x(t)\\) corresponds to the fundamental frequency of the sine-wave, \\(f_b\\) - as noted above, if \\(x_r(t)=1\\), then \\(x(t)=x_b(t)\\). For any value of \\(D\\) different from 1, the maximum value of \\(B_n\\), \\(B_{\\text{max}}=D\\) still occurs when \\(n=L\\), however other lines are present in the spectrum. These lines are at the (analog) frequencies \\(f_n=f_b+(n-L)f_r\\)."
  },
  {
    "objectID": "posts/tone_burst/index.html#examples",
    "href": "posts/tone_burst/index.html#examples",
    "title": "Frequency spectrum of a sine-wave tone burst",
    "section": "Examples",
    "text": "Examples\nA sine-wave tone burst with duty-ratio factor \\(D=0.25\\) (\\(f_b=3\\,\\text{Hz},f_r=0.25\\,\\text{Hz},T_b=1\\,\\text{s}\\)) was simulated in MATLAB. The sampling frequency chosen for the simulation was \\(f_s=80\\,\\text{Hz}\\). The Fast Fourier Transform (FFT) algorithm was applied for spectrum computation. The guidelines discussed in one previous post of mine (here) were followed to prevent spectral leakage and amplitude ambiguity. Superimposed on the values computed by the FFT (solid black points) is the envelope obtained by interpolating the discrete spectrum \\(B_n\\) (see above), Figure 2.\n\n\n\n\n\n\nFigure 2: The main part of the envelope of the spectrum of a sine-wave tone burst at 3 Hz.\n\n\n\nThe MATLAB code was run again in a scenario where the frequency of the sine wave moved up to \\(f_b=60\\,\\text{Hz}\\), with a corresponding adjustment of the sampling frequency (\\(f_s=800\\,\\text{Hz}\\)). The results of the FFT-based spectrum analysis are reported in Figure 3.\n\n\n\n\n\n\nFigure 3: The main part of the envelope of the spectrum of a sine-wave tone burst at 60 Hz.\n\n\n\nThe FFT-based spectrum can be also interpreted as the outcome of the following cascaded frequency-domain signal-processing operations:\n\nfrequency sampling the spectrum \\(T_g\\,\\text{sinc}(f\\,T_g)\\) - this is explained by the time periodization of the rectangular function \\(\\text{rect}(t/T_g)\\) with period \\(T_r\\).\nfrequency shifting the frequency-sampled spectrum - this is explained by the amplitude modulation impressed on the periodized rectangular function when the latter is multiplied by the sine wave at frequency \\(f_b\\)."
  },
  {
    "objectID": "posts/numerical_simulation/index.html",
    "href": "posts/numerical_simulation/index.html",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "",
    "text": "We consider here the class of stochastic processes known as diffusion processes, which are solutions to stochastic differential equation (SDE) of the form:\n\\[\ndX(t)=b(t,X(t))\\,dt+\\sigma(t,X(t))\\,dW(t)\n\\tag{1}\\]\nwith some initial condition \\(X(0)\\) which can be regarded as being either constant or random. The SDE is interpreted in the Itô sense:\n\\[\nX(t)=X(0)+\\int_0^tb(s,X(s))\\,ds+\\int_0^t\\sigma(s,X(s))\\,dW(s)\n\\tag{2}\\]\nand \\(\\{W(t),t\\in[0,T]\\}\\) is the so-called Brownian motion or Wiener process.\nThe two deterministic functions \\(b(\\cdot,\\cdot)\\) and \\(\\sigma(\\cdot,\\cdot)\\) are called, respectively, the drift and the diffusion coefficients of the SDE. Under a number of assumptions regarding their properties (i.e., global Lipschitz and linear growth, see (Iacus 2008) for mathematical details) the SDE has a unique solution with continuous sample paths such that\n\\[\nE\\left[\\int_0^T\\vert\\,X(t)\\,\\vert^2\\,dt\\right]&lt;\\infty\n\\tag{3}\\]\nFor the sake of simpler notation, \\(X(t)\\rightarrow X_t\\) is used in the following, therefore Equation 1 and Equation 2 can be written:\n\\[\n\\left\\{\n\\begin{split}\ndX_t&=b(t,X_t)\\,dt+\\sigma(t,X_t)\\,dW_t\\\\\nX_t&=X_0+\\int_0^tb(s,X_s)\\,ds+\\int_0^t\\sigma(s,X_s)\\,dW_s\n\\end{split}\n\\right.\n\\tag{4}\\]\nThe Itô formula is an important tool of stochastic calculus. It can be regarded as the stochastic version of the Taylor expansion of a function \\(f(t,X_t)\\) stopped at the second order, where \\(X_t\\) is a diffusion process. If the function \\(f(\\cdot,\\cdot)\\) is a twice differentiable function on both arguments, we have:\n\\[\ndf(t,X_t)=f_t(t,X_t)\\,dt+f_x(t,X_t)\\,dX_t+\\frac{1}{2}f_{xx}(t,X_t)(dX_t)^2\n\\tag{5}\\]\nwhere:\n\\[\nf_t(t,x)=\\dfrac{\\partial}{\\partial t}f(t,x),\\quad f_x(t,x)=\\dfrac{\\partial f(t,x)}{\\partial x},\\quad f_{xx}(t,x)=\\dfrac{\\partial^2 f(t,x)}{\\partial x^2}\n\\]\nUsing Equation 1 and the Itô lemma, which states that \\((dW_t)^2=dt\\), we have:\n\\[\n(dX_t)^2=\\left[b(t,X_t)\\,dt+\\sigma(t,X_t)\\,dW_t\\right]^2=\\sigma^2(t,X_t)\\,dt+\\text{O}(dt^{3/2})\n\\tag{6}\\]\nEquation 5 can thus be written:\n\\[\ndf(t,X_t)=\\left[f_t(t,X_t)+f_x(t,X_t)b(t,X_t)+\\frac{1}{2}f_{xx}(t,X_t)\\right]dt+f_x(t,X_t)\\sigma(t,X_t)\\,dW_t\n\\tag{7}\\]"
  },
  {
    "objectID": "posts/numerical_simulation/index.html#approximation-methods",
    "href": "posts/numerical_simulation/index.html#approximation-methods",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "Approximation methods",
    "text": "Approximation methods\nTo apply a numerical method to the generic SDE of Equation 1 over the time interval \\([0,T]\\), the time interval needs to be discretized (Higham 2001). Let \\(\\Delta t=T/M\\) for some positive integer \\(M\\), and \\(t_n=n\\Delta t\\). The numerical approximation to \\(X(t_n)\\) will be denoted \\(X_n\\) (\\(n=1,\\cdots,M\\)).\nThe Euler-Maruyama method takes the form:\n\\[\n\\left\\{\n\\begin{split}\nX_n&=X_{n-1}+b(X_{n-1})\\Delta t+\\sigma(X_{n-1})\\Delta W_{n-1}\\\\\nX_0&=x_0\n\\end{split}\n\\right.\n\\tag{8}\\]\nIt is worth noting that in the case when \\(\\sigma(X_t)=0\\), then the problem becomes deterministic: this approximation method reduces to the standard Euler method for the ordinary differential equation \\(\\dot{X}_t=b(t,X_t)\\).\nThe Milstein’s method adds a correction to the stochastic increment to produce the solution to the SDE. The correction arises because the Taylor expansion must be modified in the case of Itô calculus. Truncating the Taylor expansion to the second order at an appropriate point yields:\n\\[\n\\left\\{\n\\begin{split}\nX_n&=X_{n-1}+b(t_{n-1},X_{n-1})\\Delta t+\\sigma(t_{n-1},X_{n-1})\\Delta W_{n-1}\\\\\n&+\\dfrac{1}{2}\\sigma(X_{n-1})\\sigma_x(X_{n-1})\\left[(\\Delta W_{n-1})^2-\\Delta t\\right]\\\\\nX_0&=x_0\n\\end{split}\n\\right.\n\\tag{9}\\]\n\n\n\n\n\n\nStrong and weak convergence\n\n\n\nThe methods of approximation of the continuous solution to an SDE are classified according to their different properties. Mainly two criteria of optimality are used in the literature: the strong and the weak (orders of) convergence.\nA time-discretized approximation \\(X_\\delta\\) of a continuous-time process \\(X\\), with \\(\\Delta t\\) the time increment of the discretization, is said to be of strong order of convergence \\(\\gamma\\) to \\(X\\) if for any fixed time horizon \\(T\\) it holds true that\n\\[\nE[\\,\\vert X_{\\Delta t}(\\tau)-X(\\tau)\\vert\\,]\\leq C\\Delta t^\\gamma,\\quad\\forall\\Delta t&lt;\\Delta t_0\n\\tag{10}\\]\nfor any \\(\\tau\\in[0,T]\\), with \\(\\Delta t_0&gt;0\\) and \\(C\\) a constant that does not depend on \\(\\Delta t\\).\nAlong with the strong convergence, the weak convergence can also be defined. In the same conditions as above, \\(X_{\\Delta t}\\) is said to converge weakly of order \\(\\gamma\\) to \\(X\\) if for any fixed horizon \\(T\\) and any \\(2(\\beta+1)\\) continuous differentiable function \\(g\\) of polynomial growth, it holds true that\n\\[\n\\vert\\,E_g[X_{\\Delta t}(\\tau)]-E_g[X(\\tau)]\\,\\vert\\leq C\\Delta t^\\beta\n\\tag{11}\\]\nfor any \\(\\tau\\in[0,T]\\), with \\(\\Delta t_0&gt;0\\) and \\(C\\) a constant that does not depend on \\(\\Delta t\\).\nWhereas the strong order of convergence measures the rate at which the mean of the error decays as \\(\\Delta t\\rightarrow0\\), the weak order of convergence measures the rate of decay of the error of the means.\nMethods of approximation of some order that strongly converge usually have a higher order of weak convergence. This is the case with the Euler-Maruyama method, which is strongly convergent of order \\(\\gamma=1/2\\) and weakly convergent of order \\(\\beta=1\\) (under some smoothness conditions on the coefficients of the SDE). The Milstein’s method has both weak and strong order of convergence, \\(\\gamma=\\beta=1\\), which is superior to the Euler–Maruyama method.\nIt is interesting noting that, when the diffusion coefficient does not depend on the state variable of the process, the Euler-Maruyama and Milstein’s methods coincide. This is one important case, therefore, when the Euler-Maruyama method is strongly convergent of order \\(\\gamma=1\\)."
  },
  {
    "objectID": "posts/numerical_simulation/index.html#numerical-test",
    "href": "posts/numerical_simulation/index.html#numerical-test",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "Numerical test",
    "text": "Numerical test\n\n\n\n\n\n\nGeometric Brownian motion\n\n\n\nThe geometric Brownian motion (GBM) is the solution to the SDE:\n\\[\n\\left\\{\n\\begin{split}\ndX_t&=\\lambda\\,X_t\\,dt+\\mu\\,X_t\\,dW_t\\\\\nX_0&=x_0\n\\end{split}\n\\right.\n\\tag{12}\\]\nwhere the parameter \\(\\lambda\\) is interpreted as the constant interest rate and \\(\\mu&gt;0\\) as the volatility. For a GBM, the drift and diffusion coefficients are both linearly related to the state variable \\(X\\), namely \\(b(t,X_t)=\\lambda\\,X_t\\) and \\(\\sigma(X_t)=\\mu\\,X_t\\).\nThe explicit solution of the SDE Equation 12 can be found by operating the following change of variable:\n\\[\n\\begin{split}\nY_t=\\log X_t\\quad\\rightarrow\\quad dY_t&=\\frac{1}{X_t}\\,dX_t-\\frac{1}{2X_t^2}\\,(dX_t)^2\\\\\n&=\\left(\\lambda-\\frac{1}{2}\\mu^2\\right)dt+\\mu\\,dW_t\n\\end{split}\n\\tag{13}\\]\nWe obtain:\n\\[\nX_t=x_0\\exp\\left[\\left(\\lambda-\\frac{1}{2}\\mu^2\\right)t+\\mu\\,W_t\\right]\n\\tag{14}\\]\n\n\nFor the GBM, the following stochastic difference equations are obtained for the Euler-Maruyama method:\n\\[\n\\left\\{\n\\begin{split}\nX^E_n&=X^E_{n-1}(1+\\lambda\\Delta t+\\mu\\Delta W_{n-1})\\\\\nX^E_0&=x_0\n\\end{split}\n\\right.\n\\tag{15}\\]\nand for the Milstein method:\n\\[\n\\left\\{\n\\begin{split}\nX^M_n&=X^M_{n-1}+\\lambda X^M_{n-1}\\Delta t+\\mu X^M_{n-1}\\Delta W_{n-1}\\\\\n&+\\dfrac{1}{2}\\mu^2 X^M_{n-1}\\left[(\\Delta W_{n-1})^2-\\Delta t\\right]\\\\\n&=X^M_{n-1}\\left[1+\\left(\\lambda-\\dfrac{1}{2}\\mu^2\\right)\\Delta t+\\mu\\Delta W_{n-1}+\\dfrac{1}{2}\\mu^2(\\Delta W_{n-1})^2\\right]\\\\\nX^M_0&=x_0\n\\end{split}\n\\right.\n\\tag{16}\\]\nRecall that \\(\\Delta W_{n-1}=\\sqrt{\\Delta t}\\,Z\\), where \\(Z\\propto N(0,1)\\).\nEquation 15 reads:\n\\[\n\\left\\{\n\\begin{split}\nX^E_n&=X^E_{n-1}(1+\\lambda\\Delta t+\\mu\\sqrt{\\Delta t}\\,Z)\\\\\nX^E_0&=x_0\n\\end{split}\n\\right.\n\\tag{17}\\]\nEquation 16 reads:\n\\[\n\\left\\{\n\\begin{split}\nX^M_n&=X^M_{n-1}\\left[1+\\left(\\lambda+\\dfrac{1}{2}\\mu^2(Z^2-1)\\right)\\Delta t+\\mu\\sqrt{\\Delta t}\\,Z\\right]\\\\\nX^M_0&=x_0\\\\\n\\end{split}\n\\right.\n\\tag{18}\\]\nComparing the exact solution Equation 14 with Equation 18 shows how the Milstein method makes the Taylor expansion exact up to order \\(\\text{O}(\\Delta t)\\).\nThe following code will test the strong convergence of both methods when they are applied to the linear SDE Equation 12, with \\(\\lambda=2,\\mu=1,x_0=1\\). One thousand sample paths are simulated over the time interval \\([0,1]\\), for step sizes being integer multiples of \\(2^{-9}\\;(8,4,2,1)\\). The order of convergence is tested at the end point of the chosen time interval. The results of the simulations are shown in Figure 1.\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(1792)\nlambda  &lt;- 2 \nmu      &lt;- 1\nXzero   &lt;- 1\nT       &lt;- 1\nN       &lt;- 2^9\ndt      &lt;- 1/N\nM       &lt;- 1000\nXerr_EM &lt;- matrix(0, nrow = M, ncol = 5)\nXerr_M  &lt;- matrix(0, nrow = M, ncol = 5)\n\nfor (i in seq(1, M, by = 1)) {\n  dW    &lt;- rnorm(N, mean = 0, sd = sqrt(dt))\n  W     &lt;- cumsum(dW)\n  Xtrue &lt;- Xzero*exp((lambda - 0.5*mu^2)*T + mu*W[N])\n  for (j in 1:5) {\n    R  &lt;- 2^(j-1)    \n    Dt &lt;- R*dt \n    L  &lt;- N/R\n    Xtemp_EM &lt;- Xzero\n    Xtemp_M  &lt;- Xzero\n    for (k in seq(1, L, by = 1)) {\n      Winc     &lt;- sum(dW[(R*(k-1)+1):(R*k)])\n      Xtemp_EM &lt;- Xtemp_EM*(1 + Dt*lambda + mu*Winc)\n      Xtemp_M  &lt;- Xtemp_M*(1 + Dt*lambda + 0.5*mu^2*(Winc^2 - Dt) + mu*Winc)\n    }\n    Xerr_EM[i, j] &lt;- abs(Xtemp_EM - Xtrue)\n    Xerr_M[i, j]  &lt;- abs(Xtemp_M - Xtrue)\n  }\n}\nDtvals &lt;- c(1, 2, 4, 8)*dt\nXm_EM  &lt;- array(0, dim = 4)\nXm_M   &lt;- array(0, dim = 4)\nfor (i in 1:4) {\n  Xm_EM[i] &lt;- mean(Xerr_EM[, i])\n  Xm_M[i]  &lt;- mean(Xerr_M[, i])\n}\nt   &lt;- rep(Dtvals, by = 2)\ng   &lt;- rep(c(\"Euler-Maruyama\", \"Milstein\"), each = 4)\ne   &lt;- c(Xm_EM, Xm_M)\ne_t &lt;- c(sqrt(t), t)\ndf &lt;- data.frame(t = t, method = g, error = e, error_true = e_t)\n\n\n\n\n\n\n\n\n\n\nFigure 1: Strong error plot for the Euler-Maruyama and Milstein’s methods; dashed lines give the appropriate reference slope for each method, \\(\\gamma=0.5\\) (Euler-Maruyama) and \\(\\gamma=1\\) (Milstein)."
  },
  {
    "objectID": "posts/numerical_simulation/index.html#lamperti-transform",
    "href": "posts/numerical_simulation/index.html#lamperti-transform",
    "title": "Numerical simulation for stochastic differential equations",
    "section": "Lamperti transform",
    "text": "Lamperti transform\nAn interesting application of the Itô formula Equation 7 is now discussed in connection with a formulation of Equation 4 where the diffusion coefficient is not an explicit function of time, but only depends on the state variable \\(X\\):\n\\[\ndX_t=b(t,X_t)\\,dt+\\sigma(X_t)\\,dW_t\n\\tag{19}\\]\nThe Lamperti transform is defined as follows:\n\\[\nY_t=F(X_t)=\\int_z^{X_t}\\frac{1}{\\sigma(s)}\\,ds\n\\tag{20}\\]\nwhere \\(z\\) is any arbitrary value in the state space of \\(X\\). We assume that the function \\(F(\\cdot)\\) defines a one-to-one mapping from the state space of \\(X\\) to \\(\\mathbb{R}\\). We have:\n\\[\nF_t(X_t)=0\\quad\\quad F_x(X_t)=\\dfrac{1}{\\sigma(X_t)}\\quad\\quad F_{xx}(X_t)=-\\dfrac{\\sigma_x(X_t)}{\\sigma^2(X_t)}\n\\tag{21}\\]\nApplying the Itô formula Equation 5, we obtain:\n\\[\n\\left\\{\n\\begin{split}\ndY_t&=\\left[\\dfrac{b(t,X_t))}{\\sigma(X_t)}-\\dfrac{1}{2}\\sigma_x(X_t)\\right]dt+dW_t\\\\\nX_t&=F^{-1}(Y_t)\n\\end{split}\n\\right.\n\\tag{22}\\]\nThe Lamperti transform changes the generic SDE Equation 4 into another SDE with unitary diffusion coefficient. As an example of application of the Lamperti transform, we consider here a result on transformations of SDEs that is relevant for the topic of interest in this post.\nFirst, we discretize the Lamperti transform of Equation 22 using the Euler-Maruyama method (see Equation 8):\n\\[\nY_n=Y_{n-1}+\\left[\\dfrac{b(t_{n-1},X_{n-1})}{\\sigma(X_{n-1})}-\\dfrac{1}{2}\\sigma_x(X_{n-1})\\right]\\Delta t+\\sqrt{\\Delta t}\\,Z\n\\tag{23}\\]\nSecond, we apply the Taylor expansion to the inverse transform \\(X_t=G(Y_t)\\), noting that:\n\\[\n\\begin{split}\nG_y(Y_t)&=\\sigma(G(Y_t))\\\\\nG_{yy}(Y_t)&=\\sigma(G(Y_t))\\sigma_x(G(Y_t))\n\\end{split}\n\\tag{24}\\]\nWe obtain:\n\\[\nG(Y_{n})=G(Y_{n-1})+G_y(Y_{n-1})(Y_n-Y_{n-1})+\\frac{1}{2}G_{yy}(Y_{n-1})(Y_n-Y_{n-1})^2\n\\tag{25}\\]\nand hence:\n\\[\n\\begin{split}\nX_n&=X_{n-1}+\\left[b(t_{n-1},X_{n-1})-\\dfrac{1}{2}\\sigma(X_{n-1})\\sigma_x(X_{n-1})\\right]\\Delta t\\\\\n&+\\sigma(X_{n-1})\\sqrt{\\Delta t}\\,Z+\\frac{1}{2}\\sigma(X_{n-1})\\sigma_x(X_{n-1})\\Delta t\\,Z^2\n\\end{split}\n\\tag{26}\\]\nIn conclusion, the Milstein method on the original process and the Euler-Maruyama method on the transformed process are equal up to and including the order \\(\\text{O}(\\Delta t)\\).\nIn general if a transformation such as the Lamperti transform eliminates the interactions between the state of the process and the increments of the Wiener process, the performance of the Euler-Maruyama method on the transformed SDE are expected to improve.\nThe logarithmic transformation \\(F(X_t)=\\log X_t\\) is an example of nonlinear transformation that is capable of eliminating the interaction between the state \\(X_t\\) of the GBM process and the increments of the Wiener process \\(dW_t\\), as shown in Equation 13. Therefore the Euler-Maruyama method can be confidently applied to the transformed process; the Milstein method is then obtained by simply taking the Taylor expansion of the inverse transform."
  },
  {
    "objectID": "posts/probability_estimation/index.html",
    "href": "posts/probability_estimation/index.html",
    "title": "Confidence intervals for proportions",
    "section": "",
    "text": "Consider a Bernoulli test with \\(n\\) independent trials and \\(x\\) successes that are recorded in the test. The proportion \\(f=x/n\\) is assumed to be the estimate of the true probability \\(p\\); we want to determine the values \\(p_1\\) and \\(p_2\\) of the confidence interval (CI) with the required confidence level associated to \\(f\\), namely \\(CL=100(1-\\alpha)\\%\\).\nThe Clopper-Pearson equations can be used to calculate the CI:\n\\[\n\\left\\{\n\\begin{split}\n\\sum_{k=x}^{\\infty}{n\\choose k}p_1^k(1-p_1)^{n-k}&=c_1\\\\\n\\sum_{k=0}^x{n\\choose k}p_2^k(1-p_2)^{n-k}&=c_2\n\\end{split}\n\\right.\n\\tag{1}\\]\nwhere \\(CL=1-c_1-c_2\\). One common choice for \\(c_1\\) and \\(c_2\\) is the symmetric interval, \\(c_1=c_2=(1-CL)/2\\). For instance, when \\(CL=95\\%\\), \\(\\alpha=0.05\\) and \\(c_1=c_2=2.5\\%\\). The Clopper-Pearson interval is also called the exact interval, since the solution of Equation 1 with respect to \\(p_1\\) and \\(p_2\\) gives the correct probability estimate, even for small samples.\nIt is noted that the binomial model has been used in Equation 1 to capture events of the type “number of successes out of a given number of independent trials”, namely:\n\\[\n\\text{Pr}(k\\;\\text{successes out of}\\;n\\;\\text{trials})={n\\choose k}p^k(1-p)^{n-k}\n\\]\nwhere \\(0\\leq k\\leq n\\) and \\({n\\choose k}\\) is the number of combinations of \\(k\\)-length sequences from an \\(n\\)-length sequence (I discussed this in a previous post of mine).\nWe recall that a binomial random variable \\(X\\) with parameters \\(n\\) (the number of independent trials) and \\(p\\) (the probability of success) has mean value and standard deviation:\n\\[\n\\left\\{\n\\begin{split}\n\\mu_X&=np\\\\\n\\sigma_X&=\\sqrt{np(1-p)}\n\\end{split}\n\\right.\n\\]\nTherefore the random variable \\(F=X/n\\) has mean value and standard deviation:\n\\[\n\\left\\{\n\\begin{split}\n\\mu_F&=p\\\\\n\\sigma_F&=\\frac{\\sigma_X}{\\sqrt{n}}=\\sqrt{\\frac{p(1-p)}{n}}\n\\end{split}\n\\right.\n\\]\nThe normal approximation to the binomial distribution, as stated by the De Moivre-Laplace theorem, is considered as an alternative to Equation 1 for CI construction. This theorem states that the random variable\n\\[\nT=\\frac{F-p}{\\sigma_F}\n\\]\nis approximately normally distributed - in practice, good approximations are achieved for \\(np,n(1-p)\\gg1\\).\nGiven a variate \\(t\\) of \\(T\\), the CI can be written as follows:\n\\[\n\\frac{\\vert\\,f-p\\,\\vert}{\\sqrt{\\dfrac{p(1-p)}{n}}}\\leq t_{1-\\alpha/2}\n\\tag{2}\\]\nwhere \\(t_{1-\\alpha/2}\\) is the quantile of the normal distribution at specified probability \\(1-\\alpha/2\\): for \\(\\alpha=0.05\\), \\(t_{1-\\alpha/2}=1.96\\).\nSquaring Equation 2 we get:\n\\[\nn(f-p)^2\\leq t_{1-\\alpha/2}^2\\,p(1-p)\n\\]\nWe can solve the resulting second-order equation with respect to the unknown \\(p\\), yielding the Wilson interval:\n\\[\np\\in\\dfrac{f+\\dfrac{t^2_{1-\\alpha/2}}{2n}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\\pm\\dfrac{t_{1-\\alpha/2}\\sqrt{\\dfrac{t^2_{1-\\alpha/2}}{4n^2}+\\dfrac{f(1-f)}{n}}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\n\\tag{3}\\]\nIt is noted that the Wilson interval is not centered on the measured proportion \\(f\\) but on the value \\((f+t_{1-\\alpha/2}^2/2n)/(t_{1-\\alpha/2}^2/n+1)\\), which is a function of \\(f\\) and the number of trials \\(n\\). This is due to the asymmetry of the binomial distribution for small \\(n\\).\n\n\n\n\n\n\nContinuity correction\n\n\n\nThe accuracy of Equation 3 can be improved by applying the continuity correction to the proportion \\(f=x/n\\). This correction improves the accuracy of CI construction for discrete variables when they can be approximated by normal variables:\n\\[\nf_{\\pm}=\\frac{x\\pm0.5}{n}\\rightarrow\\left\\{\\begin{split}f_{+}&=\\min\\left(1,\\dfrac{x+0.5}{n}\\right)\\\\f_{-}&=\\max\\left(0,\\dfrac{x-0.5}{n}\\right)\\end{split}\\right.\n\\]\nThe interval is estimated as follows:\n\\[\np\\in[\\max(0,p_{-}),\\min(1,p_{+})]\n\\]\nwhere:\n\\[\np_{\\pm}\\in\\dfrac{f_{\\pm}+\\dfrac{t^2_{1-\\alpha/2}}{2n}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\\pm\\dfrac{t_{1-\\alpha/2}\\sqrt{\\dfrac{t^2_{1-\\alpha/2}}{4n^2}+\\dfrac{f_{\\pm}(1-f_{\\pm})}{n}}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\n\\]\n\n\nThe Wilson interval (with continuity correction) is generally considered valid for \\(n&gt;10\\). For \\(n\\gg1\\) (namely, \\(n&gt;30\\)), a very popular approximate formulation of the CI is the Wald interval:\n\\[\np\\in f\\pm t_{1-\\alpha/2}\\sqrt{\\dfrac{f(1-f)}{n}}\n\\tag{4}\\]\nThe Wald interval, which does not require any continuity correction, is symmetric around the estimated proportion. For \\(n\\gg1\\) it is noted indeed that the Wilson interval tends to become symmetric and identical to the Wald interval.\nThe question is when \\(n\\) is large enough for these methods to produce accurate inference. The R code below is a fully reproducible code to generate coverage plots for the Wilson interval (with continuity correction) and the Wald interval.\n\n\nCode\nset.seed(1789)\nn  &lt;- 30                          # number of Bernoulli trials\nL  &lt;- 1000                        # number of simulated sequences \na  &lt;- 95                          # confidence level\nt  &lt;- qnorm((1+a/100)/2)          # quantile of the normal distribution\np  &lt;- seq(0.05, 0.95, by = 0.01)  # tested values of probability\nnp &lt;- length(p)                   # number of tested values of probability\nc_wald   &lt;- numeric(np)           # Wald approach\nc_wilson &lt;- numeric(np)           # Wilson approach\n\nfor(i in 1:np) {\n  for(j in 1:L) {\n    x &lt;- rbinom(1, size = n, prob = p[i])  # number of successes\n    f &lt;- x/n                               # proportion of successes\n    fl &lt;- max(0, (x-0.5)/n) # continuity correction\n    fu &lt;- min(1, (x+0.5)/n) \n    CIl &lt;- (fl+t^2/(2*n))/(1+t^2/n) - t*sqrt(fl*(1-fl)/n + t^2/(4*n^2))/(1+t^2/n)\n    CIu &lt;- (fu+t^2/(2*n))/(1+t^2/n) + t*sqrt(fu*(1-fu)/n + t^2/(4*n^2))/(1+t^2/n)\n    CI_lower_wilson &lt;- max(0.0, CIl) # Wilson interval\n    CI_upper_wilson &lt;- min(1.0, CIu)\n    CI_lower_wald &lt;- f - t*sqrt(f*(1-f)/n) # Wald interval\n    CI_upper_wald &lt;- f + t*sqrt(f*(1-f)/n)\n    if(CI_lower_wilson &lt;= p[i] && CI_upper_wilson &gt;= p[i]) c_wilson[i] = c_wilson[i] + 1\n    if(CI_lower_wald &lt;= p[i] && CI_upper_wald &gt;= p[i]) c_wald[i] = c_wald[i] + 1\n  }\n}\nc_wilson &lt;- 100*c_wilson/L # estimated coverage\nc_wald   &lt;- 100*c_wald/L   \n\n\n\n\n\n\n\n\n\n\nFigure 1: Coverage curves for the binomial distribution with \\(n=30\\).\n\n\n\n\n\nThe simulation results of Figure 1 (\\(n=30\\)) show that, whereas the Wilson method tends just to provide some limited over-coverage for all values of \\(p\\) (this is simply due to \\(x\\) being present in both the sums of Equation 1), the Wald interval performs poorly, with severe under-coverage, especially for low and high values of \\(p\\). These results seem to raise objections against the use of the Wald interval, even in situations when \\(n\\gg1\\): Figure 2 shows the coverage plots in the case of \\(n=100\\).\n\n\n\n\n\n\n\n\nFigure 2: Coverage curves for the binomial distribution with \\(n=100\\).\n\n\n\n\n\nAlthough the performance of the Wald method has slightly improved, the Wilson method still outperforms it, especially for small and high values of \\(p\\).\n\nTo conclude, when estimating the confidence interval associated to the estimate of proportions, the Wald interval - the most basic confidence interval usually considered by experimentalists - appears to be seriously flawed, also for values of the number of trials that are considered large in most statistical textbooks. The Wilson interval with continuity correction tends to perform much better and its use should be encouraged."
  },
  {
    "objectID": "posts/correspondence_analysis_2/index.html",
    "href": "posts/correspondence_analysis_2/index.html",
    "title": "Correspondence analysis: Part II",
    "section": "",
    "text": "In this post, we will briefly discuss an example of how Simple Correspondence Analysis (SCA) can be performed using the R programming language. Several functions from different packages are available in the R software for computing SCA. No matter what package we decide to use for the analysis (in this post, we will use the package FactoMineR), the package factoextra is great for producing ggplot2-based elegant visualizations of analysis results. A couple of other packages, namely gplots and corrplot, will also be used.\n\n\nCode\n# Libraries \n\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(corrplot)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/correspondence_analysis_2/index.html#introduction",
    "href": "posts/correspondence_analysis_2/index.html#introduction",
    "title": "Correspondence analysis: Part II",
    "section": "",
    "text": "In this post, we will briefly discuss an example of how Simple Correspondence Analysis (SCA) can be performed using the R programming language. Several functions from different packages are available in the R software for computing SCA. No matter what package we decide to use for the analysis (in this post, we will use the package FactoMineR), the package factoextra is great for producing ggplot2-based elegant visualizations of analysis results. A couple of other packages, namely gplots and corrplot, will also be used.\n\n\nCode\n# Libraries \n\nlibrary(tidyverse)\nlibrary(gplots)\nlibrary(corrplot)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/correspondence_analysis_2/index.html#exemplary-dataset",
    "href": "posts/correspondence_analysis_2/index.html#exemplary-dataset",
    "title": "Correspondence analysis: Part II",
    "section": "Exemplary dataset",
    "text": "Exemplary dataset\nhousetasks, available in the package factoextra, is a data frame that contains the frequency of execution of 13 house tasks performed by the couple in four different ways: a) the wife only; b) alternatively; c) the husband only; d) jointly.\n\n\nCode\ndata(housetasks)\nhousetasks %&gt;%\n  kable(\n    align = \"rrrr\",\n    col.names = c(\"Wife\", \"Alternating\", \"Husband\", \"Jointly\"),\n    caption = \"House tasks contingency table\",\n  ) %&gt;%\n  kable_classic()\n\n\n\nHouse tasks contingency table\n\n\n\nWife\nAlternating\nHusband\nJointly\n\n\n\n\nLaundry\n156\n14\n2\n4\n\n\nMain_meal\n124\n20\n5\n4\n\n\nDinner\n77\n11\n7\n13\n\n\nBreakfeast\n82\n36\n15\n7\n\n\nTidying\n53\n11\n1\n57\n\n\nDishes\n32\n24\n4\n53\n\n\nShopping\n33\n23\n9\n55\n\n\nOfficial\n12\n46\n23\n15\n\n\nDriving\n10\n51\n75\n3\n\n\nFinances\n13\n13\n21\n66\n\n\nInsurance\n8\n1\n53\n77\n\n\nRepairs\n0\n3\n160\n2\n\n\nHolidays\n0\n1\n6\n153\n\n\n\n\n\n\n\nCode\n# Convert the data frame to a table\ndt &lt;- as.table(as.matrix(housetasks))\n# Graph\nballoonplot(t(dt), main =\"House tasks contingency table\", xlab = \"\", ylab = \"\",\n            label = FALSE, show.margins = FALSE)"
  },
  {
    "objectID": "posts/correspondence_analysis_2/index.html#implementation",
    "href": "posts/correspondence_analysis_2/index.html#implementation",
    "title": "Correspondence analysis: Part II",
    "section": "Implementation",
    "text": "Implementation\nThe function CA() from the package FactoMineR is used. It performs the Singular Value Decomposition of the standardized residuals, see my previous post here. The first step of the analysis was done there in order to evaluate whether there was a significant dependency between the rows and columns using the chi-square statistic.\n\nres.ca &lt;- CA(housetasks, graph = FALSE)\nprint(res.ca)\n\n**Results of the Correspondence Analysis (CA)**\nThe row variable has  13  categories; the column variable has 4 categories\nThe chi square of independence between the two variables is equal to 1944.456 (p-value =  0 ).\n*The results are available in the following objects:\n\n   name              description                   \n1  \"$eig\"            \"eigenvalues\"                 \n2  \"$col\"            \"results for the columns\"     \n3  \"$col$coord\"      \"coord. for the columns\"      \n4  \"$col$cos2\"       \"cos2 for the columns\"        \n5  \"$col$contrib\"    \"contributions of the columns\"\n6  \"$row\"            \"results for the rows\"        \n7  \"$row$coord\"      \"coord. for the rows\"         \n8  \"$row$cos2\"       \"cos2 for the rows\"           \n9  \"$row$contrib\"    \"contributions of the rows\"   \n10 \"$call\"           \"summary called parameters\"   \n11 \"$call$marge.col\" \"weights of the columns\"      \n12 \"$call$marge.row\" \"weights of the rows\""
  },
  {
    "objectID": "posts/correspondence_analysis_2/index.html#visualization-and-interpretation",
    "href": "posts/correspondence_analysis_2/index.html#visualization-and-interpretation",
    "title": "Correspondence analysis: Part II",
    "section": "Visualization and interpretation",
    "text": "Visualization and interpretation\nThe following functions available the factoextra package can be used to help in the SCA interpretation and visualization:\n\n\n\nFunctions from FactoMineR package for SCA\n\n\nFunction name\nPurpose\n\n\n\n\nget_eigenvalue()\nExtract the eigenvalues/variances retained by each dimension (axis)\n\n\nfviz_eig()\nVisualize the eigenvalues\n\n\nfviz_screeplot()\nScree plot\n\n\nget_ca_row()\nExtract the results for rows\n\n\nget_ca_col()\nExtract the results for columns\n\n\nfviz_ca_row()\nVisualize the results for rows\n\n\nfviz_ca_col()\nVisualize the results for columns\n\n\nfviz_ca_cos2()\nCos2 of rows on selected axes\n\n\nfviz_ca_biplot()\nMake a biplot of rows and columns\n\n\n\n\n\n\n\n\nEigenvalues/variance\nEigenvalues correspond to the amount of information retained by each axis. Dimensions are ordered from largest to smallest and reported with the amount of variance each of them explains in the solution. Dimension 1 (Dim.1) explains the most variance in the solution, followed by dimension 2 (Dim.2) and so on. The cumulative percentage explained is obtained by adding the successive proportions of variance explained. For instance, 48.69% plus 39.91% equals 88.6%, and so forth. Therefore, about 88.6% of the variance is explained by the first two dimensions.\nEigenvalues can be used to determine the number of axes to retain. There is no “rule of thumb” to choose the number of dimensions to keep for the data interpretation. It depends on the research question and the need of the researcher. For example, if 80% of the total variation explained is believed satisfactory for the purpose of the analysis, then the number of dimensions necessary to achieve that goal will be considered. Generally, a good dimension reduction is achieved with the first few dimensions accounting for a large proportion of the variation. In the example in this post, the first two axes explain 88.6% of the variation.\nAn alternative method to determine the number of dimensions is to look at a Scree Plot, which is the plot of eigenvalues/variances ordered from largest to smallest. The number of components is determined at the point, beyond which the remaining eigenvalues are all relatively small and of comparable size.\nIn the example in this post, the house tasks contingency table contains 13 rows and 4 columns. If the data were random, the expected value of the eigenvalue for each axis would be 1/(nrow(housetasks)-1) (rows), and 1/(ncol(housetasks)-1) (columns). Any axis contributing more than the maximum of these two percentages, namely \\(33.33\\%\\) in the example here, should be considered as important and included in the final solution.\n\n\nCode\neig.val &lt;- get_eigenvalue(res.ca)\nround(eig.val, 2)\n\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1       0.54            48.69                       48.69\nDim.2       0.45            39.91                       88.60\nDim.3       0.13            11.40                      100.00\n\n\nCode\nv &lt;- max(c(1/(nrow(housetasks)-1), 1/(ncol(housetasks)-1)))*100\n\nfviz_screeplot(res.ca, addlabels = TRUE, ylim = c(0, 50)) + \n  geom_hline(yintercept = v, linetype = 2, color = \"red\")\n\n\n\n\n\n\n\n\n\nAccording to the graph above, only Dim.1 and Dim.2 should be used in the solution. Dim.3 explains only 11.4% of the total variation, which is well below the stated threshold at 33.33%, hence it can be discarded. Dim.1 and Dim.2 explain approximately 48.7% and 39.9% of the total variation, respectively. This corresponds to a cumulative total of 88.6% of total variation retained by them.\n\n\nBiplot\n\n\nCode\n# repel = TRUE to avoid text overlapping\nfviz_ca_biplot(res.ca, repel = TRUE)\n\n\n\n\n\n\n\n\n\nThe graph above is called symmetric plot, or biplot. The biplot is helpful to show global patterns existing within the data. Rows are represented by blue points and columns by red triangles. The distance between any pair of row points, or any pair of column points, gives a measure of their similarity (or dissimilarity). Row (column) points with similar profile are close in the biplot.\nThe biplot represents the row and column profiles simultaneously in a common space. It is worth stress that:\n\nthe distance between any row and column items is not meaningful\nonly the distance between row points or the distance between column points can be interpreted.\n\nIn order to interpret the distance between column and row points, the column (row) profiles must be presented in row (column) space. The resulting type of map is called asymmetric biplot; it is not discussed in this post.\n\n\nGraph of row variables\nThe function get_ca_row() is used to extract the results for row points. This function returns a list containing the coordinates, the squared cosine (cos2), the contribution and the inertia of row points:\n\n\nCode\nrow &lt;- get_ca_row(res.ca)\nrow\n\n\nCorrespondence Analysis - Results for rows\n ===================================================\n  Name       Description                \n1 \"$coord\"   \"Coordinates for the rows\" \n2 \"$cos2\"    \"Cos2 for the rows\"        \n3 \"$contrib\" \"contributions of the rows\"\n4 \"$inertia\" \"Inertia of the rows\"      \n\n\nThe components of the get_ca_row() function can be used in the plot of rows as follows:\n\nrow$coord: coordinates of each row point in each dimension (1, 2 and 3).\nrow$cos2: quality of representation of rows.\nvar$contrib: contribution of rows (in %) to the definition of the dimensions.\n\nCoordinates of row points\n\n\nCode\nfviz_ca_row(res.ca, col.row = \"blue\", shape.row = 16, repel = TRUE)\n\n\n\n\n\n\n\n\n\nThe plot above shows the relationships between row points:\n\nRows with a similar profile are grouped together.\nNegatively correlated rows are positioned on opposite sides of the plot origin.\nThe distance between row points and the origin measures the quality of the row points on the scatter plot. The higher the distance, the better the quality.\n\nQuality of representation of rows\nI recall that the quality of representation of the rows on the biplot is called the squared cosine (cos2) or the squared correlation. The cos2 measures the degree of association between rows and a particular axis. The values of the cos2 range in the interval between 0 and 1. The sum of the cos2 for rows on all the CA dimensions is equal to one. The quality of representation of a row in \\(n\\) dimensions is simply the sum of the squared cosine of that row over the \\(n\\) dimensions. If a row item is well represented by two dimensions, the sum of the cos2 should be close to one. For some of the row items, more than 2 dimensions may be required to perfectly represent the data.\nIt is possible to color row points by their cos2 values using the argument col.row = \"cos2\" in fviz_ca_row(). This produces a color gradient, which can be customized using the argument gradient.cols. For instance, gradient.cols = c(\"white\", \"blue\", \"red\") means that:\n\nvariables with low cos2 values will be colored in “white”\nvariables with mid cos2 values will be colored in “blue”\nvariables with high cos2 values will be colored in “red”\n\nThe row variables with the larger value of cos2, contribute the most to the definition of the dimensions. Rows that contribute the most to the retained dimensions, i.e., Dim.1 and Dim.2, are the most important in the process of variance explanation. Rows that do not contribute much to the retained dimensions or that contribute to the last dimensions, i.e., Dim.3, are less important.\n\n\nCode\n# Color by cos2 values\nfviz_ca_row(res.ca, col.row = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# Cos2 of rows on Dim.1 and Dim.2\nfviz_cos2(res.ca, choice = \"row\", axes = 1:2)\n\n\n\n\n\n\n\n\n\nThe function fviz_contrib() can be used to draw a bar plot of row contributions. The red dashed line on the graph below indicates the expected average value, if the contributions were uniform.\n\n\nCode\n# Contributions of rows to dimension 1\nfviz_contrib(res.ca, choice = \"row\", axes = 1, top = 10)\n\n\n\n\n\n\n\n\n\nCode\n# Contributions of rows to dimension 2\nfviz_contrib(res.ca, choice = \"row\", axes = 2, top = 10)\n\n\n\n\n\n\n\n\n\nCode\n# Total contribution to dimension 1 and 2\nfviz_contrib(res.ca, choice = \"row\", axes = 1:2, top = 10)\n\n\n\n\n\n\n\n\n\nIt can be seen that:\n\nThe row items “Repairs”, “Laundry”, “Main_meal” and “Driving” are the most important in the definition of the first dimension.\nThe row items “Holidays” and “Repairs” contribute the most to the dimension 2.\nThe most important row points can be highlighted on the biplot, as explained above.\n\n\n\nCode\nfviz_ca_row(res.ca, col.row = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\n\n\nThe scatter plot gives an idea of what pole of the retained dimensions the row categories are contributing to. The row categories “Repair” and “Driving” have an important contribution to the positive pole of the first dimension, while the categories “Laundry” and “Main_meal” have an important contribution to the negative pole of the first dimension, and so forth. In other words, Dim.1 is mainly defined by the opposition of “Repair” and “Driving” (positive pole), and “Laundry” and “Main_meal” (negative pole).\n\n\nGraph of column variables\nThe function get_ca_col() is used to extract the results for column variables. This function returns a list containing the coordinates, the squared cosine (cos2), the contribution and the inertia of column variables:\n\n\nCode\ncol &lt;- get_ca_col(res.ca)\ncol\n\n\nCorrespondence Analysis - Results for columns\n ===================================================\n  Name       Description                   \n1 \"$coord\"   \"Coordinates for the columns\" \n2 \"$cos2\"    \"Cos2 for the columns\"        \n3 \"$contrib\" \"contributions of the columns\"\n4 \"$inertia\" \"Inertia of the columns\"      \n\n\nThe components of the get_ca_col() function can be used in the plot of columns as follows:\n\ncol$coord: coordinates of each column point in each dimension (1, 2 and 3).\ncol$cos2: quality of representation of columns\nvar$contrib: contribution of columns (in %) to the definition of the dimensions.\n\nCoordinates of column points\n\n\nCode\nfviz_ca_col(res.ca, col.col = \"red\", shape.col = 17, repel = TRUE)\n\n\n\n\n\n\n\n\n\nThe plot above shows the relationships between column points:\n\nColumns with a similar profile are grouped together.\nNegatively correlated columns are positioned on opposite sides of the plot origin.\nThe distance between column points and the origin measures the quality of the column points on the scatter polot. The higher the distance, the better the quality.\n\nQuality of representation of columns\nI recall that the quality of representation of the columns on the biplot is called the squared cosine (cos2) or the squared correlation. The cos2 measures the degree of association between columns and a particular axis. The values of the cos2 range in the interval between 0 and 1. The sum of the cos2 for columns on all the CA dimensions is equal to one. The quality of representation of a column in \\(n\\) dimensions is simply the sum of the squared cosine of that column over the \\(n\\) dimensions. If a column item is well represented by two dimensions, the sum of the cos2 should be close to one. For some of the column items, more than 2 dimensions may be required to perfectly represent the data.\nIt is possible to color column points by their cos2 values using the argument col.col = \"cos2\" in fviz_ca_col(). This produces a color gradient, which can be customized using the argument gradient.cols. For instance, gradient.cols = c(\"white\", \"blue\", \"red\") means that:\n\nvariables with low cos2 values will be colored in “white”\nvariables with mid cos2 values will be colored in “blue”\nvariables with high cos2 values will be colored in “red”\n\nThe column variables with the larger value of cos2, contribute the most to the definition of the dimensions. Columns that contribute the most to the retained dimensions, i.e., Dim.1 and Dim.2, are the most important in the process of variance explanation. Columns that do not contribute much to the retained dimensions or that contribute to the last dimensions, i.e., Dim.3, are less important.\n\n\nCode\n# Color by cos2 values\nfviz_ca_col(res.ca, col.col = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\n\n\nCode\n# Cos2 of rows on Dim.1 and Dim.2\nfviz_cos2(res.ca, choice = \"col\", axes = 1:2)\n\n\n\n\n\n\n\n\n\nThe function fviz_contrib() can be used to draw a bar plot of column contributions. The red dashed line on the graph below indicates the expected average value, if the contributions were uniform.\n\n\nCode\n# Contributions of columns to dimension 1\nfviz_contrib(res.ca, choice = \"col\", axes = 1, top = 10)\n\n\n\n\n\n\n\n\n\nCode\n# Contributions of colums to dimension 2\nfviz_contrib(res.ca, choice = \"col\", axes = 2, top = 10)\n\n\n\n\n\n\n\n\n\nCode\n# Total contribution to dimension 1 and 2\nfviz_contrib(res.ca, choice = \"col\", axes = 1:2, top = 10)\n\n\n\n\n\n\n\n\n\nIt can be seen that only the column item “Alternating” is not very well displayed on the first two dimensions. The position of this item must be interpreted with caution in the space formed by Dim.1 and Dim.2.\n\n\nCode\nfviz_ca_col(res.ca, col.col = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nSymmetric biplot\nAs mentioned above, the standard plot of SCAs is a symmetric biplot, in which both rows (blue points) and columns (red triangles) are represented in the same space using the principal coordinates. These coordinates represent the row and column profiles. In this case, only the distance between row points or the distance between column points can be interpreted.\n\n\nCode\nfviz_ca_biplot(res.ca, repel = TRUE)\n\n\n\n\n\n\n\n\n\n\nThe series of two posts I have compiled so far concerning SCA just scratched the surface of their fascinating, and challenging, world. A lot more should be said to achieve a serious and meaningful interpretation of the results they can offer to the analyst. Maybe, in the future, I will continue the series with additional contributions. Up to now, that’s it."
  },
  {
    "objectID": "posts/gambler/index.html",
    "href": "posts/gambler/index.html",
    "title": "Gambler’s ruin",
    "section": "",
    "text": "The gambler’s ruin problem, first mentioned by Blaise Pascal in one of his letters to Pierre Fermat, was then reformulated by Christiaan Huygens, leading to important advances in the mathematical theory of probability. The statement of the problem considered here is slightly different from the one described by either Pascal or Huygens (Bertsekas and Tsitsiklis 2008).\n\nConsider a gambler who starts with an initial wealth of €\\(k\\) and then on each successive bet either wins €1 or loses €1 with probabilities \\(p\\) and \\(q=1−p\\) respectively. Different bets are assumed independent. The gambler’s objective is to reach a total wealth of €\\(n\\) (\\(n&gt;k\\)). If the gambler succeeds, then the gambler is said to win the game. The gambler stops playing after winning or getting ruined (running out of money), whichever happens first.\n\nCalculate the probability that the gambler wins. In particular, which is the probability of the gambler becoming infinitely rich?"
  },
  {
    "objectID": "posts/gambler/index.html#problem",
    "href": "posts/gambler/index.html#problem",
    "title": "Gambler’s ruin",
    "section": "",
    "text": "The gambler’s ruin problem, first mentioned by Blaise Pascal in one of his letters to Pierre Fermat, was then reformulated by Christiaan Huygens, leading to important advances in the mathematical theory of probability. The statement of the problem considered here is slightly different from the one described by either Pascal or Huygens (Bertsekas and Tsitsiklis 2008).\n\nConsider a gambler who starts with an initial wealth of €\\(k\\) and then on each successive bet either wins €1 or loses €1 with probabilities \\(p\\) and \\(q=1−p\\) respectively. Different bets are assumed independent. The gambler’s objective is to reach a total wealth of €\\(n\\) (\\(n&gt;k\\)). If the gambler succeeds, then the gambler is said to win the game. The gambler stops playing after winning or getting ruined (running out of money), whichever happens first.\n\nCalculate the probability that the gambler wins. In particular, which is the probability of the gambler becoming infinitely rich?"
  },
  {
    "objectID": "posts/gambler/index.html#solution",
    "href": "posts/gambler/index.html#solution",
    "title": "Gambler’s ruin",
    "section": "Solution",
    "text": "Solution\nLet us denote by \\(A\\) the event that the gambler ends up with €\\(n\\) and by \\(F\\) the event that the gambler wins the first bet. The probability of the event \\(A\\) given that the gambler starts with €\\(k\\) is written \\(w_k\\). The total probability law allows us to write:\n\\[\n\\begin{split}\nw_k&=P(A\\,\\vert\\,F)P(F)+P(A\\,\\vert\\,\\overline{F})P(\\overline{F})\\\\\n&=pP(A\\,\\vert\\,F)+(1-p)P(A\\,\\vert\\,\\overline{F})\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\overline{F}\\) is the complement of \\(F\\), namely the event \\(\\overline{F}\\) is that the gambler loses the first bet.\nNow, the event \\(A\\,\\vert\\,F\\) is the event \\(A\\) given that the gambler starts with €\\((k+1)\\), whose probability is \\(w_{k+1}\\) (because of the independence from the past). The event \\(A\\,\\vert\\,\\overline{F}\\) is the event \\(A\\) given that the gambler starts with €\\((k-1)\\), whose probability is \\(w_{k-1}\\) (because of the independence from the past). Equation 1 reads:\n\\[\nw_k=p\\,w_{k+1}+(1-p)\\,w_{k-1}\n\\tag{2}\\]\nIf we define \\(r=(1-p)/p\\), we can also write:\n\\[\n\\left\\{\n\\begin{split}\n&w_{k+1}-w_k=r(w_k-w_{k-1}),\\;0&lt;k&lt;n\\\\\n&w_0=0\\\\\n&w_n=1\n\\end{split}\n\\right.\n\\tag{3}\\]\nSince \\(w_{k+1}=w_k+r^k(w_1-w_0)=w_k+r^kw_1=w_1\\sum_{i=0}^kr^i\\), we can calculate \\(w_k\\) as follows:\n\\[\nw_k=\\left\\{\n\\begin{split}\n&\\frac{1-r^k}{1-r}w_1&\\quad\\text{if}\\;r\\neq1\\\\\n&kw_1&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.\n\\tag{4}\\]\nWe also know that \\(w_n=1\\). Therefore, we can solve Equation 4 for \\(w_1\\):\n\\[\nw_1=\\left\\{\n\\begin{split}\n&\\frac{1-r}{1-r^n}&\\quad\\text{if}\\;r\\neq1\\\\\n&\\frac{1}{n}&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.\n\\]\nPlugging this expression of \\(w_1\\) into Equation 4, we obtain:\n\\[\n\\boxed{w_k=\\left\\{\n\\begin{split}\n&\\frac{1-r^k}{1-r^n}&\\quad\\text{if}\\;r\\neq1\\\\\n&\\frac{k}{n}&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.}\n\\tag{5}\\]\nThe probability that the gambler becomes infinitely rich (event \\(R\\)) is written:\n\\[\n\\left\\{\n\\begin{split}\n\\text{Pr}(R)&=\\lim_{n\\rightarrow\\infty}w_k=1-\\left(\\frac{1-p}{p}\\right)^k&gt;0,&\\quad p&gt;1/2\\;(r&lt;1)\\\\\n\\text{Pr}(R)&=\\lim_{n\\rightarrow\\infty}w_k=0,&\\quad p\\leq1/2\\;(r\\geq1)\\\\\n\\end{split}\n\\right.\n\\tag{6}\\]\nTo interpret the meaning of Equation 6, suppose that the gambler starting with an initial wealth of €\\(k\\) wishes to continue gambling, with the intention of earning as much money as possible. So there is no winning value €\\(n\\): the gambler will only stop if ruined. If \\(p&gt;1/2\\) (each gamble is in favor of the gambler), then there is a positive probability that the gambler will never get ruined but instead will become infinitely rich. Conversely, if \\(p\\leq0.5\\) (each gamble is not in favor of the gambler), then the gambler will get ruined with probability one."
  },
  {
    "objectID": "posts/gambler/index.html#markov-chain-model",
    "href": "posts/gambler/index.html#markov-chain-model",
    "title": "Gambler’s ruin",
    "section": "Markov chain model",
    "text": "Markov chain model\nThe discrete-state discrete-time Markov chain show in Figure 1 can be proposed to model the gambler process (Bertsekas and Tsitsiklis 2008). The \\(n+1\\) states, numbered from 0 to \\(n\\), represent the gambler’s wealth at the start of each bet. The first state 0 corresponds to ruin (getting out of money), the last state \\(n\\) corresponds to win. Given the rules of the game, the states 0 and \\(n\\) are absorbing, in the sense that it is not possible to escape from each of them anymore, once they are entered. All the other states are transient.\n\n\n\n\n\n\nFigure 1: Transition probability graph for the gambler’s ruin problem.\n\n\n\nThe problem is then to find the probabilities of absorption at each one of the two absorbing states. These absorption probabilities depend on the initial state \\(k\\;(0&lt;k&lt;n)\\). It is worthy noting that the memoryless character of the process implies that, if the gambler happens to revisit the initial state \\(k\\) after a while, the absorption probabilities are the same they were initially.\n\n\n\n\n\n\nTransition probability matrix\n\n\n\nThe state of a discrete-state discrete-time Markov chain is denoted by \\(X_t\\), that can change at certain discrete time instants \\(n\\). The state space of the chain is composed of a finite set \\(\\mathscr{S}=\\{1,\\cdots,m\\}\\). The Markov chain is described in terms of its transition probability \\(p_{ij}\\):\n\\[\np_{ij}=P(X_{t+1}=j\\,\\vert\\,X_t=i),\\quad i,j\\in\\mathscr{S}\n\\tag{7}\\]\nThe key assumption underlying Markov chains is that the Markov property holds:\n\\[\nP(X_{t+1}=j\\,\\vert\\,X_t=i,X_{t-1}=i_{t-1},\\cdots,X_0=i_0)=P(X_{t+1}=j \\,\\vert\\,X_t=i)=p_{ij}\n\\tag{8}\\]\nfor all times \\(t\\), all states \\(i,j\\in\\mathscr{S}\\) and all possible sequences \\(i_0,i_1,\\cdots,i_{t-1}\\) of earlier states. Thus, the probability law of the next state \\(X_{t+1}\\) depends on the past only through the value of the present state \\(X_t\\).\nAll of the elements of a Markov chain model can be encoded in a transition probability matrix, which is simply a two-dimensional array whose elements at the \\(i\\)th row and \\(j\\)th column is \\(P_{ij}\\):\n\\[\n\\begin{bmatrix}\np_{11}&p_{12}&\\cdots&p_{1m}\\\\\np_{21}&p_{22}&\\cdots&p_{2m}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\np_{m1}&p_{m2}&\\cdots&p_{mm}\n\\end{bmatrix}\n\\tag{9}\\]\nConsider a Markov chain in which each state is either transient or absorbing. We fix a particular absorbing state \\(s\\). It is possible to show that the probabilities \\(w_k\\) of eventually reaching state \\(s\\), starting from \\(k\\), are the unique solution of the equations \\(w_s=1, w_i=0\\), for all absorbing \\(i\\neq s\\), and for all transient \\(i\\):\n\\[\nw_i=\\sum_{j=1}^mp_{ij}w_j\n\\tag{10}\\]\n\n\n\nExample 1 (Gambler’s discrete-time Markov chain) For \\(n=4,p,q=1-p\\) the transition probability matrix of the gambler’s discrete-time Markov chain is given by:\n\\[\n\\begin{bmatrix}\n1&0&0&0&0\\\\\nq&0&p&0&0\\\\\n0&q&0&p&0\\\\\n0&0&q&0&p\\\\\n0&0&0&0&1\n\\end{bmatrix}\n\\]\nIn the case of the gambler’s ruin problem, Equation 10 can be reformulated as follows:\n\\[\nw_k=(1-p)\\,w_{k-1}+p\\,w_{k+1}\n\\]\nwith \\(w_0=0,w_n=1\\), leading to Equation 2."
  },
  {
    "objectID": "posts/gambler/index.html#simulation",
    "href": "posts/gambler/index.html#simulation",
    "title": "Gambler’s ruin",
    "section": "Simulation",
    "text": "Simulation\nThe following code written in R simulates the gambler process (adapted from (Dobrow 2016)).\n\n\nCode\nset.seed(2001)\ngambler &lt;- function(k, n, p) {\n  stake &lt;- k\n    while (stake &gt; 0 & stake &lt; n) {\n        bet &lt;- sample(c(-1, 1), 1, prob = c(1-p, p))\n        stake &lt;- stake + bet\n    }\n    if (stake == n) return(1) else return(0)\n}   \nk &lt;- 15         # initial wealth\nn &lt;- 120        # final wealth\np &lt;- 0.5        # probability of win at each bet\nntrials &lt;- 1000 # number of independent trials\n\nsim &lt;- replicate(ntrials, gambler(k, n, p))\np_ruin &lt;- 1 - sum(sim)/ntrials \n\n\nThe probability of ruin estimated over 1000 iterations of the gambler process is 0.888, in close agreement with the value 0.875 computed from Equation 5.\nIt is noted that, at each time step, the gambler appears to move randomly, either to the left or to the right by a fixed unit distance with probability \\(p\\) and \\(1-p\\), respectively. The gambler undergoes a simple random walk in one dimension, whose state space is the set of the integers \\(\\mathbb{Z}\\). The limitations to values of \\(\\mathbb{Z}\\) between 0 (ruin) and \\(n\\) (win) are the consequence of the gambling rules: these rules settle two absorbing barriers to the motion of the random walker. Nine exemplary sample paths of the random walk are shown in Figure 2, for the case that \\(k=30,n=120,p=0.5\\).\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\n\nset.seed(2001)\nk &lt;- 30       # initial wealth\nn &lt;- 120      # final wealth\np &lt;- 0.5      # probability of win at each bet\nntrials &lt;- 9  # number of independent trials\nnstep &lt;- 1000 # number of steps\ntraj &lt;- matrix(NA, nrow = nstep, ncol = ntrials)\nfor (i in 1:ntrials) {\n  traj[1, i] &lt;- k\n  jmax &lt;- nstep\n  for (j in 2:nstep) {\n          bet &lt;- sample(c(-1, 1), 1, prob = c(1-p, p))\n          traj[j, i] &lt;- traj[j-1, i] + bet\n          if (traj[j, i] == 0 | traj[j, i] == n) {\n            break\n          }\n  }\n}   \nt &lt;- rep(seq(0, nstep-1, by = 1), ntrials)\npath &lt;- rep(c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"), each = nstep)\ntraj &lt;- c(traj)\ndf &lt;- data.frame(t = t, path = path, data = traj)\n\nmy_theme = theme(\n    axis.title.x = element_text(size = 14),\n    axis.text.x = element_text(size = 12),\n    axis.title.y = element_text(size = 14),\n    axis.text.y = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    panel.border = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    panel.spacing = unit(1, \"lines\"),\n    axis.line = element_line(colour = \"black\"))\n\nggplot(df, aes(x = t, y = data, path = path)) + \n  geom_line() +\n  labs(x = \"bets\",\n       y = \"gambler's wealth\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  geom_hline(yintercept = 120, linetype = \"dashed\") +\n  scale_y_continuous(breaks = c(0, 30, 60, 90, 120)) +\n  facet_wrap(~path, labeller = \"label_both\") + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 2: Gambler’s random walk: nine sample paths. The outcome of up to a maximum of 1000 bets is considered here."
  },
  {
    "objectID": "posts/resolution/index.html",
    "href": "posts/resolution/index.html",
    "title": "Frequency resolution of spectral analysis",
    "section": "",
    "text": "Frequency resolution is the size of the smallest frequency for which details in the frequency response and the spectrum can be resolved by the estimate. For example, a resolution of 0.1 Hz means that the frequency response variations at frequency intervals at or below 0.1 Hz cannot be resolved.\nConsider an analog band-limited signal \\(x(t)\\) with bandwidth \\(B\\) Hz; \\(x(t)\\) is observed over a sample period of \\(T_r\\) s (the length of the data record) and sampled at a sampling frequency of \\(f_s\\) Hz (\\(T_s=1/f_s\\) denotes the sampling interval). The total available number of samples of \\(x(t)\\) is \\(N=\\lfloor T_r/T_s\\rfloor\\), where \\(\\lfloor\\cdot\\rfloor\\) denotes the floor function (or greatest integer function), namely the function that takes as input a real number \\(r\\) and gives as output the greatest integer less than or equal to \\(r\\).\nAccording to the Shannon-Nyquist sampling theorem, if the sampling frequency \\(f_s\\) is chosen to be \\(2B\\), the maximum resolvable frequency is:\n\\[\nf_{\\text{max}}=\\frac{f_s}{2}\n\\]\nOn the other hand, the minimum resolvable frequency is inversely related to the sample period:\n\\[\nf_{\\text{min}}=\\frac{1}{T_r}=\\frac{1}{NT_s}\n\\tag{1}\\]\nand hence the number of frequencies that can be resolved from \\(f_{\\text{min}}\\) to \\(f_{\\text{max}}\\) is\n\\[\nN_f=\\frac{f_{\\text{max}}-f_{\\text{min}}}{\\Delta f}\n\\]\nwhere \\(\\Delta f\\) is the frequency resolution. Since \\(\\Delta f=f_{\\text{min}}\\), I also have:\n\\[\nN_f=\\dfrac{\\dfrac{f_s}{2}-\\dfrac{f_s}{N}}{\\dfrac{f_s}{N}}=\\frac{N}{2}-1\n\\]\nThis implies that will be \\(N/2\\) discrete frequencies from \\(0\\) to \\(f_{\\text{max}}\\).\nThe accurate detection of frequency components in the spectrum \\(X(f)\\) of the signal \\(x(t)\\) is challenged by the phenomenon known as spectral leakage, or amplitude ambiguity; it consists of ambiguous and false amplitudes occurring in the spectrum \\(X(f)\\) whenever the sample period \\(T_r\\) is not an integer multiple of all of the contributory periods in \\(x(t)\\). That is, false or ambiguous amplitudes will occur at frequencies that are immediately adjacent to the actual frequency.\nFor aperiodic signals, \\(T_r\\) theoretically must be infinite. For periodic signals, \\(T_r\\) must be equal to the least common integer multiple of all the periods contained in the signal. Application of the DFT (Discrete Fourier Transform) or FFT (Fast Fourier Transform) to an aperiodic signal implicitly assumes that the signal is infinite in length and formed by repeating the signal of length \\(T_r\\) an infinite number of times. This leads to discontinuities in the amplitude that occur at each integer multiple of \\(T_r\\). These discontinuities are step-like, which introduce false amplitudes that decrease around the main frequencies.\nAll the periods contained in the signal cannot be known before the spectral analysis is performed, therefore the stated condition on the sample period cannot be fulfilled; the best method to minimize the effect of spectral leakage is windowing.\nHerein, I just provide a brief explanation of windowing, without testing it in the examples to follow. Windowing reduces the amplitude of the discontinuities at the boundaries of each finite sequence of samples of \\(x(t)\\). It does so by multiplying the acquired sequence by a finite-length window with an amplitude that varies smoothly and gradually toward zero at the edges. This makes the endpoints of the waveform meet and, therefore, results in a continuous waveform without sharp transitions.\n\n\n\n\n\n\nCan discrete-time sinusoids be non-periodic?\n\n\n\nThe normalized frequency \\(f\\) in cycles per sample of a discrete-time sinusoid:\n\\[\nx[n]=\\cos(2\\pi fn)\n\\]\nis restricted to values in the interval \\(-1/2\\leq f\\leq1/2\\). This is because, for any discrete-time sinusoid with frequency \\(\\vert f\\vert&gt;1/2\\), an integer \\(m\\) exists such that \\(f=f_0+m\\) and \\(\\vert f_0\\vert\\leq1/2\\):\n\\[\n\\cos 2\\pi fn=\\cos[2\\pi (f_0+m)\\,n]=\\cos(2\\pi f_0n)+\\cos(2\\pi mn)=\\cos(2\\pi f_0n)\n\\]\nIt is worth noting that the highest rate of oscillation of discrete-time sinusoids is when, at every time instant, the output sample flips polarity with respect to the previous output sample:\n\\[\nf_0=\\frac{1}{2}\\rightarrow\\cos(2\\pi f_0n)=\\cos(\\pi n)=(-1)^n\n\\]\nMoreover, a discrete-time sinusoid can be periodic, i.e, characterized by patterns that exactly repeat themselves in time, if and only if its frequency can be expressed in terms of a rational number; moreover, an additive mixture of periodical discrete-time sinusoids (called here sinusoidal mixture) is periodic with period equal to the least common integer multiple of their periods.\n\n\nAs outlined above, a spectrum line at frequency \\(f\\) will be accurately represented by the DFT when \\(f_s\\geq2f_{\\text{max}}\\) and \\(T_r = mT\\), where \\(m=1,2,\\cdots\\), and \\(T=1/f\\). This implies that \\(N=mf_s/f\\). If \\(T_r\\) is not an integer multiple of \\(T\\), leakage will occur in the DFT. This appears as amplitudes at \\(f\\) spilling onto adjacent frequencies.\nAs an example, consider a sinusoidal mixture with three unit-amplitude components at 2.875 Hz, 3 Hz, 3.125 Hz. The sampling frequency is set to \\(f_s=16\\) Hz and the sample period to \\(T=8\\) s (\\(N=128\\)): the component at 2.875 Hz is observed for 23 periods, the component at 3 Hz for 24 periods, and the component at 3.125 Hz for 25 periods. The resolution calculated according to Equation 1 is 0.125 Hz.\nA chunk of code written in MATLAB (Natick, Massachusetts: The MathWorks, Inc., https://www.mathworks.com) shows how to simulate the sinusoidal mixture and to perform spectral analysis.\n% *****************************\n% sinusoidal mixture generation\n% *****************************\n\nfs = 16;   % sampling frequency, Hz\nTs = 1/fs; % sampling interval, s\nT  = 8;    % sample period, s\n\nt  = (0:Ts:T-Ts); % time domain, s\nN  = length(t);   % number of samples\n\nf1 = 2.9375;      % frequency component 1, Hz\nf2 = 3;           % frequency component 2, Hz\nf3 = 3.0625;      % frequency component 3, Hz\n\nx1 = cos(t.*(2*pi*f1)); % sinusoid 1\nx2 = cos(t.*(2*pi*f2)); % sinusoid 2\nx3 = cos(t.*(2*pi*f3)); % sinusoid 3\nX  = x1 + x2 + x3;      % sinusoidal mixture\n\n% ********************************************************\n% calculation of single-side spectrum (see comments below)\n% ********************************************************\n\nP              = fft(X, N);         % FFT calculation\nP2             = abs(P/N);          % double-side spectrum, rescaled by N     \nP1             = P2(1:N/2+1);  \nP1(:, 2:end-1) = 2*P1(2:end-1); \nY              = P1(1:end-1);       % single-side spectrum\nf              = (0:N/2-1).*(fs/N); % frequency domain, Hz\n\n\n\n\n\n\nComments\n\n\n\nThe documentation in MATLAB explains the procedure to convert the N-points FFT spectrum of the signal to the single-side amplitude spectrum:\n\nBecause the FFT function includes a scaling factor N between the original and the transformed signals, rescale the spectrum by dividing by N.\nTake the complex magnitude of the FFT spectrum. The two-side amplitude spectrum P2, where the spectrum in the positive frequencies is the complex conjugate of the spectrum in the negative frequencies, has half the peak amplitudes of the time-domain signal.\nTo convert to the single-side spectrum Y, take the first half P1 of the two-side spectrum P2 and multiply by 2. P1(1) and P1(end) are not multiplied by 2 because these amplitudes correspond to the zero and Nyquist frequencies, respectively, and they do not have the complex conjugate pairs in the negative frequencies.\nDefine the frequency domain f for the single-side spectrum Y.\n\n\n\nThe single-side spectrum of the sinusoidal mixture is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Single-side spectrum of the sinusoidal mixture composed of three components at 2.875 Hz, 3 Hz, 3.125 Hz; sampling frequency: 16 Hz; sample period: 8 s. For the sake of visualization the frequency interval shown is limited to 2.5-3.5 Hz.\n\n\n\nConsider now the case that the three components of the sinusoidal mixture have frequencies 2.9375 Hz, 3 Hz, 3.0625 Hz. To resolve them, I would need to double the frequency resolution of the FFT, up to 0.0625 Hz, with respect to the scenario of Figure 1. Erroneously, I double the sampling frequency. However, this expedient leaves the frequency resolution unaltered, since doubling the sample period without touching the sample period also doubles the number of samples. Moreover, the sample period is not properly chosen to avoid spectral leakage, as clearly seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 32 Hz; sample period: 8 s.\n\n\n\nTo resolve the components of the sinusoidal mixture and avoid spectral leakage, the frequency resolution of 0.0625 Hz can be achieved by extending the time period to \\(T_r=16\\) s, which corresponds to \\(N=256\\) samples at the original sampling frequency \\(f_s=16\\) Hz, see Figure 3.\n\n\n\n\n\n\nFigure 3: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 16 Hz; sample period: 16 s.\n\n\n\n\nTo conclude: whereas the sampling frequency sets the time resolution, the sample period occupies a central place in setting the frequency resolution when spectral analysis is performed. The sample period being the same, just increasing the sampling frequency simply increases the number of samples by the same ratio, leaving their ratio unaltered. The only means to increase the frequency resolution is to increase the sample period, trying at the same time to minimize the effects of spectral leakage."
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html",
    "href": "posts/statistics_vs_ML/index.html",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "",
    "text": "One of the most important similarities between statistics and machine learning (ML) is that both rely heavily on models. While this connection is sometimes overlooked, it forms the backbone of how we organize and analyze data. Regardless of the number and type of variables involved—whether explanatory or response variables, continuous or categorical—statistical modeling remains at the core of the process. Whether our goal is to predict outcomes, uncover relationships, or assess differences, modeling gives structure and meaning to the patterns we observe. ML, too, builds on this idea of modeling, but with a slightly different emphasis. Here, the focus is often more explicitly on defining a mathematical mapping between inputs and outputs—a modeling aspect that tends to be more immediately visible than in traditional statistical practice.\nSeveral practical factors make a full overlap between the two approaches less straightforward. One is the sheer difference in the size of datasets: classical statistical analyses often rely on relatively small samples, especially in fields such as medicine or social sciences. This distinction, however, is beginning to blur: modern applications in fields like digital health, education technology, or online behavioral studies increasingly generate larger datasets, enabling the use of ML-inspired methods even in traditionally small-sample domains. ML, by contrast, tends to operate in contexts where large datasets are available—or at least assumed. Another difference lies in the typical workflow: in ML, it is standard practice to split data into training, validation, and testing sets, managing the trade-off between fitting well and generalizing. This concern, central to ML, is less explicitly present in traditional statistical workflows.\nDespite these differences, statistics and ML increasingly share common objectives and approaches: building models that capture meaningful patterns while balancing complexity against generalization.(For readers unfamiliar with some of the technical terms, a brief summary is provided in the Key Concepts sections below.)"
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html#what-is-a-statistical-model",
    "href": "posts/statistics_vs_ML/index.html#what-is-a-statistical-model",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "What is a statistical model?",
    "text": "What is a statistical model?\nIn classical statistics — for example, for methods introduced early in the training of sophomore students, such as simple linear regression and ANOVA (ANalysis Of VAriance) — the model is typically written in a very structured form. Let us suppose that \\(Y\\) is the response variable (outcome) and \\(X_1,X_2,\\cdots,X_m\\) are explanatory variables (predictors), collected into the vector \\(\\mathbf{X}\\); the model is the equation that we suppose relate together the predictors to the outcome, for any of a number \\(n\\) of observational units, with an important element noticed in the formulation of the model-the random error term, \\(\\varepsilon\\):\n\\[\nY_{i}=f(\\mathbf{X}_i;\\theta)+\\varepsilon_i,\\quad i=1,...,n\n\\tag{1}\\]\nwhere \\(\\theta\\) are the parameters to be estimated. This decomposition highlights two essential ideas:\n\nThere is an underlying systematic component (\\(f(\\mathbf{X}_i;\\theta)\\)),\nPlus an error component (\\(\\varepsilon_i\\)), which captures variability not explained by the model.\n\nThe classical statistical inferential procedures—whether hypothesis testing or interval estimation—are fundamentally designed around the behavior of this error term. It is also important to realize that we can never observe the error term, but we can infer its behavior from the analysis of the model residual — the difference between observed and predicted values.\nIn classical parametric inference, crucial assumptions are made about the behavior of the error terms \\(\\varepsilon_i\\) in the general model Equation 1.\nTo ensure analytical tractability and valid inference, it is common to summarize these assumptions using the acronym LINE — Linearity, Independence, Normality, and Equality of variance:\n\nLinearity: The model is linear in the parameters \\(\\theta\\); that is, the expected value of the outcome variable \\(Y\\) can be expressed as a linear combination of the parameters, possibly through transformations of the predictors \\(\\mathbf{X}\\).\nIndependence: The error terms are independent across observations. In particular, for time series data, consecutive errors are assumed to be uncorrelated. In cross-sectional data, failure of independence often reflects issues in experimental design, such as inadequate random assignment or hidden confounding. Detecting such problems can be challenging and typically requires careful planning at the data collection stage.\nNormality: The error terms are normally distributed with mean zero.\nEquality of variance (Homoscedasticity): The error terms have constant variance across the range of the predictors.\n\nThese assumptions provide the theoretical foundation for estimating parameters, constructing confidence intervals, and performing hypothesis tests in a coherent analytical framework. Understanding how residuals behave is not only crucial in classical inference, but also serves as a bridge toward modern predictive approaches, as discussed in the note below.\n\n\n\n\n\n\nResiduals and ML\n\n\n\nThe concept of residuals — as discrepancies between observed outcomes and model predictions — remains central in ML as well. Although the term “residual” is less frequently emphasized explicitly, the analysis of prediction errors underpins critical tasks such as model evaluation, hyperparameter tuning, and post-hoc diagnostics across supervised learning workflows.\nUnderstanding residuals provides a natural bridge between classical inferential thinking and modern predictive modeling practices.\n\n\nThis careful structure is what allows classical statistics to calibrate confidence intervals, compute p-values, and estimate statistical power, all based on theoretically derived sampling distributions. However, when these assumptions are violated, the inferential reliability may break down — often in unpredictable ways. Modern approaches like bootstrap resampling offer alternatives that reduce dependency on strong theoretical assumptions, and interestingly, align closely with resampling strategies widely used in ML (such as cross-validation for hyperparameter tuning). In a sense, bootstrapping can be seen as a bridge: a classical method moving closer to the data-driven, flexible spirit that characterizes much of modern ML practice."
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html#key-concepts",
    "href": "posts/statistics_vs_ML/index.html#key-concepts",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "Key Concepts",
    "text": "Key Concepts\n\n\n\n\n\n\nStatistics-Based Key Concepts\n\n\n\n\nEstimator: A function of sample data used to infer the value of an unknown population parameter. Estimators form the basis for constructing confidence intervals and conducting hypothesis tests.\nSampling Distribution: The probability distribution of an estimator over many hypothetical repeated samples from the same population.\nConfidence Interval: An estimated range, derived from an estimator, that likely contains the true value of the population parameter with a given level of confidence.\np-value: The probability, under the null hypothesis, of obtaining a test statistic at least as extreme as the one observed.\nStatistical Power: The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true.\nType I Error: Incorrectly rejecting a true null hypothesis (false positive).\nType II Error: Failing to reject a false null hypothesis (false negative).\nResidual: The difference between the observed value and the corresponding predicted value from a statistical model. Residuals are empirical proxies for the unobservable error terms, enabling diagnostic analyses of model fit and assumptions.\nDegrees of Freedom: The number of independent values in a calculation that are free to vary. In statistical inference, degrees of freedom determine the shape of sampling distributions (e.g., t or F) and reflect the amount of information available to estimate variability.\n\n\n\n\n\n\n\n\n\nML-based key concepts\n\n\n\n\nConfusion Matrix: a table showing true positives, false positives, true negatives, and false negatives.\nROC Curve: a plot of true positive rate vs false positive rate across different thresholds.\nROC AUC: a single-number summary of a model’s discrimination ability across thresholds.\nOperating Point (Threshold Selection): the decision threshold balancing false positives and false negatives, based on context.\nClass Imbalance: a condition where the categories in the outcome variable are unequally represented. It can lead to biased models, misleading performance metrics, and poor generalization — especially when one class dominates. Common remedies include resampling, weighting, and using metrics like ROC AUC instead of raw accuracy. Although widely addressed in ML, class imbalance also affects classical inference, where low-prevalence categories can distort p-values and test power.\n\n\n\n\n\n\n\n\n\nAdvanced key concepts\n\n\n\n\nCross-Entropy Loss: measures the dissimilarity between predicted probabilities and true labels.\nRegularization: techniques (e.g., L1, L2) to prevent overfitting by penalizing model complexity.\nGradient Descent: an optimization method for minimizing loss functions.\nOverfitting and Underfitting: learning too much noise (overfitting) or missing the signal (underfitting)."
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html#from-hypothesis-testing-to-predictive-modeling",
    "href": "posts/statistics_vs_ML/index.html#from-hypothesis-testing-to-predictive-modeling",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "From Hypothesis Testing to Predictive Modeling",
    "text": "From Hypothesis Testing to Predictive Modeling\nLet us now illustrate the modeling perspective starting with a classical example: the independent-samples t-test. First, the neded libraries are uploaded.\n\n\nCode\n# Load required libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(kableExtra)\n\n\n\nSimulating Data for Classical Inference\n\n# Simulate two groups with slightly different means\nset.seed(123)\ngroup1 &lt;- rnorm(64, mean = 5.0, sd = 1.0)\ngroup2 &lt;- rnorm(64, mean = 5.5, sd = 1.0)\n\n# Combine data into a single data frame\ndf &lt;- data.frame(\n  group = factor(c(rep(\"A\", 64), rep(\"B\", 64))),\n  outcome = c(group1, group2)\n)\n\nWe simulate two groups, randomly drawn from two normally distributed populations, with means \\(\\mu_A = 5.0\\) and \\(\\mu_B = 5.5\\), and standard deviations \\(\\sigma_A = \\sigma_B = 1.0\\). Each group contains \\(n = 64\\) observations. In the spirit of a real experiment, the population parameters are unknown — the only information we have comes from the observed data. Our inferential task is to test whether a difference exists between the group means, under the standard null and alternative hypotheses:\n\\[\nH_0: \\mu_A = \\mu_B \\quad\\quad H_1: \\mu_A \\ne \\mu_B\n\\]\nIn this example, we simulate 64 observations per group — a total of 128 — to achieve 80% statistical power for detecting a medium effect size (\\(d = 0.5\\)) with a two-sided t-test at the conventional 5% significance level. This decision is not arbitrary: in a well-planned experiment, the sample size should be defined before data collection, based on acceptable error rates, either of Type I or Type II. Surprisingly, many — even published — experimental studies skip this crucial planning step, treating sample size as an afterthought rather than a design parameter.\nThis consideration can be visualized by examining how statistical power varies with the sample size and the effect size. The plot in Figure 1 shows the power curves for three typical values of Cohen’s effect size (\\(d = 0.2\\), \\(0.5\\), \\(0.8\\)), representing small, medium, and large effects. The dashed line marks the conventional threshold of 80% power.\nAs the plot shows, detecting a medium effect with 80% reliability requires at least 64 observations per group. Informal rules of thumb such as “30 is enough” may drastically underestimate the sample size needed for meaningful results.\n\n\n\n\n\n\n\n\nFigure 1: Power curves for two-sample t-tests at significance level (= 0.05), across three common effect sizes ((d = 0.2), (0.5), (0.8)). The dashed horizontal line marks the conventional 80% power threshold, often adopted as a design goal. The plot shows how detecting even a moderate effect requires at least 64 observations per group — far more than suggested by popular heuristics.\n\n\n\n\n\n\n\nPerforming a t-test and Fitting a Regression Model\nIn the snippet below, we perform a t-test and fit a regression model on the same simulated dataset. The outputs are produced by different functions but convey equivalent statistical information — all centered on testing whether the two group means differ. This equivalence is not accidental: it arises from how categorical variables are treated in regression models. Specifically, dummy (or indicator) coding when doing ANOVA translates group membership into a numeric predictor, allowing the regression coefficient to represent the mean difference between groups — exactly as tested by the t-test.\n\n\nAnalysis code\n# Perform an independent-samples t-test\nt_test_res &lt;- t.test(outcome ~ group, data = df, var.equal = TRUE)\n\n# Fit a linear model with group as a predictor\nlm_model &lt;- lm(outcome ~ group, data = df)\n\n# Apply ANOVA to the linear model\nanova_res &lt;- anova(lm_model)\n\n\n\n\nDisplay results\n\n\n\n→ t-test\n──────────────\n\n\n\n    Two Sample t-test\n\ndata:  outcome by group\nt = -2.8597, df = 126, p-value = 0.004964\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -0.7629029 -0.1388665\nsample estimates:\nmean in group A mean in group B \n       5.038478        5.489362 \n\n\n\n→ ANOVA\n──────────────\n\n\nAnalysis of Variance Table\n\nResponse: outcome\n           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \ngroup       1   6.506  6.5055  8.1781 0.004964 **\nResiduals 126 100.231  0.7955                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n→ Linear Model (summary)\n──────────────\n\n\n\nCall:\nlm(formula = outcome ~ group, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.29853 -0.59548 -0.05597  0.53407  2.19797 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0385     0.1115   45.19  &lt; 2e-16 ***\ngroupB        0.4509     0.1577    2.86  0.00496 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8919 on 126 degrees of freedom\nMultiple R-squared:  0.06095,   Adjusted R-squared:  0.0535 \nF-statistic: 8.178 on 1 and 126 DF,  p-value: 0.004964\n\n\nHow should we interpret these numbers? The goal of our analysis was to assess whether the two groups differ in their mean outcome. Both the t-test and the regression model provide consistent answers to this question.\nIn the regression output, the coefficient groupB = 0.4509 represents the estimated difference in means between the two groups. The associated p-value (0.00496) indicates that this difference is statistically significant at the conventional 5% level — strong evidence against the null hypothesis \\((H_0:\\mu_A=\\mu_B)\\).\nThe F-statistic from the ANOVA table (8.178) confirms this result, and it is numerically equal to the square of the t-statistic from the regression output \\((F=t^2)\\), as expected when comparing two groups using a linear model.\nThe residual standard error (0.8919) provides an estimate of the variability within groups, and the degrees of freedom (126) reflect that all available data were used for estimation. This is typical in classical inference, where no data is held out for validation — in contrast with ML practice.\n\n\n\n\n\n\n🎯 Interpretation\n\n\n\nAs expected, the t-statistic and the p-value from the t-test correspond exactly to the t-statistic and p-value for the group coefficient in the regression model.\n\n\n\n\n\n\n\n\nt, F and the language of models\n\n\n\nAlthough t.test() and lm() appear to be different tools, they both test the same hypothesis — that the group means differ. The t-statistic for the group coefficient in the regression is identical to the one returned by the t-test. Furthermore, the anova() function applied to the linear model produces an F-statistic equal to the square of the t-statistic: \\((F = t^2)\\). Different outputs, same logic.\n\n\n\n\n\n\n\n\nNormality and sample size\n\n\n\nThe values of the t and F statistics shown in the tables are derived from the Student’s t and Fisher’s F distributions, respectively. These distributions — and the reliability of the associated inference — rely on the assumption that residuals follow a normal distribution.\nThis assumption is especially critical when sample sizes are small. As sample size increases, the central limit theorem ensures that the sampling distribution of the test statistic becomes approximately normal, even if the underlying data are not. With 64 observations per group, the inference is considered robust to moderate departures from normality.\n\n\nThe p-value expresses the degree of statistical surprise associated with the observed value of the test statistic — under the assumption that the null hypothesis is true. In other words, it quantifies how incompatible the data are with what would be expected from the sampling distribution under the null. Paradoxically, in statistical practice we often look for surprise: the lower the p-value, the more tempted we are to conclude that the null hypothesis may not hold.\nThe degrees of freedom associated with a statistical model offer a practical indication of its vulnerability to overfitting. Since they increase with the amount of data and decrease with the number of parameters estimated, a low number of degrees of freedom is a warning signal: the model may be too complex relative to the information available. In traditional statistical practice, this aspect is often underappreciated — yet it plays a critical role in determining the stability and generalizability of inferential results.\nThis example shows how seemingly different statistical tools — a t-test, a linear model, and an ANOVA — all converge toward the same conclusion, using distinct but mathematically connected representations. What may appear at first as separate techniques are, in fact, different expressions of the same modeling logic.\nIt is also worth noting that, in this classical setting, all available observations — 64 per group, for a total of 128 — are used entirely for model estimation. No data is reserved for model validation. All degrees of freedom are “spent” in fitting and inference, with no external check on how well the model might generalize to new data. This design choice — to use all data for estimation — is rooted in the assumptions and goals of classical inference. In contrast, ML workflows routinely reserve part of the data for validation, tuning, and performance assessment."
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html#from-inference-to-prediction-a-first-machine-learning-example",
    "href": "posts/statistics_vs_ML/index.html#from-inference-to-prediction-a-first-machine-learning-example",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "From Inference to Prediction: A First Machine Learning Example",
    "text": "From Inference to Prediction: A First Machine Learning Example\nWe now shift from inference to prediction. In supervised ML, model validation is built into the workflow — often using separate training and testing sets — to evaluate how well the model generalizes.\nBefore diving deeper, it’s worth highlighting that although ML is often associated with “algorithms,” the core idea remains modeling. Algorithms are tools to find models — representations of patterns in the data — much as in statistics. The shift lies in the greater flexibility and adaptability expected of models built via ML.\nLet’s now construct a simple classification example using simulated data. To keep the comparison with the previous example meaningful, we simulate two equally sized groups — 64 observations each — and generate two continuous predictors for classification. As with the t-test setting, the balanced design is not strictly necessary, but it remains highly advisable. Severe class imbalance can distort statistical inference and degrade the performance of classifiers, especially when probabilistic calibration is involved.\nWherever possible, experimental design should strive for balanced or near-balanced group sizes. In statistical inference, imbalance may reduce the precision of estimates or distort p-values. In ML, it can lead to biased classifiers, especially when the outcome classes are highly skewed. While techniques exist to handle imbalance — such as weighting, oversampling, or downsampling, it is good experimental practice to minimize it at the design stage whenever possible.\n\nSimulating Data for Binary Classification\n\n# Simulate data\nset.seed(123)\nn &lt;- 100\nx1 &lt;- c(rnorm(n, 2.5, 1), rnorm(n, 3.75, 1))\nx2 &lt;- c(rnorm(n, 2.5, 1), rnorm(n, 3.75, 1))\nclass &lt;- factor(c(rep(0, n), rep(1, n)), levels = c(\"0\", \"1\"))\n\ndf_ml &lt;- tibble(x1 = x1, x2 = x2, class = class)\n\nThe two classes are linearly separable, by design — each group was generated from a distinct bivariate Gaussian distribution, Figure 2. This separation facilitates learning and allows us to explore classification behavior in a clean, controlled setting.\n\n\n\n\n\n\n\n\nFigure 2: Simulated dataset with two linearly separable classes, used to illustrate binary classification.\n\n\n\n\n\n\n\nFitting a Logistic Regression Model\nAlthough the name might suggest otherwise, logistic regression is not a method for modeling continuous outcomes. It is, in fact, one of the simplest and most widely used algorithms for binary classification. Given its solid statistical foundations, efficiency, and interpretability, logistic regression often serves as a reliable first choice in many applied ML workflows.\nRather than predicting a numeric value, the model estimates the probability that each observation belongs to a given class — typically class 1. The model learns a relationship between the features and this probability, and a threshold (usually 0.5) is then applied to assign class labels.\nHere we use the tidymodels framework to fit the model. Tidymodels provides a unified and expressive syntax for specifying, training, and evaluating models in R, following the tidyverse principles.\nTo keep the example simple but realistic, we perform a basic train/test split before fitting the model. This reflects a fundamental aspect of the ML workflow: models are expected to generalize to new, unseen data. In contrast to traditional statistical modeling — where inference is often based on the full dataset, with uncertainty captured via confidence intervals or p-values — ML emphasizes predictive performance on held-out data.\nSince logistic regression has no tunable hyperparameters, and our example involves just two features, we do not perform cross-validation or model tuning. A single split suffices to illustrate the predictive logic.\n\n\nAnalysis code\n# Split and annotate\nset.seed(123)\nsplit &lt;- initial_split(df_ml, prop = 0.75, strata = class)\ntrain_df &lt;- training(split) %&gt;% mutate(set = \"Train\")\ntest_df  &lt;- testing(split)  %&gt;% mutate(set = \"Test\")\n\ndf_all   &lt;- bind_rows(train_df, test_df)\n\n# Fit logistic regression on training set\nmodel_ml &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(class ~ x1 + x2, data = train_df)\n\n\n\n\nVisualizing the Decision Boundary\nIn classical statistical testing, the choice of an operational point is often driven by conventions: a significance level of 0.05 and a power of 0.80 are typically adopted without extensive reflection. In ML, however, this decision is more explicitly tied to the ROC curve. The model provides estimated probabilities, and it is up to the practitioner — in consultation with stakeholders — to choose a classification threshold that reflects real-world priorities: minimizing false positives, false negatives, or some balanced trade-off. In the example discussed here, the boundary decision is plotted as a line over the point distributions of the two classes, as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Decision boundary produced by a logistic regression classifier fitted on the training set (circles). Points in the test set are shown as triangles. Class 0 and Class 1 are represented in blue and red, respectively. The model learns to separate the two classes by estimating a probability of class membership and applying a threshold (here, 0.5). This visualization helps assess how well the model generalizes, especially when overlaid on unseen test data.\n\n\n\n\n\n\n\nEvaluating Performance: ROC Curve and AUC\nWe now evaluate model performance using the ROC curve, introduced earlier in the ML-based key concepts callout.\n\n\nCode\n# Predict and evaluate\ntest_df &lt;- test_df %&gt;%\n  bind_cols(predict(model_ml, new_data = test_df, type = \"prob\"))\n\nroc_data &lt;- roc_curve(test_df, truth = class, .pred_0)\nauc_val  &lt;- roc_auc(test_df, truth = class, .pred_0)\n\n# Predict on training set (add probabilities + class prediction)\ntrain_df &lt;- train_df %&gt;%\n  bind_cols(predict(model_ml, new_data = ., type = \"prob\")) %&gt;%\n  mutate(pred_class = if_else(.pred_1 &gt;= 0.5, \"1\", \"0\") %&gt;% factor(levels = c(\"0\", \"1\")))\n\n# Ensure predicted class exists on test set\ntest_df &lt;- test_df %&gt;%\n  mutate(pred_class = if_else(.pred_1 &gt;= 0.5, \"1\", \"0\") %&gt;% factor(levels = c(\"0\", \"1\")))\n\n\nThis curve, shown in Figure 4, summarizes the trade-off between sensitivity and specificity across all possible thresholds, and provides a basis for threshold-independent comparison between classifiers.\n\n\n\n\n\n\n\n\nFigure 4: ROC curve for the logistic regression model, evaluated on the test set. The curve shows how sensitivity (true positive rate) and specificity (1 - false positive rate) evolve as the decision threshold varies. This version reproduces the staircase structure typical of ROC curves, using a step plot with cleaned and ordered data. The dashed line represents chance-level performance. The area under the curve (AUC) summarizes overall discriminative ability.\n\n\n\n\n\nThe ROC curve provides a global view of model discrimination across all possible thresholds, but model decisions are ultimately made at specific thresholds. This is where the confusion matrix (Figure 5) and the classification metrics table come into play: they reveal how a fixed cutoff — here 0.5 — translates into concrete outcomes in terms of correct and incorrect predictions. Such threshold-dependent evaluations are essential when aligning models with real-world decision-making contexts.\n\n\n\n\n\n\n\n\nFigure 5: Confusion matrix for the logistic regression model on the test set (threshold = 0.5). The matrix summarizes the number of correct and incorrect predictions, broken down by actual and predicted class. It highlights how model decisions play out at a specific threshold, complementing the global view offered by the ROC curve.\n\n\n\n\n\nTo complement the confusion matrix, we report key performance metrics in Table 1. These include accuracy, sensitivity (recall for the positive class), and specificity (true negative rate), computed separately on the training and test sets.\n\n\nCode\n# Compute each metric separately, then combine\nmetrics_train &lt;- tibble(\n  set = \"Train\",\n  accuracy = yardstick::accuracy(train_df, truth = class, estimate = pred_class) %&gt;% pull(.estimate),\n  sensitivity = yardstick::sens(train_df, truth = class, estimate = pred_class) %&gt;% pull(.estimate),\n  specificity = yardstick::spec(train_df, truth = class, estimate = pred_class) %&gt;% pull(.estimate)\n)\n\nmetrics_test &lt;- tibble(\n  set = \"Test\",\n  accuracy = yardstick::accuracy(test_df, truth = class, estimate = pred_class) %&gt;% pull(.estimate),\n  sensitivity = yardstick::sens(test_df, truth = class, estimate = pred_class) %&gt;% pull(.estimate),\n  specificity = yardstick::spec(test_df, truth = class, estimate = pred_class) %&gt;% pull(.estimate)\n)\n\n\nThis summary highlights how well the model generalizes and reinforces the idea that threshold selection — while often fixed by convention in statistical testing — plays a pivotal role in applied ML.\n\n\n\n\nTable 1: Summary of classification metrics at threshold 0.5 for training and test sets. Reported metrics include accuracy, sensitivity (recall for class 1), and specificity (true negative rate). These help interpret model performance beyond the AUC, and illustrate how different metrics depend on the choice of decision threshold.\n\n\n\n\n\n \n  \n    set \n    accuracy \n    sensitivity \n    specificity \n  \n \n\n  \n    Train \n    0.76 \n    0.787 \n    0.733 \n  \n  \n    Test \n    0.74 \n    0.680 \n    0.800"
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html#concluding-remarks",
    "href": "posts/statistics_vs_ML/index.html#concluding-remarks",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nThe performance table above reveals not just how well the model generalizes from training to test data, but also highlights a deeper contrast between traditional statistical inference and machine learning practice. In classical statistics, the choice of a decision threshold — say, a significance level \\(\\alpha=0.05\\) — is often fixed by convention, serving as a gatekeeper for hypothesis testing. In ML, by contrast, the threshold (\\(0.5\\) in our case) is not sacred: it can and should be adapted depending on context, cost, and stakeholder priorities.\nThe ROC curve offers a global view of the model’s discriminative ability across all thresholds, while the confusion matrix and the table of metrics make concrete the implications of a specific threshold. Together, they illustrate that prediction is not just about accuracy. It’s about decisions, trade-offs, and the flexibility to adapt models to real-world needs. And that’s precisely where statistical reasoning and ML begin to meet — not in opposition, but in dialogue.\nThe tension between theoretical rigor and empirical adaptability will continue to shape the evolving relationship between statistics and ML. In future explorations, we will examine how models behave when ideal conditions break down, and how different strategies — from regularization to robust modeling — attempt to meet the timeless challenge of separating signal from noise."
  },
  {
    "objectID": "posts/statistics_vs_ML/index.html#further-reading",
    "href": "posts/statistics_vs_ML/index.html#further-reading",
    "title": "Statistics and Machine Learning: A shared landscape",
    "section": "Further Reading",
    "text": "Further Reading\nIf you’d like to dig deeper into the ideas behind inference, prediction, and modern data modeling, here are some recommended readings — ranging from foundational texts to more advanced perspectives:\nAll of Statistics — Larry Wasserman (2004) A concise and mathematically grounded guide to statistical reasoning, ideal for those transitioning into data science.\nStatistics — Freedman, Pisani, Purves (2007) A clear and accessible introduction to the logic and language of statistics.\nApplied Predictive Modeling — Kuhn & Johnson (2013) A hands-on resource focused on practical machine learning with a statistical backbone.\nDeep Learning — Goodfellow, Bengio, Courville (2016) The reference text for deep learning, covering theory and implementation.\nComputer Age Statistical Inference — Efron & Hastie (2016) A beautifully written journey through the convergence of statistics and algorithmic thinking."
  },
  {
    "objectID": "posts/significant_figures/index.html",
    "href": "posts/significant_figures/index.html",
    "title": "Significant figures",
    "section": "",
    "text": "It happens frequently that the number of digits that are available to report measurement results is high. Usually, measurement results are produced by carrying arithmetic operations with computers or calculators, whose level of numerical precision, albeit finite, is too high given the true information gathered by the measurements. In other words, the precision can be excessive, and too many digits can simply swamp the observer, making the message in the measurements more obscure. Significant figures, also referred to as significant digits, are specific digits within a number written in positional notation that carry both reliability and necessity in reporting a measurement result. Proper use of significant figures is thus an essential element in the presentation of both experimental and calculated results together with their associated uncertainty.\n\n\nA number of rules exist for determining how many significant figures are in a number:\n\nnon-zero digits are always significant\n\n\n4.6 has two significant figures\n\n\nleading zeros placed before the first non-zero digit are not significant (they are called placeholders)\n\n\n0.046 has two significant figures\n\n\ntrailing zeros placed after all other digits but behind a decimal point are significant\n\n\n4.60 has three significant figures\n\nThe leftmost digit which is not a zero is referred to as the most significant digit (MSD); the rightmost digit of a decimal number is the least significant digit (LSD), regardless it is a zero or not: 4 and 0 are thus, respectively, the MSD and the LSD of 4.60; 4 and 1 are, respectively, the MSD and the LSD of 4.61. Every digit between the LSD and the MSD, including zeros, should be counted as significant figures, hence 4.60 and 40.60 have, respectively, three and four significant figures.\nAmbiguous situations arise when zeros are at the end of the number and not behind a decimal point as, for example, in the number 4600. Confusion can be avoided if the number is expressed in scientific notation.\n\n\n\n\n\n\nScientific notation\n\n\n\nScientific notation is a way of expressing numbers that are much too large or much too small to be conveniently written in decimal form (i.e., their representation would involve a long string of digits). In scientific notation, nonzero numbers are written in the form:\n\\[\nm\\times10^n\n\\]\nwhere \\(n\\) is an integer, and the coefficient \\(m\\) is a nonzero real number (usually \\(1\\leq\\vert\\,m\\,\\vert&lt;10\\)). The integer \\(n\\) is called the exponent and the real number \\(m\\) is called the mantissa. If the number is negative, then a minus sign precedes \\(m\\), as in ordinary decimal notation.\n\n\nIn scientific notation, the number 4600 can be written using a different number of significant figures, based on rule 3. above:\n\\[\n\\begin{split}\n4.600\\times 10^3&\\quad\\text{four significant figures}\\\\\n4.60\\times 10^3&\\quad\\text{three significant figures}\\\\\n4.6\\times 10^3&\\quad\\text{two significant figures}\n\\end{split}\n\\]\n\n\n\nA number can be rounded so as to drop digits until a prescribed number of significant figures is retained in the final representation. Recall that all the digits after the decimal point to the right of the desired LSD are to be dropped and not replaced with zeros, which otherwise should add to the number of significant figures (rule 3 above). The rules of rounding are the following:\n\nif the digit to the right of the desired LSD is greater than 5, add 1 to the LSD, otherwise do nothing\n\n\nExample - round at the fourth significant figure\n\n\\[\n\\begin{split}\n53.8\\underline{7}4&\\rightarrow53.87\\\\\n53.8\\underline{7}9&\\rightarrow53.88\n\\end{split}\n\\]\n\nif the digit to the right of the LSD is 5, apply a tie-breaking rule, also called the five rule. When the first digit to be dropped is 5, the leading digit next to it is examined. If this digit is even, including zero, it is left unaltered; otherwise, one unit is added. This helps avoiding the accumulation of errors that would be otherwise determined by rounding systematically up or down. Using the five rule, five out of ten cases consist of rounding up and five out of ten cases consist of rounding down.\n\n\nExample - round at the fifth significant figure\n\n\\[\n\\begin{split}\n726.8\\underline{0}51\\rightarrow 726.80\\\\\n726.8\\underline{3}51\\rightarrow 726.84\\\\\n726.8\\underline{6}51\\rightarrow 726.86\\\\\n726.8\\underline{9}51\\rightarrow 726.90\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/significant_figures/index.html#significant-figures",
    "href": "posts/significant_figures/index.html#significant-figures",
    "title": "Significant figures",
    "section": "",
    "text": "It happens frequently that the number of digits that are available to report measurement results is high. Usually, measurement results are produced by carrying arithmetic operations with computers or calculators, whose level of numerical precision, albeit finite, is too high given the true information gathered by the measurements. In other words, the precision can be excessive, and too many digits can simply swamp the observer, making the message in the measurements more obscure. Significant figures, also referred to as significant digits, are specific digits within a number written in positional notation that carry both reliability and necessity in reporting a measurement result. Proper use of significant figures is thus an essential element in the presentation of both experimental and calculated results together with their associated uncertainty.\n\n\nA number of rules exist for determining how many significant figures are in a number:\n\nnon-zero digits are always significant\n\n\n4.6 has two significant figures\n\n\nleading zeros placed before the first non-zero digit are not significant (they are called placeholders)\n\n\n0.046 has two significant figures\n\n\ntrailing zeros placed after all other digits but behind a decimal point are significant\n\n\n4.60 has three significant figures\n\nThe leftmost digit which is not a zero is referred to as the most significant digit (MSD); the rightmost digit of a decimal number is the least significant digit (LSD), regardless it is a zero or not: 4 and 0 are thus, respectively, the MSD and the LSD of 4.60; 4 and 1 are, respectively, the MSD and the LSD of 4.61. Every digit between the LSD and the MSD, including zeros, should be counted as significant figures, hence 4.60 and 40.60 have, respectively, three and four significant figures.\nAmbiguous situations arise when zeros are at the end of the number and not behind a decimal point as, for example, in the number 4600. Confusion can be avoided if the number is expressed in scientific notation.\n\n\n\n\n\n\nScientific notation\n\n\n\nScientific notation is a way of expressing numbers that are much too large or much too small to be conveniently written in decimal form (i.e., their representation would involve a long string of digits). In scientific notation, nonzero numbers are written in the form:\n\\[\nm\\times10^n\n\\]\nwhere \\(n\\) is an integer, and the coefficient \\(m\\) is a nonzero real number (usually \\(1\\leq\\vert\\,m\\,\\vert&lt;10\\)). The integer \\(n\\) is called the exponent and the real number \\(m\\) is called the mantissa. If the number is negative, then a minus sign precedes \\(m\\), as in ordinary decimal notation.\n\n\nIn scientific notation, the number 4600 can be written using a different number of significant figures, based on rule 3. above:\n\\[\n\\begin{split}\n4.600\\times 10^3&\\quad\\text{four significant figures}\\\\\n4.60\\times 10^3&\\quad\\text{three significant figures}\\\\\n4.6\\times 10^3&\\quad\\text{two significant figures}\n\\end{split}\n\\]\n\n\n\nA number can be rounded so as to drop digits until a prescribed number of significant figures is retained in the final representation. Recall that all the digits after the decimal point to the right of the desired LSD are to be dropped and not replaced with zeros, which otherwise should add to the number of significant figures (rule 3 above). The rules of rounding are the following:\n\nif the digit to the right of the desired LSD is greater than 5, add 1 to the LSD, otherwise do nothing\n\n\nExample - round at the fourth significant figure\n\n\\[\n\\begin{split}\n53.8\\underline{7}4&\\rightarrow53.87\\\\\n53.8\\underline{7}9&\\rightarrow53.88\n\\end{split}\n\\]\n\nif the digit to the right of the LSD is 5, apply a tie-breaking rule, also called the five rule. When the first digit to be dropped is 5, the leading digit next to it is examined. If this digit is even, including zero, it is left unaltered; otherwise, one unit is added. This helps avoiding the accumulation of errors that would be otherwise determined by rounding systematically up or down. Using the five rule, five out of ten cases consist of rounding up and five out of ten cases consist of rounding down.\n\n\nExample - round at the fifth significant figure\n\n\\[\n\\begin{split}\n726.8\\underline{0}51\\rightarrow 726.80\\\\\n726.8\\underline{3}51\\rightarrow 726.84\\\\\n726.8\\underline{6}51\\rightarrow 726.86\\\\\n726.8\\underline{9}51\\rightarrow 726.90\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/significant_figures/index.html#finite-precision-arithmetic",
    "href": "posts/significant_figures/index.html#finite-precision-arithmetic",
    "title": "Significant figures",
    "section": "Finite precision arithmetic",
    "text": "Finite precision arithmetic\nIn mathematical operations involving significant figures, the result cannot be more precise than the least precise number. Calculations in finite precision arithmetic can be done following a few simple rules. One rule applies to multiplication and division, and another rule applies to addition and subtraction. Recall that values that are considered exact numbers, e.g., known conversion factors or physical constants, are not to be included in the determination of the number of significant figures.\n\nMultiplication and division\n\nWhen we multiply/divide two numbers, we should add their relative uncertainties. The uncertainty of the result is given roughly by the number of the digits, regardless of their placement.\n\nIn a calculation involving multiplication/division the number of significant figures in the result should equal the least number of significant figures in any one of the numbers being multiplied or divided.\nIn the following example, the number 1.6 is reported with two significant figures; the number 2, seen as a known constant, can be considered having an infinite number of significant figures, whereas the number 2.0 has two significant figures. The result should be reported with two significant figures in both cases:\n\\[\n\\begin{split}\n&1.6\\times2=3.2&\\\\\n&1.61\\times2.0=3.2&\\quad\\text{not}\\;3.22\n\\end{split}\n\\]\n\n\nAddition and subtraction\n\nWhen we add/subtract two numbers, we should add their uncertainties. The uncertainty of the result is given roughly by the placement of the digits, not by the number of digits.\n\nIn a calculation involving addition/subtraction, the number of decimal places in the result should equal the least number of decimal places in any one of the numbers being added or subtracted.\nIn the following example, the number 132.03 is reported with five significant figures, and the number 3.210 is reported with four significant figures. However, when the two numbers are added, what matters really is the number of decimal places, i.e., two for the number 132.03 and three for the number 3.210. The result should be reported with two decimal digits and not reported using four significant figures.\n\\[\n\\begin{split}\n&132.03+3.210=135.24&\\;\\text{and not}\\;135.2\\\\\n&132.03+3=135&\\;\\text{and not}\\;135.03\\\\\n&132.03+3.00=135.03&\\;\\text{and not}\\;135\\\\\n\\end{split}\n\\]\nThe prescription about the minimum number of decimal places of any of the numbers involved in the calculation can be explained by considering that, implicitly, the precision of any measurement is dictated by the decimal place. For a measurement of length expressed in meters, for example, the second decimal digit implies a measurement precise to the hundredths (centimeter-level), the third decimal digit to the thousandth (millimeter-level). So by keeping the result with the minimum number of decimal places we basically state that we do not want to imply to get a result more precise than the least precise measurement that was needed to produce the result itself.\n\n\nMultiple arithmetic operations\n\nIn a calculation involving multiple arithmetic operations, the rules are applied without rounding results after each intermediate step. Instead keep track of the rightmost digit that would be retained. The operations would be performed in the following order:\n\n\noperations in parentheses ( )\nmultiplication\ndivision\naddition\nsubtraction\n\nIt is important to always perform intermediate calculations without rounding the numbers that are involved in the operations. If numbers are rounded every time during many sequential calculations, the results are skewed and some systematic error is surely introduced. Only after that all calculations are carried out with all digits retained at each step, the final result has to be rounded to the desired number of significant figures.\nAs an example, two numbers reported with five significant figures each are added, and the final result is rounded to three significant figures. If the addends are first rounded to three significant figures and then added, the result we produce is wrong:\n\\[\n\\begin{split}\n&1.4248+1.2732=2.6980\\rightarrow 2.70&\\quad\\text{correct}\\\\\n&1.42+1.27=2.69\\rightarrow 2.70&\\quad\\text{wrong}\n\\end{split}\n\\]\n\nExample 1 (Sequential calculation) Suppose that we want to perform the following operation:\n\\[\n(2.5\\times3.42)+13.681-0.1\n\\]\n\nperform first the product between parentheses - we keep track of the first decimal place, which would be retained based on rule B above.\n\n\\[\n2.5\\times3.42=8.\\underline{5}500\n\\]\n\nperform addition - although, based on rule A above, the result would be expressed using five significant figures, only the first decimal place is kept tracked:\n\n\\[\n8.5500+13.681=22.\\underline{2}310\n\\]\n\nperform subtraction:\n\n\\[\n22.2310-0.1=22.\\underline{1}31\n\\]\n\nrounding to three significant figures:\n\n\\[\n(2.5\\times3.42)+13.681-0.1\\rightarrow22.1\n\\]\n\nWhen doing multi-step calculations, we need:\n\nto keep at least one more significant figure in intermediate results than needed in the final answer. Furthermore, never round intermediate answers: rounding, say, to two significant figures in an intermediate answer, and then writing three significant figures in the final answer is wrong.\nnot to write more significant figures in the final result (of a measurement process) than justified (by the measurement uncertainty)."
  },
  {
    "objectID": "posts/significant_figures/index.html#significant-figures-and-measurement-uncertainty",
    "href": "posts/significant_figures/index.html#significant-figures-and-measurement-uncertainty",
    "title": "Significant figures",
    "section": "Significant figures and measurement uncertainty",
    "text": "Significant figures and measurement uncertainty\nThe value of one measurand must be delivered by rounding the digit loaded by the measurement uncertainty \\(U\\), where \\(U\\) is represented by a number with, usually, no more than one or two significant figures (rounded up, possibly). The additional uncertainty due to rounding must be checked for being negligible compared to \\(U\\). Essentially, \\(U\\) gives an estimate of the errors incurred in the measurement.\nFor example, if we have a length \\(L=(12.37\\pm0.10)\\;\\text{cm}\\), we can report the length as \\(L=12.4\\;\\text{cm}\\). When we express a number with three significant figures, what we are saying is that the first two digits are essentially exactly correct, and the last one is uncertain by a small amount (generally it is only uncertain by about \\(\\pm1\\)). In the example above, we rounded our answer to \\(12.4\\;\\text{cm}\\) because our answer is uncertain to \\(\\pm0.1\\;\\text{cm}\\), namely our answer is uncertain in the last digit by about 1.\n\nExample 2 (Rounding) Round the measurement \\(z=12.0349\\;\\text{cm}\\), whose uncertainty is stated being \\(\\Delta z=0.153\\;\\text{cm}\\).\n\nround the uncertainty to two significant figures:\n\n\\[\n\\Delta z=0.15\\,\\text{cm}\n\\]\n\nround \\(z\\) using the same number of decimal places as \\(\\Delta z\\):\n\n\\[\nz=12.03\\,\\text{cm}\n\\]\n\nprovide the measurement report:\n\n\\[\nz\\pm\\Delta z=(12.03\\pm0.15)\\,\\text{cm}\n\\]\n\n\nExample 3 (Use of the scientific notation) When the answer is given in scientific notation, the uncertainty should be given in scientific notation with the same power of ten as the answer. Suppose that \\(z=1.43\\times10^6\\;\\text{s}\\) and \\(\\Delta z=2\\times10^4\\;\\text{s}\\):\n\\[\nz\\pm\\Delta z=(1.43\\pm0.02)\\,10^6\\;\\text{s}\n\\]\n\n\nExample 4 (Addition/subtraction of uncertain numbers) The length of two blocks is measured, and the measurements are \\(l_1=1.13\\;\\text{m}\\) (considered precise to the level of centimeters) and \\(l_2=0.551\\;\\text{m}\\) (considered precise to the level of millimeters). We need to compute the length \\(l\\) of the block resulting from stacking the two blocks together:\n\\[\nl=l_1+l_2=1.681\\;\\text{m}\\rightarrow l=1.68\\;\\text{m}\n\\]\nIt does not make any physical sense to consider the length of the overall block precise to the level of the millimeters, given that one of the two blocks is measured less precisely. The result should be at least as precise as the least precise term involved in the addition, as stated by the rule for addition/subtraction of uncertain numbers.\n\n\nExample 5 (Multiplication/division of uncertain numbers) A rectangular floor needs to be covered by a number of squared tiles. According to the measurements that are available, the rectangular floor has width \\(w=1.91\\;\\text{m}\\) and length \\(l=1.57\\;\\text{m}\\) and each squared tile has size \\(a=0.15\\;\\text{m}\\). All measurements are considered precise to the level of centimeters, and three significant figures should then be considered for their numerical representation. The number of tiles can be easily calculated:\n\\[\nN\\approx\\dfrac{w\\,l}{a^2}=133.2756\\;\\text{m}^2\\,\\text{m}^{-2}\n\\]\nTo comply with the rule for multiplication/division between uncertain numbers, \\(133.2756\\) has to be rounded using three significant figures, yielding the integer \\(N=133\\), which is then the number of tiles expected to cover the floor.\n\n\nExample 6 (Conversion of scale) A measurement of temperature is performed, leading to the following report:\n\\[\nT_c=(54.0\\pm0.5)\\;^{\\circ}\\text{C}\n\\]\nWe want to convert this expression in units of kelvin:\n\\[\nT_K=T_C+273.15=(327.15\\pm0.5)\\;\\text{K}\n\\]\nThe uncertainty expressed in degree Celsius (\\(\\pm0.5\\;^{\\circ}\\text{C}\\)) translates directly in the uncertainty expressed in kelvin (\\(\\pm 0.5\\;\\text{K}\\)). This is because transforming a measurement expressed in degree Celsius into a measurement expressed in kelvin implies a change of offset, but not a change of scale. Since the uncertainty loads the first decimal digit of the numerical representation of the measured temperature, we should report \\(T_K\\) with one decimal digit, which requires rounding (based on the five rule):\n\\[\nT_k=(327.2\\pm0.5)\\;\\text{K}\n\\]\nThe prescription of the minimum number of significant figures (rule for the addition/subtraction of uncertain numbers) would yield \\(T_k=327\\;\\text{K}\\). This is because \\(54.0\\) has three significant figures, and \\(273.15\\) can be considered to have an infinite number of significant figures, since it is a known constant; hence \\(T_K\\) should be reported with three significant figures according to the rule for addition/subtraction of uncertain numbers. However, this rule is superseded by considering the prescription concerning how to express the measurement uncertainty.\n\n\nThe concept of significant figures and their relation with measurement uncertainty has been briefly reviewed. This topic is important because many measured quantities are often reported with more significant figures than necessary, in the face of the loaded uncertainty. Reporting too many digits is confusing for the reader and of no relevance as for the information content associated to the measurements."
  },
  {
    "objectID": "posts/parallel_processing/index.html",
    "href": "posts/parallel_processing/index.html",
    "title": "Unleashing the power of Apple Silicon for R: Parallel processing on M1/M2",
    "section": "",
    "text": "Parallel computation is a big deal in machine learning (ML), especially when working with large datasets, complex models (like deep neural networks), or intensive tasks like hyperparameter tuning. Parallel computation refers to executing multiple calculations or processes simultaneously.\nIn ML, this helps speed up training and inference by utilizing multiple processing units (CPU cores, GPUs, TPUs, clusters). For my (small) projects in ML, I run everything on a single-CPU machine, an M2 MacBook Air, and write my code in R. Calculations can be excruciatingly slow; however, many tasks are also considered embarrassingly parallel. For example, models created during resampling are independent of each other and can be fit simultaneously without issue. Right now I’m running everything sequentially, but I want to switch to parallel processing to speed things up. In this post, I will review popular methods that are available in R to go parallel with MacBooks equipped with M1 and M2 chips.\nFor computations conducted on a single computer, the number of possible worker processes is determined by the parallel package:\n# The number of physical cores in the hardware:\nparallel::detectCores(logical = FALSE))\n\n# The number of possible independent processes that can \n# be simultaneously used:  \nparallel::detectCores(logical = TRUE)\nMy MacBook Air has:\nWhen training models, especially with resampling (like cross-validation, bootstrapping), I want consistency and high throughput, which is best achieved on the performance cores. So:\nUsing all 8 logical cores (4P + 4E) can lead to inconsistent performance, because efficiency cores are slower and not ideal for heavy computational tasks."
  },
  {
    "objectID": "posts/parallel_processing/index.html#overview",
    "href": "posts/parallel_processing/index.html#overview",
    "title": "Unleashing the power of Apple Silicon for R: Parallel processing on M1/M2",
    "section": "Overview",
    "text": "Overview\nR supports three popular parallel processing methods that work well on M1/M2 MacBooks. Programming hints on how to set up/activate and deactivate/reset each of the three methods are provided in the table below.\n\n\n\n\n\n\n\n\nMethod\nSetup / Activate\nDeactivate / Reset\n\n\n\n\ndoParallel\nlibrary(doParallel)  cl &lt;- makeCluster(4)  registerDoParallel(cl)\nstopCluster(cl)\n\n\ndoMC\nlibrary(doMC)  registerDoMC(cores = 4)\nregisterDoSEQ()\n\n\nfurrr + future\nlibrary(furrr)  library(future)  plan(multisession, workers = 4)\nplan(sequential)\n\n\n\nThe table below summarizes the key pros and cons of each method.\n\n\n\n\n\n\n\n\nMethod\nPros\nCons\n\n\n\n\ndoParallel\n✅ Cross-platform (Mac/Linux/Windows)  ✅ Works with foreach, caret, tidymodels\n❌ Verbose setup  ❌ Full R sessions increase memory\n\n\ndoMC\n✅ Simple & fast on macOS  ✅ Lightweight (uses forked processes)\n❌ Not Windows-compatible  ❌ Not safe for GUI/Shiny apps\n\n\nfurrr + future\n✅ Clean tidyverse integration  ✅ Works with future_map()  ✅ Scales to cloud\n❌ More overhead  ❌ Full R sessions\n\n\n\n\nExample 1 (The Ames housing dataset) The Ames housing dataset is a well-known real estate dataset used for regression modeling, especially as an improved alternative to the Boston Housing dataset. It contains detailed information about residential homes in Ames, Iowa, sold between 2006 and 2010 (here).\nHere’s a clean and minimal setup to load the essential libraries for working with the Ames Housing dataset using a tidymodels regression workflow.\n\n\nCode\n# Data handling and wrangling\nlibrary(tidyverse)\n\n# For the Ames Housing dataset\nlibrary(modeldata)\n\n# Modeling and resampling\nlibrary(tidymodels)\ntidymodels_prefer()\n\n\nThe dataset can be loaded easily in R using the modeldata package.\n\n\nCode\ndata(ames, package = \"modeldata\")\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\n\nSuppose a regression model is to be fit to the pre-logged sale prices (Sale_Price). In this post, we will focus on a small subset of the predictors available in the Ames housing dataset:\n\nThe neighborhood-(qualitative): physical locations within Ames city limits\nThe gross above-grade-living area-(continuous): the standard measure for determining the amount of space in residential properties)\nThe year built (Year_Built)-(discrete): original construction date\nThe type of building (Bldg_Type)-(nominal): type of dwelling\n\nI set up my regression modeling using tidymodels, including:\n\nData splitting\nRecipe for preprocessing\nModel specification\nWorkflow combining recipe + model\n\nMy workflow fits together with v-fold cross-validation in the tidymodels framework. Using the strata = Sale_Price argument in initial_split() or vfold_cv(), I use stratified sampling, which helps ensure that the distribution of the response variable (Sale_Price) remains balanced across splits.\nThe recipe defines the preprocessing steps applied to the dataset before modeling, via step_*() functions without immediately executing them; it is only a specification of what should be done.\n\n\nCode\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) \names_train &lt;- training(ames_split)\names_test  &lt;- testing(ames_split)\n\nset.seed(1004)\names_folds &lt;- vfold_cv(ames_train, v = 10, strata = Sale_Price) \n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\nI fit a random forest model to the training set using the ranger engine, which uses the ranger package for fast computation. Random forests are very powerful — they can learn complex patterns in the data with high accuracy. One big advantage is that they don’t need much preprocessing, so they’re easy to use. The tradeoff is that they can be slower to train, especially on large datasets.\n\n\nCode\nrf_model &lt;- rand_forest(trees = 1000) %&gt;% \n    set_engine(\"ranger\") %&gt;% \n    set_mode(\"regression\")\n\nrf_wflow &lt;- workflow() %&gt;% \n    add_recipe(ames_rec) %&gt;% \n    add_model(rf_model)\n\n\nThe control_resamples() function is used to customize how results are saved during cross-validation. save_pred = TRUE saves the predictions from each fold, so we can analyze or plot them later; save_workflow = TRUE saves the workflow used in each fold, which is helpful if we want to inspect the model or preprocessing steps afterward.\n\n\nCode\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\n\nSequential approach\n\n\nCode\nset.seed(1003) \n\nstart_time &lt;- Sys.time() \n\nrf_res &lt;- rf_wflow %&gt;% \n    fit_resamples(resamples = ames_folds, control = keep_pred) \n\nend_time &lt;- Sys.time() \n\nmetrics &lt;- collect_metrics(rf_res) \nperformance &lt;- list()\nperformance[[1]] &lt;- metrics %&gt;%\n    filter(.metric %in% c(\"rmse\", \"rsq\")) %&gt;%\n               pull(mean)\n\nduration &lt;- c()\nx &lt;- end_time-start_time\nduration[1] &lt;- as.numeric(regmatches(x,regexpr(\"\\\\d*\\\\.?\\\\d+\",x)))\n\n\nApproach using doParallel\n\n\nCode\nlibrary(doParallel) \n\nset.seed(1003) \n\ncl &lt;- makeCluster(4) \nregisterDoParallel(cl) \n\nstart_time &lt;- Sys.time() \nrf_res &lt;- rf_wflow %&gt;% \n    fit_resamples(resamples = ames_folds, control = keep_pred) \nend_time &lt;- Sys.time()\n\nstopCluster(cl)\n\nmetrics &lt;- collect_metrics(rf_res)  \nperformance[[2]] &lt;- metrics %&gt;%\n    filter(.metric %in% c(\"rmse\", \"rsq\")) %&gt;%\n               pull(mean)\n\nx &lt;- end_time - start_time\nduration[2] &lt;- as.numeric(regmatches(x,regexpr(\"\\\\d*\\\\.?\\\\d+\",x)))\n\n\nApproach using doMC\n\n\nCode\nset.seed(1003) \n\nlibrary(doMC) \nregisterDoMC(cores = 4) \n\nstart_time &lt;- Sys.time() \n\nrf_res &lt;- rf_wflow %&gt;% \n    fit_resamples(resamples = ames_folds, control = keep_pred) \n\nend_time &lt;- Sys.time()\n\nregisterDoSEQ()\n\nmetrics &lt;- collect_metrics(rf_res)  \nperformance[[3]] &lt;- metrics %&gt;%\n    filter(.metric %in% c(\"rmse\", \"rsq\")) %&gt;%\n               pull(mean)\n\nx &lt;- end_time - start_time\nduration[3] &lt;- as.numeric(regmatches(x,regexpr(\"\\\\d*\\\\.?\\\\d+\",x)))\n\n\nApproach using furrr + future\n\n\nCode\nlibrary(furrr) \nlibrary(future) \n\nset.seed(1003) \n\nplan(multisession, workers = 4) \n\nstart_time &lt;- Sys.time() \n\nrf_res &lt;- rf_wflow %&gt;% \n    fit_resamples(resamples = ames_folds, control = keep_pred) \n\nend_time &lt;- Sys.time()\n\nplan(sequential)\n\nmetrics &lt;- collect_metrics(rf_res)  \nperformance[[4]] &lt;- metrics %&gt;%\n    filter(.metric %in% c(\"rmse\", \"rsq\")) %&gt;%\n               pull(mean)\n\nx &lt;- end_time - start_time\nduration[4] &lt;- as.numeric(regmatches(x,regexpr(\"\\\\d*\\\\.?\\\\d+\",x)))\n\n\nComparative analysis\nTraining time (in seconds) for each method is shown below.\n\n\n\nTiming results\n \n  \n    Method \n    Time, s \n  \n \n\n  \n    doMC \n    6.06 \n  \n  \n    doParallel \n    7.94 \n  \n  \n    purrr + future \n    10.26 \n  \n  \n    sequential \n    13.34"
  },
  {
    "objectID": "posts/parallel_processing/index.html#recommended-core-use-on-m1m2-macs",
    "href": "posts/parallel_processing/index.html#recommended-core-use-on-m1m2-macs",
    "title": "Unleashing the power of Apple Silicon for R: Parallel processing on M1/M2",
    "section": "Recommended core use on M1/M2 Macs",
    "text": "Recommended core use on M1/M2 Macs\n\nUse 4 cores to target the performance cores only. This choice offers a good balance between performance and system responsiveness."
  },
  {
    "objectID": "posts/parallel_processing/index.html#summary",
    "href": "posts/parallel_processing/index.html#summary",
    "title": "Unleashing the power of Apple Silicon for R: Parallel processing on M1/M2",
    "section": "Summary",
    "text": "Summary\n\nUse doMC for lightweight, fast, Mac-only workflows.\nUse doParallel if you need Windows compatibility or more control.\nUse furrr + future for a modern, tidyverse-friendly setup and future scalability.\n\nParallel icons created by juicy_fish - Flaticon"
  },
  {
    "objectID": "posts/dickey_fuller/index.html",
    "href": "posts/dickey_fuller/index.html",
    "title": "Augmented Dickey-Fuller test",
    "section": "",
    "text": "Roughly, for a time series to be stationary three conditions are needed:\n\nit shows mean reversion, namely it fluctuates around a constant long-term mean\nit has finite variance that is time-invariant.\nautocorrelations decay relatively fast as lag lenghts increase.\n\nThe identification of stationary series can be done by checking whether the autocorrelation function (ACF) drops to zero relatively quickly; usually, the ACF of non-stationary data decreases slowly, and the first-lag value is often large and positive. However, this method is necessarily imprecise, leading to ambiguous situations that cannot be easily deciphered, especially when the sample size is small.\n\n\n\n\n\n\nBackshift notation\n\n\n\nThe backward shift operator \\(B\\) is defined as follows:\n\\[\nBY_t=Y_{t-1}\n\\]\nThe operator \\(B\\) operates on the \\(t\\)th sample of a time series, with the effect of shifting the sample back one period (first difference). Recall that two applications of the backward shift operator to \\(Y_t\\) shift the sample back two periods (second difference):\n\\[\nB(BY_t)=B^2Y_t=Y_{t-2}\n\\]\nFor example, in the case of monthly data, if we wish to consider the same month last year, the notation is \\(B^{12}Y_t=Y_{t-12}\\).\nThe backward shift operator is useful to describe the operation of differencing, the technique of election to stabilize the mean value of nonstationary time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\nA first-order difference is defined as follows:\n\\[\nY^{\\prime}_t=Y_t-Y_{t-1}=Y_t-BY_t=(1-B)Y_t\n\\]\nIf a second-order difference is considered, namely the first-order difference of a first-order difference, we can write:\n\\[\n\\begin{split}\nY^{\\prime\\prime}_t&=Y^{\\prime}_t-Y^{\\prime}_{t-1}\\\\\n&=(1-B)^2Y_t=Y_t-2BY_t+B^2Y_t\\\\\n&=Y_t-2Y_{t-1}+Y_{t-2}\n\\end{split}\n\\]\nA \\(d\\)-order difference can be written \\((1-B)^d\\). It is important to note that a second-order difference, denoted by \\((1-B)^2\\), is not the same as a second difference, which is denoted by \\(B^2\\).\nFor example, a seasonal difference followed by a first difference can be written:\n\\[\n\\begin{split}\nY^{\\prime}_t&=(1-B)(1-B^{12})Y_t\\\\\n&=(1-B-B^{12}-B^{13})Y_t\\\\\n&=Y_t-Y_{t-1}-Y_{t-12}-Y_{t-13}\n\\end{split}\n\\]\nSometimes, as I will always do in the following of this post, the notation \\(\\Delta\\) is also used to indicate the first-order difference, i.e., \\(\\Delta Y_t=(1-B)Y_t\\).\n\n\nA unit root process, also called difference-stationary process (DSP), is a data-generating process whose first difference is stationary.\nThere are two basic models for time series with linear growth characteristics:\n\nTrend stationary process\n\n\\[\nY_t=c+\\delta t+\\text{stationary process}\n\\]\n\nUnit root process\n\n\\[\nY_t=Y_{t-1}+\\text{stationary process}\n\\]\nThe processes are indistinguishable for short data records, in the sense that both a trend stationary process (TSP) and a DSP can fit short data records extremely well. However, the processes can be distinguished when restricted to a particular subclass of data-generating processes, such as AR(\\(p\\)) processes.\nConsider the case of an AR(1) process:\n\\[\nY_t=a_1Y_{t-1}+\\epsilon_t\n\\]\nWe may be interested in testing the hypothesis \\(a_1=1\\). Since under the null hypothesis the sequence \\(\\{Y_t\\}\\) is generated by a nonstationary process, and the variance becomes infinitely large as \\(t\\) increases, classical statistical methods cannot be used.\nDickey and Fuller devised a procedure to formally test for the presence of a unit root against a number of possible alternatives for explaining the data. It is important to recall that TSP and DSP can produce different forecasts and can give rise to spurious regressions, therefore their detection in time series is a truly important task."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#stationary-time-series",
    "href": "posts/dickey_fuller/index.html#stationary-time-series",
    "title": "Augmented Dickey-Fuller test",
    "section": "",
    "text": "Roughly, for a time series to be stationary three conditions are needed:\n\nit shows mean reversion, namely it fluctuates around a constant long-term mean\nit has finite variance that is time-invariant.\nautocorrelations decay relatively fast as lag lenghts increase.\n\nThe identification of stationary series can be done by checking whether the autocorrelation function (ACF) drops to zero relatively quickly; usually, the ACF of non-stationary data decreases slowly, and the first-lag value is often large and positive. However, this method is necessarily imprecise, leading to ambiguous situations that cannot be easily deciphered, especially when the sample size is small.\n\n\n\n\n\n\nBackshift notation\n\n\n\nThe backward shift operator \\(B\\) is defined as follows:\n\\[\nBY_t=Y_{t-1}\n\\]\nThe operator \\(B\\) operates on the \\(t\\)th sample of a time series, with the effect of shifting the sample back one period (first difference). Recall that two applications of the backward shift operator to \\(Y_t\\) shift the sample back two periods (second difference):\n\\[\nB(BY_t)=B^2Y_t=Y_{t-2}\n\\]\nFor example, in the case of monthly data, if we wish to consider the same month last year, the notation is \\(B^{12}Y_t=Y_{t-12}\\).\nThe backward shift operator is useful to describe the operation of differencing, the technique of election to stabilize the mean value of nonstationary time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\nA first-order difference is defined as follows:\n\\[\nY^{\\prime}_t=Y_t-Y_{t-1}=Y_t-BY_t=(1-B)Y_t\n\\]\nIf a second-order difference is considered, namely the first-order difference of a first-order difference, we can write:\n\\[\n\\begin{split}\nY^{\\prime\\prime}_t&=Y^{\\prime}_t-Y^{\\prime}_{t-1}\\\\\n&=(1-B)^2Y_t=Y_t-2BY_t+B^2Y_t\\\\\n&=Y_t-2Y_{t-1}+Y_{t-2}\n\\end{split}\n\\]\nA \\(d\\)-order difference can be written \\((1-B)^d\\). It is important to note that a second-order difference, denoted by \\((1-B)^2\\), is not the same as a second difference, which is denoted by \\(B^2\\).\nFor example, a seasonal difference followed by a first difference can be written:\n\\[\n\\begin{split}\nY^{\\prime}_t&=(1-B)(1-B^{12})Y_t\\\\\n&=(1-B-B^{12}-B^{13})Y_t\\\\\n&=Y_t-Y_{t-1}-Y_{t-12}-Y_{t-13}\n\\end{split}\n\\]\nSometimes, as I will always do in the following of this post, the notation \\(\\Delta\\) is also used to indicate the first-order difference, i.e., \\(\\Delta Y_t=(1-B)Y_t\\).\n\n\nA unit root process, also called difference-stationary process (DSP), is a data-generating process whose first difference is stationary.\nThere are two basic models for time series with linear growth characteristics:\n\nTrend stationary process\n\n\\[\nY_t=c+\\delta t+\\text{stationary process}\n\\]\n\nUnit root process\n\n\\[\nY_t=Y_{t-1}+\\text{stationary process}\n\\]\nThe processes are indistinguishable for short data records, in the sense that both a trend stationary process (TSP) and a DSP can fit short data records extremely well. However, the processes can be distinguished when restricted to a particular subclass of data-generating processes, such as AR(\\(p\\)) processes.\nConsider the case of an AR(1) process:\n\\[\nY_t=a_1Y_{t-1}+\\epsilon_t\n\\]\nWe may be interested in testing the hypothesis \\(a_1=1\\). Since under the null hypothesis the sequence \\(\\{Y_t\\}\\) is generated by a nonstationary process, and the variance becomes infinitely large as \\(t\\) increases, classical statistical methods cannot be used.\nDickey and Fuller devised a procedure to formally test for the presence of a unit root against a number of possible alternatives for explaining the data. It is important to recall that TSP and DSP can produce different forecasts and can give rise to spurious regressions, therefore their detection in time series is a truly important task."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#augmented-dickey-fuller-test",
    "href": "posts/dickey_fuller/index.html#augmented-dickey-fuller-test",
    "title": "Augmented Dickey-Fuller test",
    "section": "Augmented Dickey-Fuller test",
    "text": "Augmented Dickey-Fuller test\nA distinction between stationary and nonstationary time series is made by formal statistical procedures such as the ADF (Augmented Dickey-Fuller) test, which is frequently used since it also accounts for serial correlation in the time series.\nThree specifications of the ADF test are based on the following regression equations.\n\nType 1 (unit root with “none”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{1}\\]\nwhere \\(\\epsilon_t\\) is white Gaussian noise, stationary with zero mean and constant variance. The statistical hypothesis test is:\n\\[\n(\\tau_1)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\tag{2}\\]\nThe null hypothesis prescribes the existence of a unit-root in the model. Rejection of the null implies that the original time series does not have a unit root.\n\n\nType 2 (unit root with “drift”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{a_0}_{\\text{drift}}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{3}\\]\nThe statistical hypothesis tests are:\n\\[\n\\begin{split}\n&(\\phi_1)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_0=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_0\\neq0\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\tau_2)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\end{split}\n\\tag{4}\\]\n\n\nType 3 (unit root with “drift and trend”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{a_0}_{\\text{drift}}+\\underbrace{a_2t}_{\\text{trend}}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{5}\\]\nThe statistical hypothesis tests are:\n\\[\n\\begin{split}\n&(\\phi_2)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_0=0\\;\\text{and}\\;a_2=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_0\\neq0\\;\\text{  or}\\;\\;\\,a_2\\neq0\\\\\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\phi_3)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_2=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_2\\neq0\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\tau_3)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\end{split}\n\\tag{6}\\]\nEach of the six tests \\((\\tau_1),(\\tau_2),(\\tau_3),(\\phi_1),(\\phi_2),(\\phi_3)\\) in Equation 2, Equation 4 and Equation 6 corresponds to a progressively more complex linear regression. In all of them there is the root, but in the “drift” model there is also a drift term, and in the “drift and trend” model there are also drift and trend terms. All the coefficients in the models have an associated significance level. While the significance of the root coefficient is the most important and the main focus of the ADF test, we might also be interested in knowing whether or not the drift and trend coefficients are statistically significant.\n\n\n\n\n\n\nSummary of the Dickey-Fuller tests\n\n\n\n\\[\n\\begin{split}\n&\\begin{split}\n&\\textbf{Trend}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+a_0+a_2t+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\phi_2)\\;\\text{is rejected, unit root is NOT present OR there is trend OR there is drift}\\\\\n&\\text{if}\\;(\\phi_2)\\;\\text{fails to be rejected, unit root is present AND there is NO trend AND there is NO drift}\\\\\n&\\text{if}\\;(\\phi_3)\\;\\text{is rejected, unit root is NOT present OR there is trend}\\\\\n&\\text{if}\\;(\\phi_3)\\;\\text{fails to be rejected, unit root is present AND there is NO trend}\\\\\n&\\text{if}\\;(\\tau_3)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_3)\\;\\text{fails to be rejected, unit root is present}\\\\\n&\\end{split}\n&\\end{split}\\\\\n&\\begin{split}\n&\\textbf{Drift}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+a_0+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\phi_1)\\;\\text{is rejected, unit root is NOT present OR there is drift}\\\\\n&\\text{if}\\;(\\phi_1)\\;\\text{fails to be rejected, unit root is present AND there is NO drift}\\\\\n&\\text{if}\\;(\\tau_2)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_2)\\;\\text{fails to be rejected, unit root is present}\\\\\n&\\end{split}\n&\\end{split}\\\\\n&\\begin{split}\n&\\textbf{None}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\tau_1)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_1)\\;\\text{fails to be rejected, unit root is present}\n&\\end{split}\n&\\end{split}\n\\end{split}\n\\]\n\n\nAn important extension of the ADF test concerns the case when the noise error term is not white. If the error term is not white and we run the ADF test as it is without accounting for serial correlation, many more rejection of the null tend to be produced than stated at the specified significance level. As the ADF test also deal with the serial correlation by introducing lagged terms, hence we need to select this lag order. This is accomplished by investigating several information criteria, including the autocorrelation function (ACF), but henceforth I limit to use the automatic lag selection functionality provided by the same function of R I will use for running the ADF test. Recall that, in general, the test statistics of the ADF test are very sensitive to changes in the lag structure."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#example",
    "href": "posts/dickey_fuller/index.html#example",
    "title": "Augmented Dickey-Fuller test",
    "section": "Example",
    "text": "Example\nI use functions and data from the urca package. The data set contains the series that S. Johansen and K. Juselius considered for estimating the money demand function of Denmark (Johansen and Juselius 1990). A data frame with quarterly data from Denmark starting in 1974:Q1 until 1987:Q3 contains six variables, including the log real income LRY, whose evolution, together with that of its first difference, is shown in Figure 1.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(urca)\nlibrary(zoo)\n\ndata(denmark)\nattach(denmark)\ndenmark &lt;- as.data.frame(denmark)\na &lt;- as.character(ENTRY)\nk &lt;- length(a)\nfor (i in 1:k) substring(a[i], first = 5, last = 5) = \"-\"\nENTRY &lt;- as.yearqtr(as.factor(a))\n\ndf_ts  &lt;- data.frame(x = ENTRY, y = LRY)\ndf_dts &lt;- data.frame(x = ENTRY[1:k-1], y = diff(LRY))\n\nmy_theme = theme(\n  axis.title.x = element_text(size = 16),\n  axis.text.x = element_text(size = 14),\n  axis.title.y = element_text(size = 16),\n  axis.text.y = element_text(size = 14),\n  legend.title = element_text(size = 14),\n  legend.text = element_text(size = 14),\n  panel.border = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  panel.background = element_blank(),\n  axis.line = element_line(colour = \"black\"))\n\np_ts &lt;- ggplot(df_ts, aes(x, y)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"\",\n       y = \"LRY\") + \n  my_theme\n\np_dts &lt;- ggplot(df_dts, aes(x, y)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"\",\n       y = \"first difference, LRY\") + \n  my_theme\n\ngrid.newpage()\ngrid.draw(rbind(ggplotGrob(p_ts), ggplotGrob(p_dts), size = \"last\"))\n\n\n\n\n\n\n\n\nFigure 1: Time series to be tested for stationarity. On the left, the natural logarithm of real income vs. time; on the right its first difference.\n\n\n\n\n\nThe function ur.df() from the urca package performs the ADF test, with three types of models and related tests, named “none” (i.e., Equation 12), “drift” (i.e., Equation 34) and “trend” (Equation 56). The argument selectlags in ur.df() allows to perform automatic selection of the lag structure according to a predefined criterion, as shown in the following code block.\n\n\nCode\nmdl_none_ts   &lt;- ur.df(y = df_ts$y, type = \"none\", selectlags = c(\"BIC\"))\nmdl_drift_ts  &lt;- ur.df(y = df_ts$y, type = \"drift\", selectlags = c(\"BIC\"))\nmdl_trend_ts  &lt;- ur.df(y = df_ts$y, type = \"trend\", selectlags = c(\"BIC\"))\nmdl_none_dts  &lt;- ur.df(y = df_dts$y, type = \"none\", selectlags = c(\"BIC\"))\nmdl_drift_dts &lt;- ur.df(y = df_dts$y, type = \"drift\", selectlags = c(\"BIC\"))\nmdl_trend_dts &lt;- ur.df(y = df_dts$y, type = \"trend\", selectlags = c(\"BIC\"))\nsummary(mdl_trend_ts)\n\n\nThe summary produced when the ADF test is applied to the original time series with the type “trend” is shown in Figure 2.\n\n\n\n\n\n\nFigure 2: Results of fitting the “trend” regression model in the ADF test when applied to the `LRY`variable from the `denmark` dataset.\n\n\n\nThe part of interest for the analysis is within the rectangle highlighted in orange. The “Value of test-statistic is -2.4216 2.1927 2.9343” for tests \\((\\tau_3), (\\phi_2)\\) and \\((\\phi_3)\\) are given and the corresponding “Critical values for test statistics” at significance levels 1%, 5% and 10% are reported below, denoted by tau3, phi2 and phi3, respectively. For instance, for the test \\((\\tau_3)\\), given that the test statistic -2.4216 is within the three regions -4.04, -3.45, -3.15 (1%, 5%, 10%) where we fail to reject the null, we do not have evidence to reject the presence of a unit root in the regression model: we can say that there is a unit root. From the \\((\\phi_2)\\)-statistic, the joint null hypothesis is not rejected, therefore there is a unit root AND drift and trend terms are not needed. Similarly, the \\((\\phi_3)\\)-statistic shows that there is a unit root AND a trend term is not needed and the \\((\\tau_3)\\)-statistic shows that there is a unit root. A summary of test results for the variable LRY is reported within the rectangle highlighted in blue in Figure 3.\n\n\n\n\n\n\nFigure 3: Results of the three type of ADF tests for the variable `LRY`from the `denmark`dataset.\n\n\n\nA close examination of Figure 3 shows that all the tests applied to data of the variable LRY are consistent with failing to reject the corresponding nulls. From the \\((\\phi_1)\\)-statistic, the joint null hypothesis is not rejected, therefore there is a unit root AND the drift term is not needed, and the \\((\\tau_2)\\)-statistic shows that there is a unit root. Finally, the \\((\\tau_1)\\)-statistic also shows that there is a unit root.\nOn the other hand, all the tests applied to the first difference of the variable LRY reject the corresponding nulls, as shown in the same Figure 3 (summary within the rectangle highlighted in red): although drift and trend terms might be presumed, this variable does not therefore contain a unit root.\n\nFinally, we can conclude that the logarithm of real income contains a unit root and can be a stationary time series by taking the first difference."
  },
  {
    "objectID": "posts/lstm/index.html",
    "href": "posts/lstm/index.html",
    "title": "Using Long Short Term Memory (LSTM) in R for time series forecasting",
    "section": "",
    "text": "Long Short Term Memory (LSTM) networks are special kind of Recurrent Neural Networks (RNNs). LSTM networks can be used for time series forecasting, because of their ability to capture and learn the long-term dependencies, such as trends or patterns persisting over time, that are frequently exhibited by time series. In particular, LSTM networks can retain such information on long term behavior of time series much better than ordinary RNNs can do.\nFigure 1 shows the cell of an ordinary RNN, \\(A\\), which outputs a value \\(h_t\\) in response to the input \\(x_t\\).\nSince the cell has a loop into it, information is allowed to persist through multiple steps of the network. An RNN can be thought as multiple copies of this cell, each copy passing a message to the next one, as shown in Figure 2. It is exactly this ability to retain information over multiple steps of the network that provides an RNN with the ability, in principle, to learn long-term dependencies of the input sequence.\nLSTM are, in this regard, just a special kind of RNN, with much better capabilities to learn the long-term dependencies, compared with an ordinary RNN. Figure 3 shows that each repeating cell in an LSTM has four neural network layers (sigmoid \\(\\sigma\\) and hyperbolic tangent \\(\\text{tanh}\\)). In this figure each line carries a vector, from the output of one node to the input of others. Pointwise operations of multiplication and addition coexist with the learned neural network layers.\nThe cell state is the horizontal line at the top of the diagram. The flow of information along the cell state is controlled by the neural network layers, which learn which information to pass and which information to forget through a number of gates:\n\\[\nf_t=\\sigma\\left[w_f\\cdot(h_{t-1},x_t)+b_f\\right]\n\\]\n\\[\n\\left\\{\n\\begin{align}\ni_t&=\\sigma\\left[w_i\\cdot(h_{t-1},x_t)+b_i\\right]\\\\\n\\tilde{c}_t&=\\tanh\\left[w_c\\cdot(h_{t-1},x_t)+b_c\\right]\n\\end{align}\n\\right.\n\\]\n\\[\nc_t=f_t\\cdot c_{t-1}+i_t\\cdot \\tilde{c}_t\n\\]\n\\[\n\\left\\{\n\\begin{align}\no_t&=\\sigma\\left[w_o\\cdot(h_{t-1},x_t)+b_o\\right]\\\\\nh_t&=o_t\\cdot \\tanh({c_t})\n\\end{align}\n\\right.\n\\]\nIt is worth noting that the structure of the LSTM cell indicated in Figure 3 is just one of many variants that have been proposed in the literature."
  },
  {
    "objectID": "posts/lstm/index.html#load-the-libraries-and-the-dataset",
    "href": "posts/lstm/index.html#load-the-libraries-and-the-dataset",
    "title": "Using Long Short Term Memory (LSTM) in R for time series forecasting",
    "section": "Load the libraries and the dataset",
    "text": "Load the libraries and the dataset\nFor demonstration purposes, the historical end-of-day stock price data \\(p_t\\) for Apple Inc. are considered (period: 01/01/2022-06/30/2024). The quantmod package in R can be used to download this data from Yahoo Finance.\n\n\nCode\nlibrary(tidyverse)\nlibrary(quantmod)\nlibrary(keras)\n\ntensorflow::set_random_seed(1234)\n\n# Define the stock symbol for Apple Inc. and specifiy the date range\nstock_symbol &lt;- \"AAPL\"\nstart_date   &lt;- as.Date(\"2021-12-31\")\nend_date     &lt;- as.Date(\"2024-06-30\")\n# Download data from Yahoo Finance using quantmod\ngetSymbols(stock_symbol, src = \"yahoo\", \n           from = start_date, to = end_date) -&gt; a\n# Set data as dataframe\napple_data &lt;- data.frame(date = as.Date(index(AAPL)), \n                         close = as.numeric(Cl(AAPL)))\n# Compute log-returns from end-of-day stock price, adjusting for date\nlog_return &lt;- diff(log(apple_data$close))\ndate &lt;- apple_data %&gt;%\n    pull(date)\ndate &lt;- date[-1]\napple_data &lt;- data.frame(date = date, log_return = log_return)"
  },
  {
    "objectID": "posts/lstm/index.html#data-preparation",
    "href": "posts/lstm/index.html#data-preparation",
    "title": "Using Long Short Term Memory (LSTM) in R for time series forecasting",
    "section": "Data preparation",
    "text": "Data preparation\n\nTransform data to stationary\nUsually, this is done by getting the difference between two consecutive values in the series. This transformation, commonly known as differencing, is considered in traditional time series models, such as ARIMA and GARCH. For a short discussion about non-stationarity in time series and statistical tests that can be used to detect whether a time series is stationary or not, see my previous post (here). There are different approaches in the literature regarding whether LSTM networks can be applied to stationary or non-stationary time series (here). For the example discussed in this post, I decided to analyze the log-returns \\(z_t\\) of the end-of-day stock prices:\n\\[\nz_t=\\log\\left(\\dfrac{p_t}{p_{t-1}}\\right)=\\log(p_t)-\\log(p_{t-1})\n\\]\n\n\n\n\n\n\n\n\nFigure 4: Log-returns of end-of-day stock price.\n\n\n\n\n\nThe time series of log-returns plotted in Figure 4 is to be regarded as being stationary.\n\n\nData normalization\nFor better learning, LSTM needs normalized data, i.e., data that are centered by the mean and scaled by the standard deviation. This step can be implemented using the scale() function of base R; the function get_scaling_factors() stores the scaling factors used for data normalization. When the original data scale needs to be restored at the time of making predictions, the function reverse_data() can be used.\n\n\nCode\nget_scaling_factors &lt;- function(data) {\n  out &lt;- c(mean = mean(data), sd = sd(data))\n  return(out)\n}\n\nreverse_data &lt;- function(data, scaling_factors) {\n  temp &lt;- (data*scaling_factors[2]) + scaling_factors[1]\n  out &lt;- as.matrix(temp)\n  return(out)\n}\n\n# Data processing using differencing with diff() and normalization with scale() \nscaling_factors &lt;- get_scaling_factors(apple_data$log_return)\nlog_return      &lt;- scale(apple_data$log_return)\nlog_return      &lt;- data.frame(log_return)\n\n\n\n\nSplit dataset in training and testing sets\nLSTM works in a supervised learning mode, with a predictor \\(\\textbf{X}\\) and a target variable \\(Y\\). The time series needs therefore to be transformed by lagging the series, so as to have the values at times \\((t−k),k=1,\\cdots,L\\) as the input and the value at time \\(t\\) as the ouput, for a \\(L\\)-step lagged dataset: \\(\\textbf{X}_t=[x_{t-L},x_{t-L+1},\\cdots,x_{t-1}]\\) and \\(Y_t=x_t\\). Here, I choose \\(L=7\\). Moreover, unlike in most analysis where training and testing data sets are randomly sampled, with time series data the order of the observations does matter. The following code splits the first 80% of the series as training set and the remaining 20% as testing set.\n\n\nCode\n# Define the train and test split\nsplit_data &lt;- function(series, lag) {\n    lag &lt;- lag+1\n    data_raw &lt;- as.matrix(series) # convert to matrix (series is a data frame)\n    data &lt;- array(dim = c(0, lag, ncol(data_raw)))\n    # Create all possible sequences of length lag\n    for (i in 1:(nrow(data_raw)-lag)) {\n        data &lt;- rbind(data, data_raw[i:(i+lag-1), ])\n    }\n    train_set_size &lt;- round(0.8*nrow(data))\n    test_set_size  &lt;- nrow(data)-train_set_size\n    x_train &lt;- data[1:train_set_size, 1:(lag-1)]\n    y_train &lt;- data[1:train_set_size, lag]\n    x_test &lt;- data[(train_set_size+1):nrow(data), 1:(lag-1)]\n    y_test &lt;- data[(train_set_size+1):nrow(data), lag]\n    return(list(x_train = x_train, y_train = y_train,\n                x_test = x_test, y_test = y_test))\n}\n\n# Divide data into train and test\nlag &lt;- 7 # Choose sequence length\nsplit_data &lt;- split_data(log_return, lag)\nx_train &lt;- split_data$x_train\ny_train &lt;- split_data$y_train\nx_test &lt;- split_data$x_test\ny_test &lt;- split_data$y_test\nn_train &lt;- dim(x_train)[1]\nn_test &lt;- dim(x_test)[1]"
  },
  {
    "objectID": "posts/lstm/index.html#model-construction",
    "href": "posts/lstm/index.html#model-construction",
    "title": "Using Long Short Term Memory (LSTM) in R for time series forecasting",
    "section": "Model construction",
    "text": "Model construction\nIt is important to consider that the input to every LSTM layer must be three-dimensional. The three dimensions of the input are:\n\nSamples This dimension indicates how many samples are included in each batch; in this example, we have 494 samples for the training data and 123 for the testing data.\nTime Steps This dimension indicates how many points of observation are included in each sample; in this example, we have 7 points.\nFeatures This dimension indicates how many features are in each observation at each time step; for a univariate case, like in this example, we have just one feature.\n\n\n\nCode\ninput_dim &lt;- 1\n\n# Reshape the training and test data to have a 3D array\nx_train &lt;- array_reshape(x_train, c(n_train, lag, input_dim))\nx_test  &lt;- array_reshape(x_test, c(n_test, lag, input_dim))\n\n\n\nDefine the model\nA sequential model is defined using keras_model_sequential():\n\n\nCode\n# Decide structure\nhidden_dim &lt;- 32\nnum_layers &lt;- 2\noutput_dim &lt;- 1\n\n# Define the LSTM model using Keras\nmodel &lt;- keras_model_sequential() %&gt;%\n  layer_lstm(units = hidden_dim, input_shape = c(lag, input_dim), \n             return_sequences = TRUE) %&gt;%\n  layer_lstm(units = hidden_dim) %&gt;%\n  layer_dense(units = output_dim) \nmodel\n\n\nModel: \"sequential\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n lstm_1 (LSTM)                      (None, 7, 32)                   4352        \n lstm (LSTM)                        (None, 32)                      8320        \n dense (Dense)                      (None, 1)                       33          \n================================================================================\nTotal params: 12705 (49.63 KB)\nTrainable params: 12705 (49.63 KB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\n\nThe LSTM input layer is defined by the input_shape argument on the first hidden layer, which takes a tuple of two values that define the number of time steps and features.\n\n\nCompile and fit the model\nThe model is compiled in-place using compile(), which specifies the loss function (mean squared error) and the optimizer (Adam), with learning rate assigned over each update. The model is then fit using the training data and validated on the testing data. The training history is stored in the history object.\n\n\nCode\nnum_epochs &lt;- 125\nbatch      &lt;- 16\n\nmodel %&gt;%\n  compile(loss = \"mean_squared_error\", \n          optimizer = optimizer_adam(learning_rate = 0.01))\n\n# Train the model on the training data\nhistory &lt;- model %&gt;% fit(x_train, y_train, \n                         epochs = num_epochs, \n                         batch_size = batch, \n                         validation_data = list(x_test, y_test),\n                         verbose = FALSE,\n                         shuffle = FALSE)\n\n\n\n\n\n\n\n\n\n\nFigure 5: Loss over the number of epochs.\n\n\n\n\n\nThe loss over the training epochs is shown in Figure 5. The validation mean squared error is 1.2078.\n\n\nExtract predictions\nFigure 6 shows the results of the forecasting exercise using LSTM.\n\n\nCode\n# Extract predictions from the estimated model\ny_train_pred &lt;- model %&gt;% predict(x_train, verbose = FALSE)\ny_test_pred  &lt;- model %&gt;% predict(x_test, verbose = FALSE)\n\n# Rescale the predictions and original values\ny_train_pred_orig &lt;- reverse_data(y_train_pred, scaling_factors)\ny_train_orig      &lt;- reverse_data(y_train, scaling_factors)\ny_test_pred_orig  &lt;- reverse_data(y_test_pred, scaling_factors)\ny_test_orig       &lt;- reverse_data(y_test, scaling_factors)\n\n# Shift the predicted values to start from where the training data predictions end\ny_test_pred_orig_shifted &lt;- c(rep(NA, n_train), y_test_pred_orig[, 1])\ny_train_pred_orig        &lt;- c(y_train_pred_orig, rep(NA, n_test))\n\n\n\n\n\n\n\n\n\n\nFigure 6: LSTM predictions.\n\n\n\n\n\nForecasting icons created by zero_wing - Flaticon"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "I am an associate professor at the BioRobotics Institute of the Scuola Superiore Sant’Anna, in Pisa, Italy. In my posts, I attempt to explain various concepts that I studied, in my double role of teacher and researcher.\nThere have been two main reasons for me to start this blog: first, learning - since writing is a very effective tool against shallow understanding (mine!); second, I wish to help others who may be struggling with similar problems.\nMy teaching is in the general area of instrumentation & measurement, statistics, signal processing and data analysis. My research focuses on developing algorithms for wearable inertial-sensor-based systems applied to motion analysis and assessment of human performance.\nRecently, I started studying the theory of diffusive processes for the modeling of time series of human motion.\nFor a record of my publications, please visit this site. Feel free to reach out to me via mail.\n\n\nThis blog’s content is under the Creative Commons Attribution-ShareAlike 4.0 International License:\n\nAngelo Maria Sabatini’s blog by Angelo Maria Sabatini is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBased on a work at https://amsabatini.netlify.app"
  },
  {
    "objectID": "index.html#copyrightpermissions",
    "href": "index.html#copyrightpermissions",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "This blog’s content is under the Creative Commons Attribution-ShareAlike 4.0 International License:\n\nAngelo Maria Sabatini’s blog by Angelo Maria Sabatini is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBased on a work at https://amsabatini.netlify.app"
  }
]