[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSignificant figures\n\n\n\n\n\n\nmeasurement\n\n\n\nThe use of calculators and computers leads to lab reports with far too many digits in every number produced. Assessing the correct number of significant figures is essential in reporting either experimental or computed results together with their stated uncertainties.\n\n\n\n\n\nMay 14, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nAugmented Dickey-Fuller test\n\n\n\n\n\n\ntime series\n\n\n\nIn statistics, an augmented Dickey–Fuller test (ADF) tests the null hypothesis of a unit root in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. This post explains how to use the ADF test in R, with an attempt to make the different test statistics clear and easily interpretable.\n\n\n\n\n\nMay 13, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nGambler’s ruin\n\n\n\n\n\n\nprobability\n\n\n\nThe gambler’s ruin problem is often applied to gamblers with finite wealth playing against a bookie or casino assumed to have a much larger amount of wealth available, in principle infinite. It can then be proven that the probability of the gambler’s eventual ruin tends to 1 even in the scenario where the game is fair.\n\n\n\n\n\nMay 5, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate probability regions\n\n\n\n\n\n\nstatistics\n\n\n\nComputing probability regions in the space where sample data are assumed to live relates to the determination of regions that we are confident that the underlying population will occupy with the prescribed value of probability. After reviewing the theory for multivariate normal distributions, I present an example of application from the field of posturographic research.\n\n\n\n\n\nMay 2, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence intervals for proportions\n\n\n\n\n\n\nstatistics\n\n\n\nUsually confidence intervals for the estimation of proportions are based on methods that exploit the normal approximation to the binomial distribution. By simulation, two of these methods (Wilson and Wald) are tested for their ability to provide the stated coverage for small-to-large samples.\n\n\n\n\n\nApr 27, 2024\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\nprobability\n\n\n\nAlthough experimentalists are well familiar with the topic, nonetheless I believe it might be helpful to spend a few minutes for a review of basic formulae of combinatorial analysis.\n\n\n\n\n\nApr 24, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\nprobability\n\n\n\nIn this post, I briefly discuss the random incidence phenomenon, using the classical example of a person arriving at a bus stop at a random time, and waiting for the arrival of the next bus.\n\n\n\n\n\nApr 22, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\nsignal processing\n\n\n\nSpectral leakage and length of data record hamper our ability to resolve spectral lines by DFT/FFT analysis. In this post, I briefly discuss this problem, with examples using sinusoidal mixtures.\n\n\n\n\n\nApr 13, 2024\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Significant figures\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\nAugmented Dickey-Fuller test\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\n\n\n\n\n\nGambler’s ruin\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\n\n\n\n\n\nMultivariate probability regions\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\nConfidence intervals for proportions\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\nPills of combinatorics\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n\nRandom incidence\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\nFrequency resolution of spectral analysis\n\n\n\n\n\n\n\n\nApr 13, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dickey_fuller/index.html",
    "href": "posts/dickey_fuller/index.html",
    "title": "Augmented Dickey-Fuller test",
    "section": "",
    "text": "Roughly, for a time series to be stationary three conditions are needed:\n\nit shows mean reversion, namely it fluctuates around a constant long-term mean\nit has finite variance that is time-invariant.\nautocorrelations decay relatively fast as lag lenghts increase.\n\nThe identification of stationary series can be done by checking whether the autocorrelation function (ACF) drops to zero relatively quickly; usually, the ACF of non-stationary data decreases slowly, and the first-lag value is often large and positive. However, this method is necessarily imprecise, leading to ambiguous situations that cannot be easily deciphered, especially when the sample size is small.\n\n\n\n\n\n\nBackshift notation\n\n\n\nThe backward shift operator \\(B\\) is defined as follows:\n\\[\nBY_t=Y_{t-1}\n\\]\nThe operator \\(B\\) operates on the \\(t\\)th sample of a time series, with the effect of shifting the sample back one period (first difference). Recall that two applications of the backward shift operator to \\(Y_t\\) shift the sample back two periods (second difference):\n\\[\nB(BY_t)=B^2Y_t=Y_{t-2}\n\\]\nFor example, in the case of monthly data, if we wish to consider the same month last year, the notation is \\(B^{12}Y_t=Y_{t-12}\\).\nThe backward shift operator is useful to describe the operation of differencing, the technique of election to stabilize the mean value of nonstationary time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\nA first-order difference is defined as follows:\n\\[\nY^{\\prime}_t=Y_t-Y_{t-1}=Y_t-BY_t=(1-B)Y_t\n\\]\nIf a second-order difference is considered, namely the first-order difference of a first-order difference, we can write:\n\\[\n\\begin{split}\nY^{\\prime\\prime}_t&=Y^{\\prime}_t-Y^{\\prime}_{t-1}\\\\\n&=(1-B)^2Y_t=Y_t-2BY_t+B^2Y_t\\\\\n&=Y_t-2Y_{t-1}+Y_{t-2}\n\\end{split}\n\\]\nA \\(d\\)-order difference can be written \\((1-B)^d\\). It is important to note that a second-order difference, denoted by \\((1-B)^2\\), is not the same as a second difference, which is denoted by \\(B^2\\).\nFor example, a seasonal difference followed by a first difference can be written:\n\\[\n\\begin{split}\nY^{\\prime}_t&=(1-B)(1-B^{12})Y_t\\\\\n&=(1-B-B^{12}-B^{13})Y_t\\\\\n&=Y_t-Y_{t-1}-Y_{t-12}-Y_{t-13}\n\\end{split}\n\\]\nSometimes, as I will always do in the following of this post, the notation \\(\\Delta\\) is also used to indicate the first-order difference, i.e., \\(\\Delta Y_t=(1-B)Y_t\\).\n\n\nA unit root process, also called difference-stationary process (DSP), is a data-generating process whose first difference is stationary.\nThere are two basic models for time series with linear growth characteristics:\n\nTrend stationary process\n\n\\[\nY_t=c+\\delta t+\\text{stationary process}\n\\]\n\nUnit root process\n\n\\[\nY_t=Y_{t-1}+\\text{stationary process}\n\\]\nThe processes are indistinguishable for short data records, in the sense that both a trend stationary process (TSP) and a DSP can fit short data records extremely well. However, the processes can be distinguished when restricted to a particular subclass of data-generating processes, such as AR(\\(p\\)) processes.\nConsider the case of an AR(1) process:\n\\[\nY_t=a_1Y_{t-1}+\\epsilon_t\n\\]\nWe may be interested in testing the hypothesis \\(a_1=1\\). Since under the null hypothesis the sequence \\(\\{Y_t\\}\\) is generated by a nonstationary process, and the variance becomes infinitely large as \\(t\\) increases, classical statistical methods cannot be used.\nDickey and Fuller devised a procedure to formally test for the presence of a unit root against a number of possible alternatives for explaining the data. It is important to recall that TSP and DSP can produce different forecasts and can give rise to spurious regressions, therefore their detection in time series is a truly important task."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#stationary-time-series",
    "href": "posts/dickey_fuller/index.html#stationary-time-series",
    "title": "Augmented Dickey-Fuller test",
    "section": "",
    "text": "Roughly, for a time series to be stationary three conditions are needed:\n\nit shows mean reversion, namely it fluctuates around a constant long-term mean\nit has finite variance that is time-invariant.\nautocorrelations decay relatively fast as lag lenghts increase.\n\nThe identification of stationary series can be done by checking whether the autocorrelation function (ACF) drops to zero relatively quickly; usually, the ACF of non-stationary data decreases slowly, and the first-lag value is often large and positive. However, this method is necessarily imprecise, leading to ambiguous situations that cannot be easily deciphered, especially when the sample size is small.\n\n\n\n\n\n\nBackshift notation\n\n\n\nThe backward shift operator \\(B\\) is defined as follows:\n\\[\nBY_t=Y_{t-1}\n\\]\nThe operator \\(B\\) operates on the \\(t\\)th sample of a time series, with the effect of shifting the sample back one period (first difference). Recall that two applications of the backward shift operator to \\(Y_t\\) shift the sample back two periods (second difference):\n\\[\nB(BY_t)=B^2Y_t=Y_{t-2}\n\\]\nFor example, in the case of monthly data, if we wish to consider the same month last year, the notation is \\(B^{12}Y_t=Y_{t-12}\\).\nThe backward shift operator is useful to describe the operation of differencing, the technique of election to stabilize the mean value of nonstationary time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\nA first-order difference is defined as follows:\n\\[\nY^{\\prime}_t=Y_t-Y_{t-1}=Y_t-BY_t=(1-B)Y_t\n\\]\nIf a second-order difference is considered, namely the first-order difference of a first-order difference, we can write:\n\\[\n\\begin{split}\nY^{\\prime\\prime}_t&=Y^{\\prime}_t-Y^{\\prime}_{t-1}\\\\\n&=(1-B)^2Y_t=Y_t-2BY_t+B^2Y_t\\\\\n&=Y_t-2Y_{t-1}+Y_{t-2}\n\\end{split}\n\\]\nA \\(d\\)-order difference can be written \\((1-B)^d\\). It is important to note that a second-order difference, denoted by \\((1-B)^2\\), is not the same as a second difference, which is denoted by \\(B^2\\).\nFor example, a seasonal difference followed by a first difference can be written:\n\\[\n\\begin{split}\nY^{\\prime}_t&=(1-B)(1-B^{12})Y_t\\\\\n&=(1-B-B^{12}-B^{13})Y_t\\\\\n&=Y_t-Y_{t-1}-Y_{t-12}-Y_{t-13}\n\\end{split}\n\\]\nSometimes, as I will always do in the following of this post, the notation \\(\\Delta\\) is also used to indicate the first-order difference, i.e., \\(\\Delta Y_t=(1-B)Y_t\\).\n\n\nA unit root process, also called difference-stationary process (DSP), is a data-generating process whose first difference is stationary.\nThere are two basic models for time series with linear growth characteristics:\n\nTrend stationary process\n\n\\[\nY_t=c+\\delta t+\\text{stationary process}\n\\]\n\nUnit root process\n\n\\[\nY_t=Y_{t-1}+\\text{stationary process}\n\\]\nThe processes are indistinguishable for short data records, in the sense that both a trend stationary process (TSP) and a DSP can fit short data records extremely well. However, the processes can be distinguished when restricted to a particular subclass of data-generating processes, such as AR(\\(p\\)) processes.\nConsider the case of an AR(1) process:\n\\[\nY_t=a_1Y_{t-1}+\\epsilon_t\n\\]\nWe may be interested in testing the hypothesis \\(a_1=1\\). Since under the null hypothesis the sequence \\(\\{Y_t\\}\\) is generated by a nonstationary process, and the variance becomes infinitely large as \\(t\\) increases, classical statistical methods cannot be used.\nDickey and Fuller devised a procedure to formally test for the presence of a unit root against a number of possible alternatives for explaining the data. It is important to recall that TSP and DSP can produce different forecasts and can give rise to spurious regressions, therefore their detection in time series is a truly important task."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#augmented-dickey-fuller-test",
    "href": "posts/dickey_fuller/index.html#augmented-dickey-fuller-test",
    "title": "Augmented Dickey-Fuller test",
    "section": "Augmented Dickey-Fuller test",
    "text": "Augmented Dickey-Fuller test\nA distinction between stationary and nonstationary time series is made by formal statistical procedures such as the ADF (Augmented Dickey-Fuller) test, which is frequently used since it also accounts for serial correlation in the time series.\nThree specifications of the ADF test are based on the following regression equations.\n\nType 1 (unit root with “none”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{1}\\]\nwhere \\(\\epsilon_t\\) is white Gaussian noise, stationary with zero mean and constant variance. The statistical hypothesis test is:\n\\[\n(\\tau_1)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\tag{2}\\]\nThe null hypothesis prescribes the existence of a unit-root in the model. Rejection of the null implies that the original time series does not have a unit root.\n\n\nType 2 (unit root with “drift”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{a_0}_{\\text{drift}}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{3}\\]\nThe statistical hypothesis tests are:\n\\[\n\\begin{split}\n&(\\phi_1)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_0=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_0\\neq0\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\tau_2)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\end{split}\n\\tag{4}\\]\n\n\nType 3 (unit root with “drift and trend”)\n\\[\n\\Delta Y_t=\\gamma Y_{t-1}+\\underbrace{a_0}_{\\text{drift}}+\\underbrace{a_2t}_{\\text{trend}}+\\underbrace{\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}}_{\\text{serial correlation}}+\\epsilon_t\n\\tag{5}\\]\nThe statistical hypothesis tests are:\n\\[\n\\begin{split}\n&(\\phi_2)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_0=0\\;\\text{and}\\;a_2=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_0\\neq0\\;\\text{  or}\\;\\;\\,a_2\\neq0\\\\\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\phi_3)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\;\\text{and}\\;a_2=0\\\\\nH_1:&\\quad\\gamma\\neq 0\\;\\text{  or}\\;\\;\\,a_2\\neq0\n\\end{split}\n\\right.\\\\\n&\\\\\n&(\\tau_3)\\rightarrow\\left\\{\n\\begin{split}H_0:&\\quad\\gamma=0\\\\\nH_1:&\\quad\\gamma\\neq 0\n\\end{split}\n\\right.\n\\end{split}\n\\tag{6}\\]\nEach of the six tests \\((\\tau_1),(\\tau_2),(\\tau_3),(\\phi_1),(\\phi_2),(\\phi_3)\\) in Equation 2, Equation 4 and Equation 6 corresponds to a progressively more complex linear regression. In all of them there is the root, but in the “drift” model there is also a drift term, and in the “drift and trend” model there are also drift and trend terms. All the coefficients in the models have an associated significance level. While the significance of the root coefficient is the most important and the main focus of the ADF test, we might also be interested in knowing whether or not the drift and trend coefficients are statistically significant.\n\n\n\n\n\n\nSummary of the Dickey-Fuller tests\n\n\n\n\\[\n\\begin{split}\n&\\begin{split}\n&\\textbf{Trend}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+a_0+a_2t+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\phi_2)\\;\\text{is rejected, unit root is NOT present OR there is trend OR there is drift}\\\\\n&\\text{if}\\;(\\phi_2)\\;\\text{fails to be rejected, unit root is present AND there is NO trend AND there is NO drift}\\\\\n&\\text{if}\\;(\\phi_3)\\;\\text{is rejected, unit root is NOT present OR there is trend}\\\\\n&\\text{if}\\;(\\phi_3)\\;\\text{fails to be rejected, unit root is present AND there is NO trend}\\\\\n&\\text{if}\\;(\\tau_3)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_3)\\;\\text{fails to be rejected, unit root is present}\\\\\n&\\end{split}\n&\\end{split}\\\\\n&\\begin{split}\n&\\textbf{Drift}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+a_0+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\phi_1)\\;\\text{is rejected, unit root is NOT present OR there is drift}\\\\\n&\\text{if}\\;(\\phi_1)\\;\\text{fails to be rejected, unit root is present AND there is NO drift}\\\\\n&\\text{if}\\;(\\tau_2)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_2)\\;\\text{fails to be rejected, unit root is present}\\\\\n&\\end{split}\n&\\end{split}\\\\\n&\\begin{split}\n&\\textbf{None}\\quad\\quad\\Delta Y_t=\\gamma Y_{t-1}+\\sum_{i=2}^p\\beta_i\\Delta Y_{t-i+1}+\\epsilon_t\\\\\n&\\begin{split}\n&\\text{if}\\;(\\tau_1)\\;\\text{is rejected, unit root is NOT present}\\\\\n&\\text{if}\\;(\\tau_1)\\;\\text{fails to be rejected, unit root is present}\n&\\end{split}\n&\\end{split}\n\\end{split}\n\\]\n\n\nAn important extension of the ADF test concerns the case when the noise error term is not white. If the error term is not white and we run the ADF test as it is without accounting for serial correlation, many more rejection of the null tend to be produced than stated at the specified significance level. As the ADF test also deal with the serial correlation by introducing lagged terms, hence we need to select this lag order. This is accomplished by investigating several information criteria, including the autocorrelation function (ACF), but henceforth I limit to use the automatic lag selection functionality provided by the same function of R I will use for running the ADF test. Recall that, in general, the test statistics of the ADF test are very sensitive to changes in the lag structure."
  },
  {
    "objectID": "posts/dickey_fuller/index.html#example",
    "href": "posts/dickey_fuller/index.html#example",
    "title": "Augmented Dickey-Fuller test",
    "section": "Example",
    "text": "Example\nI use functions and data from the urca package. The data set contains the series that S. Johansen and K. Juselius considered for estimating the money demand function of Denmark (Johansen and Juselius 1990). A data frame with quarterly data from Denmark starting in 1974:Q1 until 1987:Q3 contains six variables, including the log real income LRY, whose evolution, together with that of its first difference, is shown in Figure 1.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(urca)\nlibrary(zoo)\n\ndata(denmark)\nattach(denmark)\ndenmark &lt;- as.data.frame(denmark)\na &lt;- as.character(ENTRY)\nk &lt;- length(a)\nfor (i in 1:k) substring(a[i], first = 5, last = 5) = \"-\"\nENTRY &lt;- as.yearqtr(as.factor(a))\n\ndf_ts  &lt;- data.frame(x = ENTRY, y = LRY)\ndf_dts &lt;- data.frame(x = ENTRY[1:k-1], y = diff(LRY))\n\nmy_theme = theme(\n  axis.title.x = element_text(size = 16),\n  axis.text.x = element_text(size = 14),\n  axis.title.y = element_text(size = 16),\n  axis.text.y = element_text(size = 14),\n  legend.title = element_text(size = 14),\n  legend.text = element_text(size = 14),\n  panel.border = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  panel.background = element_blank(),\n  axis.line = element_line(colour = \"black\"))\n\np_ts &lt;- ggplot(df_ts, aes(x, y)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"\",\n       y = \"LRY\") + \n  my_theme\n\np_dts &lt;- ggplot(df_dts, aes(x, y)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"\",\n       y = \"first difference, LRY\") + \n  my_theme\n\ngrid.newpage()\ngrid.draw(rbind(ggplotGrob(p_ts), ggplotGrob(p_dts), size = \"last\"))\n\n\n\n\n\n\n\n\nFigure 1: Time series to be tested for stationarity. On the left, the natural logarithm of real income vs. time; on the right its first difference.\n\n\n\n\n\nThe function ur.df() from the urca package performs the ADF test, with three types of models and related tests, named “none” (i.e., Equation 1-Equation 2), “drift” (i.e., Equation 3-Equation 4) and “trend” (Equation 5-Equation 6). The argument selectlags in ur.df() allows to perform automatic selection of the lag structure according to a predefined criterion, as shown in the following code block.\n\n\nCode\nmdl_none_ts   &lt;- ur.df(y = df_ts$y, type = \"none\", selectlags = c(\"BIC\"))\nmdl_drift_ts  &lt;- ur.df(y = df_ts$y, type = \"drift\", selectlags = c(\"BIC\"))\nmdl_trend_ts  &lt;- ur.df(y = df_ts$y, type = \"trend\", selectlags = c(\"BIC\"))\nmdl_none_dts  &lt;- ur.df(y = df_dts$y, type = \"none\", selectlags = c(\"BIC\"))\nmdl_drift_dts &lt;- ur.df(y = df_dts$y, type = \"drift\", selectlags = c(\"BIC\"))\nmdl_trend_dts &lt;- ur.df(y = df_dts$y, type = \"trend\", selectlags = c(\"BIC\"))\nsummary(mdl_trend_ts)\n\n\nThe summary produced when the ADF test is applied to the original time series with the type “trend” is shown in Figure 2.\n\n\n\n\n\n\nFigure 2: Results of fitting the “trend” regression model in the ADF test when applied to the `LRY`variable from the `denmark` dataset.\n\n\n\nThe part of interest for the analysis is within the rectangle highlighted in orange. The “Value of test-statistic is -2.4216 2.1927 2.9343” for tests \\((\\tau_3), (\\phi_2)\\) and \\((\\phi_3)\\) are given and the corresponding “Critical values for test statistics” at significance levels 1%, 5% and 10% are reported below, denoted by tau3, phi2 and phi3, respectively. For instance, for the test \\((\\tau_3)\\), given that the test statistic -2.4216 is within the three regions -4.04, -3.45, -3.15 (1%, 5%, 10%) where we fail to reject the null, we do not have evidence to reject the presence of a unit root in the regression model: we can say that there is a unit root. From the \\((\\phi_2)\\)-statistic, the joint null hypothesis is not rejected, therefore there is a unit root AND drift and trend terms are not needed. Similarly, the \\((\\phi_3)\\)-statistic shows that there is a unit root AND a trend term is not needed and the \\((\\tau_3)\\)-statistic shows that there is a unit root. A summary of test results for the variable LRY is reported within the rectangle highlighted in blue in Figure 3.\n\n\n\n\n\n\nFigure 3: Results of the three type of ADF tests for the variable `LRY`from the `denmark`dataset.\n\n\n\nA close examination of Figure 3 shows that all the tests applied to data of the variable LRY are consistent with failing to reject the corresponding nulls. From the \\((\\phi_1)\\)-statistic, the joint null hypothesis is not rejected, therefore there is a unit root AND the drift term is not needed, and the \\((\\tau_2)\\)-statistic shows that there is a unit root. Finally, the \\((\\tau_1)\\)-statistic also shows that there is a unit root.\nOn the other hand, all the tests applied to the first difference of the variable LRY reject the corresponding nulls, as shown in the same Figure 3 (summary within the rectangle highlighted in red): although drift and trend terms might be presumed, this variable does not therefore contain a unit root.\n\nFinally, we can conclude that the logarithm of real income contains a unit root and can be a stationary time series by taking the first difference."
  },
  {
    "objectID": "posts/significant_figures/index.html",
    "href": "posts/significant_figures/index.html",
    "title": "Significant figures",
    "section": "",
    "text": "It happens frequently that the number of digits that are available to report measurement results is high. Usually, measurement results are produced by carrying arithmetic operations with computers or calculators, whose level of numerical precision, albeit finite, is too high given the true information gathered by the measurements. In other words, the precision can be excessive, and too many digits can simply swamp the observer, making the message in the measurements more obscure. Significant figures, also referred to as significant digits, are specific digits within a number written in positional notation that carry both reliability and necessity in reporting a measurement result. Proper use of significant figures is thus an essential element in the presentation of both experimental and calculated results together with their associated uncertainty.\n\n\nA number of rules exist for determining how many significant figures are in a number:\n\nnon-zero digits are always significant\n\n\n4.6 has two significant figures\n\n\nleading zeros placed before the first non-zero digit are not significant (they are called placeholders)\n\n\n0.046 has two significant figures\n\n\ntrailing zeros placed after all other digits but behind a decimal point are significant\n\n\n4.60 has three significant figures\n\nThe leftmost digit which is not a zero is referred to as the most significant digit (MSD); the rightmost digit of a decimal number is the least significant digit (LSD), regardless it is a zero or not: 4 and 0 are thus, respectively, the MSD and the LSD of 4.60; 4 and 1 are, respectively, the MSD and the LSD of 4.61. Every digit between the LSD and the MSD, including zeros, should be counted as significant figures, hence 4.60 and 40.60 have, respectively, three and four significant figures.\nAmbiguous situations arise when zeros are at the end of the number and not behind a decimal point as, for example, in the number 4600. Confusion can be avoided if the number is expressed in scientific notation.\n\n\n\n\n\n\nScientific notation\n\n\n\nScientific notation is a way of expressing numbers that are much too large or much too small to be conveniently written in decimal form (i.e., their representation would involve a long string of digits). In scientific notation, nonzero numbers are written in the form:\n\\[\nm\\times10^n\n\\]\nwhere \\(n\\) is an integer, and the coefficient \\(m\\) is a nonzero real number (usually \\(1\\leq\\vert\\,m\\,\\vert&lt;10\\)). The integer \\(n\\) is called the exponent and the real number \\(m\\) is called the mantissa. If the number is negative, then a minus sign precedes \\(m\\), as in ordinary decimal notation.\n\n\nIn scientific notation, the number 4600 can be written using a different number of significant figures, based on rule 3. above:\n\\[\n\\begin{split}\n4.600\\times 10^3&\\quad\\text{four significant figures}\\\\\n4.60\\times 10^3&\\quad\\text{three significant figures}\\\\\n4.6\\times 10^3&\\quad\\text{two significant figures}\n\\end{split}\n\\]\n\n\n\nA number can be rounded so as to drop digits until a prescribed number of significant figures is retained in the final representation. Recall that all the digits after the decimal point to the right of the desired LSD are to be dropped and not replaced with zeros, which otherwise should add to the number of significant figures (rule 3 above). The rules of rounding are the following:\n\nif the digit to the right of the desired LSD is greater than 5, add 1 to the LSD, otherwise do nothing\n\n\nExample - round at the fourth significant figure\n\n\\[\n\\begin{split}\n53.8\\underline{7}4&\\rightarrow53.87\\\\\n53.8\\underline{7}9&\\rightarrow53.88\n\\end{split}\n\\]\n\nif the digit to the right of the LSD is 5, apply a tie-breaking rule, also called the five rule. When the first digit to be dropped is 5, the leading digit next to it is examined. If this digit is even, including zero, it is left unaltered; otherwise, one unit is added. This helps avoiding the accumulation of errors that would be otherwise determined by rounding systematically up or down. Using the five rule, five out of ten cases consist of rounding up and five out of ten cases consist of rounding down.\n\n\nExample - round at the fifth significant figure\n\n\\[\n\\begin{split}\n726.8\\underline{0}51\\rightarrow 726.80\\\\\n726.8\\underline{3}51\\rightarrow 726.84\\\\\n726.8\\underline{6}51\\rightarrow 726.86\\\\\n726.8\\underline{9}51\\rightarrow 726.90\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/significant_figures/index.html#significant-figures",
    "href": "posts/significant_figures/index.html#significant-figures",
    "title": "Significant figures",
    "section": "",
    "text": "It happens frequently that the number of digits that are available to report measurement results is high. Usually, measurement results are produced by carrying arithmetic operations with computers or calculators, whose level of numerical precision, albeit finite, is too high given the true information gathered by the measurements. In other words, the precision can be excessive, and too many digits can simply swamp the observer, making the message in the measurements more obscure. Significant figures, also referred to as significant digits, are specific digits within a number written in positional notation that carry both reliability and necessity in reporting a measurement result. Proper use of significant figures is thus an essential element in the presentation of both experimental and calculated results together with their associated uncertainty.\n\n\nA number of rules exist for determining how many significant figures are in a number:\n\nnon-zero digits are always significant\n\n\n4.6 has two significant figures\n\n\nleading zeros placed before the first non-zero digit are not significant (they are called placeholders)\n\n\n0.046 has two significant figures\n\n\ntrailing zeros placed after all other digits but behind a decimal point are significant\n\n\n4.60 has three significant figures\n\nThe leftmost digit which is not a zero is referred to as the most significant digit (MSD); the rightmost digit of a decimal number is the least significant digit (LSD), regardless it is a zero or not: 4 and 0 are thus, respectively, the MSD and the LSD of 4.60; 4 and 1 are, respectively, the MSD and the LSD of 4.61. Every digit between the LSD and the MSD, including zeros, should be counted as significant figures, hence 4.60 and 40.60 have, respectively, three and four significant figures.\nAmbiguous situations arise when zeros are at the end of the number and not behind a decimal point as, for example, in the number 4600. Confusion can be avoided if the number is expressed in scientific notation.\n\n\n\n\n\n\nScientific notation\n\n\n\nScientific notation is a way of expressing numbers that are much too large or much too small to be conveniently written in decimal form (i.e., their representation would involve a long string of digits). In scientific notation, nonzero numbers are written in the form:\n\\[\nm\\times10^n\n\\]\nwhere \\(n\\) is an integer, and the coefficient \\(m\\) is a nonzero real number (usually \\(1\\leq\\vert\\,m\\,\\vert&lt;10\\)). The integer \\(n\\) is called the exponent and the real number \\(m\\) is called the mantissa. If the number is negative, then a minus sign precedes \\(m\\), as in ordinary decimal notation.\n\n\nIn scientific notation, the number 4600 can be written using a different number of significant figures, based on rule 3. above:\n\\[\n\\begin{split}\n4.600\\times 10^3&\\quad\\text{four significant figures}\\\\\n4.60\\times 10^3&\\quad\\text{three significant figures}\\\\\n4.6\\times 10^3&\\quad\\text{two significant figures}\n\\end{split}\n\\]\n\n\n\nA number can be rounded so as to drop digits until a prescribed number of significant figures is retained in the final representation. Recall that all the digits after the decimal point to the right of the desired LSD are to be dropped and not replaced with zeros, which otherwise should add to the number of significant figures (rule 3 above). The rules of rounding are the following:\n\nif the digit to the right of the desired LSD is greater than 5, add 1 to the LSD, otherwise do nothing\n\n\nExample - round at the fourth significant figure\n\n\\[\n\\begin{split}\n53.8\\underline{7}4&\\rightarrow53.87\\\\\n53.8\\underline{7}9&\\rightarrow53.88\n\\end{split}\n\\]\n\nif the digit to the right of the LSD is 5, apply a tie-breaking rule, also called the five rule. When the first digit to be dropped is 5, the leading digit next to it is examined. If this digit is even, including zero, it is left unaltered; otherwise, one unit is added. This helps avoiding the accumulation of errors that would be otherwise determined by rounding systematically up or down. Using the five rule, five out of ten cases consist of rounding up and five out of ten cases consist of rounding down.\n\n\nExample - round at the fifth significant figure\n\n\\[\n\\begin{split}\n726.8\\underline{0}51\\rightarrow 726.80\\\\\n726.8\\underline{3}51\\rightarrow 726.84\\\\\n726.8\\underline{6}51\\rightarrow 726.86\\\\\n726.8\\underline{9}51\\rightarrow 726.90\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/significant_figures/index.html#finite-precision-arithmetic",
    "href": "posts/significant_figures/index.html#finite-precision-arithmetic",
    "title": "Significant figures",
    "section": "Finite precision arithmetic",
    "text": "Finite precision arithmetic\nIn mathematical operations involving significant figures, the result cannot be more precise than the least precise number. Calculations in finite precision arithmetic can be done following a few simple rules. One rule applies to multiplication and division, and another rule applies to addition and subtraction. Recall that values that are considered exact numbers, e.g., known conversion factors or physical constants, are not to be included in the determination of the number of significant figures.\n\nMultiplication and division\n\nWhen we multiply/divide two numbers, we should add their relative uncertainties. The uncertainty of the result is given roughly by the number of the digits, regardless of their placement.\n\nIn a calculation involving multiplication/division the number of significant figures in the result should equal the least number of significant figures in any one of the numbers being multiplied or divided.\nIn the following example, the number 1.6 is reported with two significant figures; the number 2, seen as a known constant, can be considered having an infinite number of significant figures, whereas the number 2.0 has two significant figures. The result should be reported with two significant figures in both cases:\n\\[\n\\begin{split}\n&1.6\\times2=3.2&\\\\\n&1.61\\times2.0=3.2&\\quad\\text{not}\\;3.22\n\\end{split}\n\\]\n\n\nAddition and subtraction\n\nWhen we add/subtract two numbers, we should add their uncertainties. The uncertainty of the result is given roughly by the placement of the digits, not by the number of digits.\n\nIn a calculation involving addition/subtraction, the number of decimal places in the result should equal the least number of decimal places in any one of the numbers being added or subtracted.\nIn the following example, the number 132.03 is reported with five significant figures, and the number 3.210 is reported with four significant figures. However, when the two numbers are added, what matters really is the number of decimal places, i.e., two for the number 132.03 and three for the number 3.210. The result should be reported with two decimal digits and not reported using four significant figures.\n\\[\n\\begin{split}\n&132.03+3.210=135.24&\\;\\text{and not}\\;135.2\\\\\n&132.03+3=135&\\;\\text{and not}\\;135.03\\\\\n&132.03+3.00=135.03&\\;\\text{and not}\\;135\\\\\n\\end{split}\n\\]\nThe prescription about the minimum number of decimal places of any of the numbers involved in the calculation can be explained by considering that, implicitly, the precision of any measurement is dictated by the decimal place. For a measurement of length expressed in meters, for example, the second decimal digit implies a measurement precise to the hundredths (centimeter-level), the third decimal digit to the thousandth (millimeter-level). So by keeping the result with the minimum number of decimal places we basically state that we do not want to imply to get a result more precise than the least precise measurement that was needed to produce the result itself.\n\n\nMultiple arithmetic operations\n\nIn a calculation involving multiple arithmetic operations, the rules are applied without rounding results after each intermediate step. Instead keep track of the rightmost digit that would be retained. The operations would be performed in the following order:\n\n\noperations in parentheses ( )\nmultiplication\ndivision\naddition\nsubtraction\n\nIt is important to always perform intermediate calculations without rounding the numbers that are involved in the operations. If numbers are rounded every time during many sequential calculations, the results are skewed and some systematic error is surely introduced. Only after that all calculations are carried out with all digits retained at each step, the final result has to be rounded to the desired number of significant figures.\nAs an example, two numbers reported with five significant figures each are added, and the final result is rounded to three significant figures. If the addends are first rounded to three significant figures and then added, the result we produce is wrong:\n\\[\n\\begin{split}\n&1.4248+1.2732=2.6980\\rightarrow 2.70&\\quad\\text{correct}\\\\\n&1.42+1.27=2.69\\rightarrow 2.70&\\quad\\text{wrong}\n\\end{split}\n\\]\n\nExample 1 (Sequential calculation) Suppose that we want to perform the following operation:\n\\[\n(2.5\\times3.42)+13.681-0.1\n\\]\n\nperform first the product between parentheses - we keep track of the first decimal place, which would be retained based on rule B above.\n\n\\[\n2.5\\times3.42=8.\\underline{5}500\n\\]\n\nperform addition - although, based on rule A above, the result would be expressed using five significant figures, only the first decimal place is kept tracked:\n\n\\[\n8.5500+13.681=22.\\underline{2}310\n\\]\n\nperform subtraction:\n\n\\[\n22.2310-0.1=22.\\underline{1}31\n\\]\n\nrounding to three significant figures:\n\n\\[\n(2.5\\times3.42)+13.681-0.1\\rightarrow22.1\n\\]\n\nWhen doing multi-step calculations, we need:\n\nto keep at least one more significant figure in intermediate results than needed in the final answer. Furthermore, never round intermediate answers: rounding, say, to two significant figures in an intermediate answer, and then writing three significant figures in the final answer is wrong.\nnot to write more significant figures in the final result (of a measurement process) than justified (by the measurement uncertainty)."
  },
  {
    "objectID": "posts/significant_figures/index.html#significant-figures-and-measurement-uncertainty",
    "href": "posts/significant_figures/index.html#significant-figures-and-measurement-uncertainty",
    "title": "Significant figures",
    "section": "Significant figures and measurement uncertainty",
    "text": "Significant figures and measurement uncertainty\nThe value of one measurand must be delivered by rounding the digit loaded by the measurement uncertainty \\(U\\), where \\(U\\) is represented by a number with, usually, no more than one or two significant figures (rounded up, possibly). The additional uncertainty due to rounding must be checked for being negligible compared to \\(U\\). Essentially, \\(U\\) gives an estimate of the errors incurred in the measurement.\nFor example, if we have a length \\(L=(12.37\\pm0.10)\\;\\text{cm}\\), we can report the length as \\(L=12.4\\;\\text{cm}\\). When we express a number with three significant figures, what we are saying is that the first two digits are essentially exactly correct, and the last one is uncertain by a small amount (generally it is only uncertain by about \\(\\pm1\\)). In the example above, we rounded our answer to \\(12.4\\;\\text{cm}\\) because our answer is uncertain to \\(\\pm0.1\\;\\text{cm}\\), namely our answer is uncertain in the last digit by about 1.\n\nExample 2 (Rounding) Round the measurement \\(z=12.0349\\;\\text{cm}\\), whose uncertainty is stated being \\(\\Delta z=0.153\\;\\text{cm}\\).\n\nround the uncertainty to two significant figures:\n\n\\[\n\\Delta z=0.15\\,\\text{cm}\n\\]\n\nround \\(z\\) using the same number of decimal places as \\(\\Delta z\\):\n\n\\[\nz=12.03\\,\\text{cm}\n\\]\n\nprovide the measurement report:\n\n\\[\nz\\pm\\Delta z=(12.03\\pm0.15)\\,\\text{cm}\n\\]\n\n\nExample 3 (Use of the scientific notation) When the answer is given in scientific notation, the uncertainty should be given in scientific notation with the same power of ten as the answer. Suppose that \\(z=1.43\\times10^6\\;\\text{s}\\) and \\(\\Delta z=2\\times10^4\\;\\text{s}\\):\n\\[\nz\\pm\\Delta z=(1.43\\pm0.02)\\,10^6\\;\\text{s}\n\\]\n\n\nExample 4 (Addition/subtraction of uncertain numbers) The length of two blocks is measured, and the measurements are \\(l_1=1.13\\;\\text{m}\\) (considered precise to the level of centimeters) and \\(l_2=0.551\\;\\text{m}\\) (considered precise to the level of millimeters). We need to compute the length \\(l\\) of the block resulting from stacking the two blocks together:\n\\[\nl=l_1+l_2=1.681\\;\\text{m}\\rightarrow l=1.68\\;\\text{m}\n\\]\nIt does not make any physical sense to consider the length of the overall block precise to the level of the millimeters, given that one of the two blocks is measured less precisely. The result should be at least as precise as the least precise term involved in the addition, as stated by the rule for addition/subtraction of uncertain numbers.\n\n\nExample 5 (Multiplication/division of uncertain numbers) A rectangular floor needs to be covered by a number of squared tiles. According to the measurements that are available, the rectangular floor has width \\(w=1.91\\;\\text{m}\\) and length \\(l=1.57\\;\\text{m}\\) and each squared tile has size \\(a=0.15\\;\\text{m}\\). All measurements are considered precise to the level of centimeters, and three significant figures should then be considered for their numerical representation. The number of tiles can be easily calculated:\n\\[\nN\\approx\\dfrac{w\\,l}{a^2}=133.2756\\;\\text{m}^2\\,\\text{m}^{-2}\n\\]\nTo comply with the rule for multiplication/division between uncertain number, 133.2756 has to be rounded using three significant figures, yielding the integer \\(N=133\\), which is then the number of tiles expected to cover the floor.\n\n\nExample 6 (Conversion of scale) A measurement of temperature is performed, leading to the following report:\n\\[\nT_c=(54.0\\pm0.5)\\;^{\\circ}\\text{C}\n\\]\nWe want to convert this expression in units of kelvin:\n\\[\nT_K=T_C+273.15=(327.15\\pm0.5)\\;\\text{K}\n\\]\nThe uncertainty expressed in degree Celsius (\\(\\pm0.5\\;^{\\circ}\\text{C}\\)) translates directly in the uncertainty expressed in kelvin (\\(\\pm 0.5\\;\\text{K}\\)). This is because transforming a measurement expressed in degree Celsius into a measurement expressed in kelvin implies a change of offset, but not a change of scale. Since the uncertainty loads the first decimal digit of the numerical representation of the measured temperature, we should report \\(T_K\\) with one decimal digit, which requires rounding (based on the five rule):\n\\[\nT_k=(327.2\\pm0.5)\\;\\text{K}\n\\]\nThe prescription of the minimum number of significant figures (rule for the addition/subtraction of uncertain numbers) would yield \\(T_k=327\\;\\text{K}\\). This is because \\(54.0\\) has three significant figures, and \\(273.15\\) can be considered to have an infinite number of significant figures, since it is a known constant; hence \\(T_K\\) should be reported with three significant figures according to the rule for addition/subtraction of uncertain numbers. However, this rule is superseded by considering the prescription concerning how to express the measurement uncertainty.\n\n\nThe concept of significant figures and their relation with measurement uncertainty has been briefly reviewed. This topic is important because many measured quantities are often reported with more significant figures than necessary, in the face of the loaded uncertainty. Reporting too many digits is confusing for the reader and of no relevance as for the information content associated to the measurements."
  },
  {
    "objectID": "posts/resolution/index.html",
    "href": "posts/resolution/index.html",
    "title": "Frequency resolution of spectral analysis",
    "section": "",
    "text": "Frequency resolution is the size of the smallest frequency for which details in the frequency response and the spectrum can be resolved by the estimate. For example, a resolution of 0.1 Hz means that the frequency response variations at frequency intervals at or below 0.1 Hz cannot be resolved.\nConsider an analog band-limited signal \\(x(t)\\) with bandwidth \\(B\\) Hz; \\(x(t)\\) is observed over a sample period of \\(T_r\\) s (the length of the data record) and sampled at a sampling frequency of \\(f_s\\) Hz (\\(T_s=1/f_s\\) denotes the sampling interval). The total available number of samples of \\(x(t)\\) is \\(N=\\lfloor T_r/T_s\\rfloor\\), where \\(\\lfloor\\cdot\\rfloor\\) denotes the floor function (or greatest integer function), namely the function that takes as input a real number \\(r\\) and gives as output the greatest integer less than or equal to \\(r\\).\nAccording to the Shannon-Nyquist sampling theorem, if the sampling frequency \\(f_s\\) is chosen to be \\(2B\\), the maximum resolvable frequency is:\n\\[\nf_{\\text{max}}=\\frac{f_s}{2}\n\\]\nOn the other hand, the minimum resolvable frequency is inversely related to the sample period:\n\\[\nf_{\\text{min}}=\\frac{1}{T_r}=\\frac{1}{NT_s}\n\\tag{1}\\]\nand hence the number of frequencies that can be resolved from \\(f_{\\text{min}}\\) to \\(f_{\\text{max}}\\) is\n\\[\nN_f=\\frac{f_{\\text{max}}-f_{\\text{min}}}{\\Delta f}\n\\]\nwhere \\(\\Delta f\\) is the frequency resolution. Since \\(\\Delta f=f_{\\text{min}}\\), I also have:\n\\[\nN_f=\\dfrac{\\dfrac{f_s}{2}-\\dfrac{f_s}{N}}{\\dfrac{f_s}{N}}=\\frac{N}{2}-1\n\\]\nThis implies that will be \\(N/2\\) discrete frequencies from \\(0\\) to \\(f_{\\text{max}}\\).\nThe accurate detection of frequency components in the spectrum \\(X(f)\\) of the signal \\(x(t)\\) is challenged by the phenomenon known as spectral leakage, or amplitude ambiguity; it consists of ambiguous and false amplitudes occurring in the spectrum \\(X(f)\\) whenever the sample period \\(T_r\\) is not an integer multiple of all of the contributory periods in \\(x(t)\\). That is, false or ambiguous amplitudes will occur at frequencies that are immediately adjacent to the actual frequency.\nFor aperiodic signals, \\(T_r\\) theoretically must be infinite. For periodic signals, \\(T_r\\) must be equal to the least common integer multiple of all the periods contained in the signal. Application of the DFT (Discrete Fourier Transform) or FFT (Fast Fourier Transform) to an aperiodic signal implicitly assumes that the signal is infinite in length and formed by repeating the signal of length \\(T_r\\) an infinite number of times. This leads to discontinuities in the amplitude that occur at each integer multiple of \\(T_r\\). These discontinuities are step-like, which introduce false amplitudes that decrease around the main frequencies.\nAll the periods contained in the signal cannot be known before the spectral analysis is performed, therefore the stated condition on the sample period cannot be fulfilled; the best method to minimize the effect of spectral leakage is windowing.\nHerein, I just provide a brief explanation of windowing, without testing it in the examples to follow. Windowing reduces the amplitude of the discontinuities at the boundaries of each finite sequence of samples of \\(x(t)\\). It does so by multiplying the acquired sequence by a finite-length window with an amplitude that varies smoothly and gradually toward zero at the edges. This makes the endpoints of the waveform meet and, therefore, results in a continuous waveform without sharp transitions.\n\n\n\n\n\n\nCan discrete-time sinusoids be non-periodic?\n\n\n\nThe normalized frequency \\(f\\) in cycles per sample of a discrete-time sinusoid:\n\\[\nx[n]=\\cos(2\\pi fn)\n\\]\nis restricted to values in the interval \\(-1/2\\leq f\\leq1/2\\). This is because, for any discrete-time sinusoid with frequency \\(\\vert f\\vert&gt;1/2\\), an integer \\(m\\) exists such that \\(f=f_0+m\\) and \\(\\vert f_0\\vert\\leq1/2\\):\n\\[\n\\cos 2\\pi fn=\\cos[2\\pi (f_0+m)\\,n]=\\cos(2\\pi f_0n)+\\cos(2\\pi mn)=\\cos(2\\pi f_0n)\n\\]\nIt is worth noting that the highest rate of oscillation of discrete-time sinusoids is when, at every time instant, the output sample flips polarity with respect to the previous output sample:\n\\[\nf_0=\\frac{1}{2}\\rightarrow\\cos(2\\pi f_0n)=\\cos(\\pi n)=(-1)^n\n\\]\nMoreover, a discrete-time sinusoid can be periodic, i.e, characterized by patterns that exactly repeat themselves in time, if and only if its frequency can be expressed in terms of a rational number; moreover, an additive mixture of periodical discrete-time sinusoids (called here sinusoidal mixture) is periodic with period equal to the least common integer multiple of their periods.\n\n\nAs outlined above, a spectrum line at frequency \\(f\\) will be accurately represented by the DFT when \\(f_s\\geq2f_{\\text{max}}\\) and \\(T_r = mT\\), where \\(m=1,2,\\cdots\\), and \\(T=1/f\\). This implies that \\(N=mf_s/f\\). If \\(T_r\\) is not an integer multiple of \\(T\\), leakage will occur in the DFT. This appears as amplitudes at \\(f\\) spilling onto adjacent frequencies.\nAs an example, consider a sinusoidal mixture with three unit-amplitude components at 2.875 Hz, 3 Hz, 3.125 Hz. The sampling frequency is set to \\(f_s=16\\) Hz and the sample period to \\(T=8\\) s (\\(N=128\\)): the component at 2.875 Hz is observed for 23 periods, the component at 3 Hz for 24 periods, and the component at 3.125 Hz for 25 periods. The resolution calculated according to Equation 1 is 0.125 Hz.\nA chunk of code written in MATLAB (Natick, Massachusetts: The MathWorks, Inc., https://www.mathworks.com) shows how to simulate the sinusoidal mixture and to perform spectral analysis.\n% *****************************\n% sinusoidal mixture generation\n% *****************************\n\nfs = 16;   % sampling frequency, Hz\nTs = 1/fs; % sampling interval, s\nT  = 8;    % sample period, s\n\nt  = (0:Ts:T-Ts); % time domain, s\nN  = length(t);   % number of samples\n\nf1 = 2.9375;      % frequency component 1, Hz\nf2 = 3;           % frequency component 2, Hz\nf3 = 3.0625;      % frequency component 3, Hz\n\nx1 = cos(t.*(2*pi*f1)); % sinusoid 1\nx2 = cos(t.*(2*pi*f2)); % sinusoid 2\nx3 = cos(t.*(2*pi*f3)); % sinusoid 3\nX  = x1 + x2 + x3;      % sinusoidal mixture\n\n% ********************************************************\n% calculation of single-side spectrum (see comments below)\n% ********************************************************\n\nP              = fft(X, N);         % FFT calculation\nP2             = abs(P/N);          % double-side spectrum, rescaled by N     \nP1             = P2(1:N/2+1);  \nP1(:, 2:end-1) = 2*P1(2:end-1); \nY              = P1(1:end-1);       % single-side spectrum\nf              = (0:N/2-1).*(fs/N); % frequency domain, Hz\n\n\n\n\n\n\nComments\n\n\n\nThe documentation in MATLAB explains the procedure to convert the N-points FFT spectrum of the signal to the single-side amplitude spectrum:\n\nBecause the FFT function includes a scaling factor N between the original and the transformed signals, rescale the spectrum by dividing by N.\nTake the complex magnitude of the FFT spectrum. The two-side amplitude spectrum P2, where the spectrum in the positive frequencies is the complex conjugate of the spectrum in the negative frequencies, has half the peak amplitudes of the time-domain signal.\nTo convert to the single-side spectrum Y, take the first half P1 of the two-side spectrum P2 and multiply by 2. P1(1) and P1(end) are not multiplied by 2 because these amplitudes correspond to the zero and Nyquist frequencies, respectively, and they do not have the complex conjugate pairs in the negative frequencies.\nDefine the frequency domain f for the single-side spectrum Y.\n\n\n\nThe single-side spectrum of the sinusoidal mixture is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Single-side spectrum of the sinusoidal mixture composed of three components at 2.875 Hz, 3 Hz, 3.125 Hz; sampling frequency: 16 Hz; sample period: 8 s. For the sake of visualization the frequency interval shown is limited to 2.5-3.5 Hz.\n\n\n\nConsider now the case that the three components of the sinusoidal mixture have frequencies 2.9375 Hz, 3 Hz, 3.0625 Hz. To resolve them, I would need to double the frequency resolution of the FFT, up to 0.0625 Hz, with respect to the scenario of Figure 1. Erroneously, I double the sampling frequency. However, this expedient leaves the frequency resolution unaltered, since doubling the sample period without touching the sample period also doubles the number of samples. Moreover, the sample period is not properly chosen to avoid spectral leakage, as clearly seen in Figure 2.\n\n\n\n\n\n\nFigure 2: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 32 Hz; sample period: 8 s.\n\n\n\nTo resolve the components of the sinusoidal mixture and avoid spectral leakage, the frequency resolution of 0.0625 Hz can be achieved by extending the time period to \\(T_r=16\\) s, which corresponds to \\(N=256\\) samples at the original sampling frequency \\(f_s=16\\) Hz, see Figure 3.\n\n\n\n\n\n\nFigure 3: Single-side spectrum of the sinusoidal mixture composed of three components at 2.9375 Hz, 3 Hz, 3.0625 Hz; sampling frequency: 16 Hz; sample period: 16 s.\n\n\n\n\nTo conclude: whereas the sampling frequency sets the time resolution, the sample period occupies a central place in setting the frequency resolution when spectral analysis is performed. The sample period being the same, just increasing the sampling frequency simply increases the number of samples by the same ratio, leaving their ratio unaltered. The only means to increase the frequency resolution is to increase the sample period, trying at the same time to minimize the effects of spectral leakage."
  },
  {
    "objectID": "posts/probability_estimation/index.html",
    "href": "posts/probability_estimation/index.html",
    "title": "Confidence intervals for proportions",
    "section": "",
    "text": "Consider a Bernoulli test with \\(n\\) independent trials and \\(x\\) successes that are recorded in the test. The proportion \\(f=x/n\\) is assumed to be the estimate of the true probability \\(p\\); we want to determine the values \\(p_1\\) and \\(p_2\\) of the confidence interval (CI) with the required confidence level associated to \\(f\\), namely \\(CL=100(1-\\alpha)\\%\\).\nThe Clopper-Pearson equations can be used to calculate the CI:\n\\[\n\\left\\{\n\\begin{split}\n\\sum_{k=x}^{\\infty}{n\\choose k}p_1^k(1-p_1)^{n-k}&=c_1\\\\\n\\sum_{k=0}^x{n\\choose k}p_2^k(1-p_2)^{n-k}&=c_2\n\\end{split}\n\\right.\n\\tag{1}\\]\nwhere \\(CL=1-c_1-c_2\\). One common choice for \\(c_1\\) and \\(c_2\\) is the symmetric interval, \\(c_1=c_2=(1-CL)/2\\). For instance, when \\(CL=95\\%\\), \\(\\alpha=0.05\\) and \\(c_1=c_2=2.5\\%\\). The Clopper-Pearson interval is also called the exact interval, since the solution of Equation 1 with respect to \\(p_1\\) and \\(p_2\\) gives the correct probability estimate, even for small samples.\nIt is noted that the binomial model has been used in Equation 1 to capture events of the type “number of successes out of a given number of independent trials”, namely:\n\\[\n\\text{Pr}(k\\;\\text{successes out of}\\;n\\;\\text{trials})={n\\choose k}p^k(1-p)^{n-k}\n\\]\nwhere \\(0\\leq k\\leq n\\) and \\({n\\choose k}\\) is the number of combinations of \\(k\\)-length sequences from an \\(n\\)-length sequence (I discussed this in a previous post of mine).\nWe recall that a binomial random variable \\(X\\) with parameters \\(n\\) (the number of independent trials) and \\(p\\) (the probability of success) has mean value and standard deviation:\n\\[\n\\left\\{\n\\begin{split}\n\\mu_X&=np\\\\\n\\sigma_X&=\\sqrt{np(1-p)}\n\\end{split}\n\\right.\n\\]\nTherefore the random variable \\(F=X/n\\) has mean value and standard deviation:\n\\[\n\\left\\{\n\\begin{split}\n\\mu_F&=p\\\\\n\\sigma_F&=\\frac{\\sigma_X}{\\sqrt{n}}=\\sqrt{\\frac{p(1-p)}{n}}\n\\end{split}\n\\right.\n\\]\nThe normal approximation to the binomial distribution, as stated by the De Moivre-Laplace theorem, is considered as an alternative to Equation 1 for CI construction. This theorem states that the random variable\n\\[\nT=\\frac{F-p}{\\sigma_F}\n\\]\nis approximately normally distributed - in practice, good approximations are achieved for \\(np,n(1-p)\\gg1\\).\nGiven a variate \\(t\\) of \\(T\\), the CI can be written as follows:\n\\[\n\\frac{\\vert\\,f-p\\,\\vert}{\\sqrt{\\dfrac{p(1-p)}{n}}}\\leq t_{1-\\alpha/2}\n\\tag{2}\\]\nwhere \\(t_{1-\\alpha/2}\\) is the quantile of the normal distribution at specified probability \\(1-\\alpha/2\\): for \\(\\alpha=0.05\\), \\(t_{1-\\alpha/2}=1.96\\).\nSquaring Equation 2 we get:\n\\[\nn(f-p)^2\\leq t_{1-\\alpha/2}^2\\,p(1-p)\n\\]\nWe can solve the resulting second-order equation with respect to the unknown \\(p\\), yielding the Wilson interval:\n\\[\np\\in\\dfrac{f+\\dfrac{t^2_{1-\\alpha/2}}{2n}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\\pm\\dfrac{t_{1-\\alpha/2}\\sqrt{\\dfrac{t^2_{1-\\alpha/2}}{4n^2}+\\dfrac{f(1-f)}{n}}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\n\\tag{3}\\]\nIt is noted that the Wilson interval is not centered on the measured proportion \\(f\\) but on the value \\((f+t_{1-\\alpha/2}^2/2n)/(t_{1-\\alpha/2}^2/n+1)\\), which is a function of \\(f\\) and the number of trials \\(n\\). This is due to the asymmetry of the binomial distribution for small \\(n\\).\n\n\n\n\n\n\nContinuity correction\n\n\n\nThe accuracy of Equation 3 can be improved by applying the continuity correction to the proportion \\(f=x/n\\). This correction improves the accuracy of CI construction for discrete variables when they can be approximated by normal variables:\n\\[\nf_{\\pm}=\\frac{x\\pm0.5}{n}\\rightarrow\\left\\{\\begin{split}f_{+}&=\\min\\left(1,\\dfrac{x+0.5}{n}\\right)\\\\f_{-}&=\\max\\left(0,\\dfrac{x-0.5}{n}\\right)\\end{split}\\right.\n\\]\nThe interval is estimated as follows:\n\\[\np\\in[\\max(0,p_{-}),\\min(1,p_{+})]\n\\]\nwhere:\n\\[\np_{\\pm}\\in\\dfrac{f_{\\pm}+\\dfrac{t^2_{1-\\alpha/2}}{2n}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\\pm\\dfrac{t_{1-\\alpha/2}\\sqrt{\\dfrac{t^2_{1-\\alpha/2}}{4n^2}+\\dfrac{f_{\\pm}(1-f_{\\pm})}{n}}}{\\dfrac{t^2_{1-\\alpha/2}}{n}+1}\n\\]\n\n\nThe Wilson interval (with continuity correction) is generally considered valid for \\(n&gt;10\\). For \\(n\\gg1\\) (namely, \\(n&gt;30\\)), a very popular approximate formulation of the CI is the Wald interval:\n\\[\np\\in f\\pm t_{1-\\alpha/2}\\sqrt{\\dfrac{f(1-f)}{n}}\n\\tag{4}\\]\nThe Wald interval, which does not require any continuity correction, is symmetric around the estimated proportion. For \\(n\\gg1\\) it is noted indeed that the Wilson interval tends to become symmetric and identical to the Wald interval.\nThe question is when \\(n\\) is large enough for these methods to produce accurate inference. The R code below is a fully reproducible code to generate coverage plots for the Wilson interval (with continuity correction) and the Wald interval.\n\n\nCode\nset.seed(1789)\nn  &lt;- 30                          # number of Bernoulli trials\nL  &lt;- 1000                        # number of simulated sequences \na  &lt;- 95                          # confidence level\nt  &lt;- qnorm((1+a/100)/2)          # quantile of the normal distribution\np  &lt;- seq(0.05, 0.95, by = 0.01)  # tested values of probability\nnp &lt;- length(p)                   # number of tested values of probability\nc_wald   &lt;- numeric(np)           # Wald approach\nc_wilson &lt;- numeric(np)           # Wilson approach\n\nfor(i in 1:np) {\n  for(j in 1:L) {\n    x &lt;- rbinom(1, size = n, prob = p[i])  # number of successes\n    f &lt;- x/n                               # proportion of successes\n    fl &lt;- max(0, (x-0.5)/n) # continuity correction\n    fu &lt;- min(1, (x+0.5)/n) \n    CIl &lt;- (fl+t^2/(2*n))/(1+t^2/n) - t*sqrt(fl*(1-fl)/n + t^2/(4*n^2))/(1+t^2/n)\n    CIu &lt;- (fu+t^2/(2*n))/(1+t^2/n) + t*sqrt(fu*(1-fu)/n + t^2/(4*n^2))/(1+t^2/n)\n    CI_lower_wilson &lt;- max(0.0, CIl) # Wilson interval\n    CI_upper_wilson &lt;- min(1.0, CIu)\n    CI_lower_wald &lt;- f - t*sqrt(f*(1-f)/n) # Wald interval\n    CI_upper_wald &lt;- f + t*sqrt(f*(1-f)/n)\n    if(CI_lower_wilson &lt;= p[i] && CI_upper_wilson &gt;= p[i]) c_wilson[i] = c_wilson[i] + 1\n    if(CI_lower_wald &lt;= p[i] && CI_upper_wald &gt;= p[i]) c_wald[i] = c_wald[i] + 1\n  }\n}\nc_wilson &lt;- 100*c_wilson/L # estimated coverage\nc_wald   &lt;- 100*c_wald/L   \n\n\n\n\n\n\n\n\n\n\nFigure 1: Coverage curves for the binomial distribution with \\(n=30\\).\n\n\n\n\n\nThe simulation results of Figure 1 (\\(n=30\\)) show that, whereas the Wilson method tends just to provide some limited over-coverage for all values of \\(p\\) (this is simply due to \\(x\\) being present in both the sums of Equation 1), the Wald interval performs poorly, with severe under-coverage, especially for low and high values of \\(p\\). These results seem to raise objections against the use of the Wald interval, even in situations when \\(n\\gg1\\): Figure 2 shows the coverage plots in the case of \\(n=100\\).\n\n\n\n\n\n\n\n\nFigure 2: Coverage curves for the binomial distribution with \\(n=100\\).\n\n\n\n\n\nAlthough the performance of the Wald method has slightly improved, the Wilson method still outperforms it, especially for small and high values of \\(p\\).\n\nTo conclude, when estimating the confidence interval associated to the estimate of proportions, the Wald interval - the most basic confidence interval usually considered by experimentalists - appears to be seriously flawed, also for values of the number of trials that are considered large in most statistical textbooks. The Wilson interval with continuity correction tends to perform much better and its use should be encouraged."
  },
  {
    "objectID": "posts/gambler/index.html",
    "href": "posts/gambler/index.html",
    "title": "Gambler’s ruin",
    "section": "",
    "text": "The gambler’s ruin problem, first mentioned by Blaise Pascal in one of his letters to Pierre Fermat, was then reformulated by Christiaan Huygens, leading to important advances in the mathematical theory of probability. The statement of the problem considered here is slightly different from the one described by either Pascal or Huygens (Bertsekas and Tsitsiklis 2008).\n\nConsider a gambler who starts with an initial wealth of €\\(k\\) and then on each successive bet either wins €1 or loses €1 with probabilities \\(p\\) and \\(q=1−p\\) respectively. Different bets are assumed independent. The gambler’s objective is to reach a total wealth of €\\(n\\) (\\(n&gt;k\\)). If the gambler succeeds, then the gambler is said to win the game. The gambler stops playing after winning or getting ruined (running out of money), whichever happens first.\n\nCalculate the probability that the gambler wins. In particular, which is the probability of the gambler becoming infinitely rich?"
  },
  {
    "objectID": "posts/gambler/index.html#problem",
    "href": "posts/gambler/index.html#problem",
    "title": "Gambler’s ruin",
    "section": "",
    "text": "The gambler’s ruin problem, first mentioned by Blaise Pascal in one of his letters to Pierre Fermat, was then reformulated by Christiaan Huygens, leading to important advances in the mathematical theory of probability. The statement of the problem considered here is slightly different from the one described by either Pascal or Huygens (Bertsekas and Tsitsiklis 2008).\n\nConsider a gambler who starts with an initial wealth of €\\(k\\) and then on each successive bet either wins €1 or loses €1 with probabilities \\(p\\) and \\(q=1−p\\) respectively. Different bets are assumed independent. The gambler’s objective is to reach a total wealth of €\\(n\\) (\\(n&gt;k\\)). If the gambler succeeds, then the gambler is said to win the game. The gambler stops playing after winning or getting ruined (running out of money), whichever happens first.\n\nCalculate the probability that the gambler wins. In particular, which is the probability of the gambler becoming infinitely rich?"
  },
  {
    "objectID": "posts/gambler/index.html#solution",
    "href": "posts/gambler/index.html#solution",
    "title": "Gambler’s ruin",
    "section": "Solution",
    "text": "Solution\nLet us denote by \\(A\\) the event that the gambler ends up with €\\(n\\) and by \\(F\\) the event that the gambler wins the first bet. The probability of the event \\(A\\) given that the gambler starts with €\\(k\\) is written \\(w_k\\). The total probability law allows us to write:\n\\[\n\\begin{split}\nw_k&=P(A\\,\\vert\\,F)P(F)+P(A\\,\\vert\\,\\overline{F})P(\\overline{F})\\\\\n&=pP(A\\,\\vert\\,F)+(1-p)P(A\\,\\vert\\,\\overline{F})\n\\end{split}\n\\tag{1}\\]\nwhere \\(\\overline{F}\\) is the complement of \\(F\\), namely the event \\(\\overline{F}\\) is that the gambler loses the first bet.\nNow, the event \\(A\\,\\vert\\,F\\) is the event \\(A\\) given that the gambler starts with €\\((k+1)\\), whose probability is \\(w_{k+1}\\) (because of the independence from the past). The event \\(A\\,\\vert\\,\\overline{F}\\) is the event \\(A\\) given that the gambler starts with €\\((k-1)\\), whose probability is \\(w_{k-1}\\) (because of the independence from the past). Equation 1 reads:\n\\[\nw_k=p\\,w_{k+1}+(1-p)\\,w_{k-1}\n\\tag{2}\\]\nIf we define \\(r=(1-p)/p\\), we can also write:\n\\[\n\\left\\{\n\\begin{split}\n&w_{k+1}-w_k=r(w_k-w_{k-1}),\\;0&lt;k&lt;n\\\\\n&w_0=0\\\\\n&w_n=1\n\\end{split}\n\\right.\n\\tag{3}\\]\nSince \\(w_{k+1}=w_k+r^k(w_1-w_0)=w_k+r^kw_1=w_1\\sum_{i=0}^kr^i\\), we can calculate \\(w_k\\) as follows:\n\\[\nw_k=\\left\\{\n\\begin{split}\n&\\frac{1-r^k}{1-r}w_1&\\quad\\text{if}\\;r\\neq1\\\\\n&kw_1&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.\n\\tag{4}\\]\nWe also know that \\(w_n=1\\). Therefore, we can solve Equation 4 for \\(w_1\\):\n\\[\nw_1=\\left\\{\n\\begin{split}\n&\\frac{1-r}{1-r^n}w_1&\\quad\\text{if}\\;r\\neq1\\\\\n&\\frac{1}{n}&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.\n\\]\nPlugging this expression of \\(w_1\\) into Equation 4, we obtain:\n\\[\n\\boxed{w_k=\\left\\{\n\\begin{split}\n&\\frac{1-r^k}{1-r^n}&\\quad\\text{if}\\;r\\neq1\\\\\n&\\frac{k}{n}&\\quad\\text{if}\\;r=1\n\\end{split}\n\\right.}\n\\tag{5}\\]\nThe probability that the gambler becomes infinitely rich (event \\(R\\)) is written:\n\\[\n\\left\\{\n\\begin{split}\n\\text{Pr}(R)&=\\lim_{n\\rightarrow\\infty}w_k=1-\\left(\\frac{1-p}{p}\\right)^k&gt;0,&\\quad p&gt;1/2\\;(r&lt;1)\\\\\n\\text{Pr}(R)&=\\lim_{n\\rightarrow\\infty}w_k=0,&\\quad p\\leq1/2\\;(r\\geq1)\\\\\n\\end{split}\n\\right.\n\\tag{6}\\]\nTo interpret the meaning of Equation 6, suppose that the gambler starting with an initial wealth of €\\(k\\) wishes to continue gambling, with the intention of earning as much money as possible. So there is no winning value €\\(n\\): the gambler will only stop if ruined. If \\(p&gt;1/2\\) (each gamble is in favor of the gambler), then there is a positive probability that the gambler will never get ruined but instead will become infinitely rich. Conversely, if \\(p\\leq0.5\\) (each gamble is not in favor of the gambler), then the gambler will get ruined with probability one."
  },
  {
    "objectID": "posts/gambler/index.html#markov-chain-model",
    "href": "posts/gambler/index.html#markov-chain-model",
    "title": "Gambler’s ruin",
    "section": "Markov chain model",
    "text": "Markov chain model\nThe discrete-state discrete-time Markov chain show in Figure 1 can be proposed to model the gambler process (Bertsekas and Tsitsiklis 2008). The \\(n+1\\) states, numbered from 0 to \\(n\\), represent the gambler’s wealth at the start of each bet. The first state 0 corresponds to ruin (getting out of money), the last state \\(n\\) corresponds to win. Given the rules of the game, the states 0 and \\(n\\) are absorbing, in the sense that it is not possible to escape from each of them anymore, once they are entered. All the other states are transient.\n\n\n\n\n\n\nFigure 1: Transition probability graph for the gambler’s ruin problem.\n\n\n\nThe problem is then to find the probabilities of absorption at each one of the two absorbing states. These absorption probabilities depend on the initial state \\(k\\;(0&lt;k&lt;n)\\). It is worthy noting that the memoryless character of the process implies that, if the gambler happens to revisit the initial state \\(k\\) after a while, the absorption probabilities are the same they were initially.\n\n\n\n\n\n\nTransition probability matrix\n\n\n\nThe state of a discrete-state discrete-time Markov chain is denoted by \\(X_t\\), that can change at certain discrete time instants \\(n\\). The state space of the chain is composed of a finite set \\(\\mathscr{S}=\\{1,\\cdots,m\\}\\). The Markov chain is described in terms of its transition probability \\(p_{ij}\\):\n\\[\np_{ij}=P(X_{t+1}=j\\,\\vert\\,X_t=i),\\quad i,j\\in\\mathscr{S}\n\\tag{7}\\]\nThe key assumption underlying Markov chains is that the Markov property holds:\n\\[\nP(X_{t+1}=j\\,\\vert\\,X_t=i,X_{t-1}=i_{t-1},\\cdots,X_0=i_0)=P(X_{t+1}=j \\,\\vert\\,X_t=i)=p_{ij}\n\\tag{8}\\]\nfor all times \\(t\\), all states \\(i,j\\in\\mathscr{S}\\) and all possible sequences \\(i_0,i_1,\\cdots,i_{t-1}\\) of earlier states. Thus, the probability law of the next state \\(X_{t+1}\\) depends on the past only through the value of the present state \\(X_t\\).\nAll of the elements of a Markov chain model can be encoded in a transition probability matrix, which is simply a two-dimensional array whose elements at the \\(i\\)th row and \\(j\\)th column is \\(P_{ij}\\):\n\\[\n\\begin{bmatrix}\np_{11}&p_{12}&\\cdots&p_{1m}\\\\\np_{21}&p_{22}&\\cdots&p_{2m}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\np_{m1}&p_{m2}&\\cdots&p_{mm}\n\\end{bmatrix}\n\\tag{9}\\]\nConsider a Markov chain in which each state is either transient or absorbing. We fix a particular absorbing state \\(s\\). It is possible to show that the probabilities \\(w_k\\) of eventually reaching state \\(s\\), starting from \\(k\\), are the unique solution of the equations \\(w_s=1, w_i=0\\), for all absorbing \\(i\\neq s\\), and for all transient \\(i\\):\n\\[\nw_i=\\sum_{j=1}^mp_{ij}w_j\n\\tag{10}\\]\n\n\n\nExample 1 (Gambler’s discrete-time Markov chain) For \\(n=4,p,q=1-p\\) the transition probability matrix of the gambler’s discrete-time Markov chain is given by:\n\\[\n\\begin{bmatrix}\n1&0&0&0&0\\\\\nq&0&p&0&0\\\\\n0&q&0&p&0\\\\\n0&0&q&0&p\\\\\n0&0&0&0&1\n\\end{bmatrix}\n\\]\nIn the case of the gambler’s ruin problem, Equation 10 can be reformulated as follows:\n\\[\nw_k=(1-p)\\,w_{k-1}+p\\,w_{k+1}\n\\]\nwith \\(w_0=0,w_n=1\\), leading to Equation 2."
  },
  {
    "objectID": "posts/gambler/index.html#simulation",
    "href": "posts/gambler/index.html#simulation",
    "title": "Gambler’s ruin",
    "section": "Simulation",
    "text": "Simulation\nThe following code written in R simulates the gambler process (adapted from (Dobrow 2016)).\n\n\nCode\nset.seed(2001)\ngambler &lt;- function(k, n, p) {\n  stake &lt;- k\n    while (stake &gt; 0 & stake &lt; n) {\n        bet &lt;- sample(c(-1, 1), 1, prob = c(1-p, p))\n        stake &lt;- stake + bet\n    }\n    if (stake == n) return(1) else return(0)\n}   \nk &lt;- 15         # initial wealth\nn &lt;- 120        # final wealth\np &lt;- 0.5        # probability of win at each bet\nntrials &lt;- 1000 # number of independent trials\n\nsim &lt;- replicate(ntrials, gambler(k, n, p))\np_ruin &lt;- 1 - sum(sim)/ntrials \n\n\nThe probability of ruin estimated over 1000 iterations of the gambler process is 0.888, in close agreement with the value 0.875 computed from Equation 5.\nIt is noted that, at each time step, the gambler appears to move randomly, either to the left or to the right by a fixed unit distance with probability \\(p\\) and \\(1-p\\), respectively. The gambler undergoes a simple random walk in one dimension, whose state space is the set of the integers \\(\\mathbb{Z}\\). The limitations to values of \\(\\mathbb{Z}\\) between 0 (ruin) and \\(n\\) (win) are the consequence of the gambling rules: these rules settle two absorbing barriers to the motion of the random walker. Nine exemplary sample paths of the random walk are shown in Figure 2, for the case that \\(k=30,n=120,p=0.5\\).\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggpubr)\n\nset.seed(2001)\nk &lt;- 30       # initial wealth\nn &lt;- 120      # final wealth\np &lt;- 0.5      # probability of win at each bet\nntrials &lt;- 9  # number of independent trials\nnstep &lt;- 1000 # number of steps\ntraj &lt;- matrix(NA, nrow = nstep, ncol = ntrials)\nfor (i in 1:ntrials) {\n  traj[1, i] &lt;- k\n  jmax &lt;- nstep\n  for (j in 2:nstep) {\n          bet &lt;- sample(c(-1, 1), 1, prob = c(1-p, p))\n          traj[j, i] &lt;- traj[j-1, i] + bet\n          if (traj[j, i] == 0 | traj[j, i] == n) {\n            break\n          }\n  }\n}   \nt &lt;- rep(seq(0, nstep-1, by = 1), ntrials)\npath &lt;- rep(c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"), each = nstep)\ntraj &lt;- c(traj)\ndf &lt;- data.frame(t = t, path = path, data = traj)\n\nmy_theme = theme(\n    axis.title.x = element_text(size = 14),\n    axis.text.x = element_text(size = 12),\n    axis.title.y = element_text(size = 14),\n    axis.text.y = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    panel.border = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    panel.spacing = unit(1, \"lines\"),\n    axis.line = element_line(colour = \"black\"))\n\nggplot(df, aes(x = t, y = data, path = path)) + \n  geom_line() +\n  labs(x = \"bets\",\n       y = \"gambler's wealth\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  geom_hline(yintercept = 120, linetype = \"dashed\") +\n  scale_y_continuous(breaks = c(0, 30, 60, 90, 120)) +\n  facet_wrap(~path, labeller = \"label_both\") + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 2: Gambler’s random walk: nine sample paths. The outcome of up to a maximum of 1000 bets is considered here."
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html",
    "href": "posts/multivariate_probability_regions/index.html",
    "title": "Multivariate probability regions",
    "section": "",
    "text": "Consider the case of a multivariate random variable \\(\\mathbf{X}\\in\\mathbb{R}^d\\), whose distribution is normal with mean value \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{C}\\):\n\\[\n\\mathbf{C}=\\begin{bmatrix}\\sigma_1^2&\\cdots&\\sigma_{1d}\\\\\\vdots&\\ddots&\\vdots\\\\\\sigma_{d1}&\\cdots&\\sigma^2_d\\end{bmatrix}\n\\]\nfrom which we draw a sample of \\(n\\) data points:\n\\[\n\\mathbf{x}_i=\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}\n\\]\nWe suppose that either the mean value or the covariance matrix are unknown; they can be estimated using the sample mean, denoted by \\(\\bar{\\mathbf{x}}\\):\n\\[\n\\bar{\\mathbf{x}}=\\begin{bmatrix}\\bar{x}_1\\\\\\vdots\\\\\\bar{x}_d\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\n\\]\nand the sample covariance matrix, denoted by \\(\\mathbf{\\Sigma}\\):\n\\[\n\\mathbf{\\Sigma}=\\{\\,\\sigma_{jk},j,k=1,\\cdots d\\,\\}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n\\]\nwhose \\(jk\\)th element is written:\n\\[\n\\Sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)\n\\]\nThe \\(95\\%\\) prediction interval is a \\(d\\)-dimensional prediction hyperellipsoid, which, formally, is written as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{1}\\]\nwhere \\(\\mathbf{x}_{\\text{new}}\\) is the new observation for which the prediction interval is desired. \\(F_{0.95,n,n-d}\\) is the quantile with probability 0.95 of the Fisher’s \\(F\\)-distribution with \\(d\\) and \\((n-d)\\) degrees of freedom.\nEquation 1 is the formula for predicting the next single observation where we only have estimates of the mean value and the covariance matrix from the sample.\nAnother interesting formula can be elaborated as for the computation of the volume of the prediction hyperellipsoid. Recall that the prediction hyperellipsoid is the transformation of the hypersphere of radius\n\\[\nr=\\sqrt{F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{2}\\]\nby the linear transform of matrix \\(\\Sigma^{1/2}\\). Let \\(V\\) the volume of the hypersphere of radius \\(r\\) in an \\(d\\)-dimensional space. The volume of the hyperellipsoid can be obtained from \\(V\\) by multiplying with the determinant of the linear transform:\n\\[\nV_d=\\underbrace{\\frac{2}{d}\\frac{\\pi^{d/2}}{\\Gamma(d/2)}r^d}_{V}\\sqrt{\\det(\\Sigma)}\n\\tag{3}\\]\n\n\n\n\n\n\nUnivariate and bivariate random variables\n\n\n\nUnivariate random variable\nWhen \\(d=1\\), Equation 1 reads:\n\\[\n\\frac{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert}{s}\\leq\\sqrt{F_{0.95,1,n-1}^{\\phantom{'}}}\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{4}\\]\nwhere \\(s\\) is the sample standard deviation:\n\\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nIt is noted that\n\\[\nF_{0.95,1,n-1}=t_{0.975,n-1}^2\n\\tag{5}\\]\nwhere \\(t_{0.975,n-1}\\) is the quantile with probability 0.975 of the \\(t\\)-Student distribution with \\(n-1\\) degrees of freedom. Therefore Equation 4 can be written:\n\\[\n\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq t_{0.975,n-1}s\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{6}\\]\nFor large \\(n\\), when the central limit theorem can be applied, Equation 6 can be simplified:\n\\[\n\\boxed{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq z_{0.975}s}\n\\]\nwhere \\(z_{0.975}=1.96\\) is the quantile with probability 0.975 of the standard normal distribution.\nBivariate random variable\nWhen \\(d=2\\), Equation 1 reads:\n\\[\n(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq2F_{0.95,2,n-2}\\frac{(n-1)(n+1)}{n(n-2)}\n\\tag{7}\\]\nSince \\(2F_{0.95,2,\\infty}=\\chi^2(2)\\), for large \\(n\\) we can simplity Equation 7 as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq\\chi^2(0.95,2)}\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#prediction-hyperellipsoid",
    "href": "posts/multivariate_probability_regions/index.html#prediction-hyperellipsoid",
    "title": "Multivariate probability regions",
    "section": "",
    "text": "Consider the case of a multivariate random variable \\(\\mathbf{X}\\in\\mathbb{R}^d\\), whose distribution is normal with mean value \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{C}\\):\n\\[\n\\mathbf{C}=\\begin{bmatrix}\\sigma_1^2&\\cdots&\\sigma_{1d}\\\\\\vdots&\\ddots&\\vdots\\\\\\sigma_{d1}&\\cdots&\\sigma^2_d\\end{bmatrix}\n\\]\nfrom which we draw a sample of \\(n\\) data points:\n\\[\n\\mathbf{x}_i=\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}\n\\]\nWe suppose that either the mean value or the covariance matrix are unknown; they can be estimated using the sample mean, denoted by \\(\\bar{\\mathbf{x}}\\):\n\\[\n\\bar{\\mathbf{x}}=\\begin{bmatrix}\\bar{x}_1\\\\\\vdots\\\\\\bar{x}_d\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\begin{bmatrix}x_{i1}\\\\\\vdots\\\\x_{id}\\end{bmatrix}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\n\\]\nand the sample covariance matrix, denoted by \\(\\mathbf{\\Sigma}\\):\n\\[\n\\mathbf{\\Sigma}=\\{\\,\\sigma_{jk},j,k=1,\\cdots d\\,\\}=\\frac{1}{n-1}\\sum_{i=1}^n(\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n\\]\nwhose \\(jk\\)th element is written:\n\\[\n\\Sigma_{jk}=\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)(x_{ik}-\\bar{x}_k)\n\\]\nThe \\(95\\%\\) prediction interval is a \\(d\\)-dimensional prediction hyperellipsoid, which, formally, is written as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{1}\\]\nwhere \\(\\mathbf{x}_{\\text{new}}\\) is the new observation for which the prediction interval is desired. \\(F_{0.95,n,n-d}\\) is the quantile with probability 0.95 of the Fisher’s \\(F\\)-distribution with \\(d\\) and \\((n-d)\\) degrees of freedom.\nEquation 1 is the formula for predicting the next single observation where we only have estimates of the mean value and the covariance matrix from the sample.\nAnother interesting formula can be elaborated as for the computation of the volume of the prediction hyperellipsoid. Recall that the prediction hyperellipsoid is the transformation of the hypersphere of radius\n\\[\nr=\\sqrt{F_{0.95,d,n-d}\\frac{(n-1)(n+1)}{n(n-d)}d}\n\\tag{2}\\]\nby the linear transform of matrix \\(\\Sigma^{1/2}\\). Let \\(V\\) the volume of the hypersphere of radius \\(r\\) in an \\(d\\)-dimensional space. The volume of the hyperellipsoid can be obtained from \\(V\\) by multiplying with the determinant of the linear transform:\n\\[\nV_d=\\underbrace{\\frac{2}{d}\\frac{\\pi^{d/2}}{\\Gamma(d/2)}r^d}_{V}\\sqrt{\\det(\\Sigma)}\n\\tag{3}\\]\n\n\n\n\n\n\nUnivariate and bivariate random variables\n\n\n\nUnivariate random variable\nWhen \\(d=1\\), Equation 1 reads:\n\\[\n\\frac{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert}{s}\\leq\\sqrt{F_{0.95,1,n-1}^{\\phantom{'}}}\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{4}\\]\nwhere \\(s\\) is the sample standard deviation:\n\\[\ns=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nIt is noted that\n\\[\nF_{0.95,1,n-1}=t_{0.975,n-1}^2\n\\tag{5}\\]\nwhere \\(t_{0.975,n-1}\\) is the quantile with probability 0.975 of the \\(t\\)-Student distribution with \\(n-1\\) degrees of freedom. Therefore Equation 4 can be written:\n\\[\n\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq t_{0.975,n-1}s\\sqrt{1+\\frac{1}{n}^{\\phantom{'}}}\n\\tag{6}\\]\nFor large \\(n\\), when the central limit theorem can be applied, Equation 6 can be simplified:\n\\[\n\\boxed{\\vert\\,x_{\\text{new}}-\\bar{x}\\,\\vert\\leq z_{0.975}s}\n\\]\nwhere \\(z_{0.975}=1.96\\) is the quantile with probability 0.975 of the standard normal distribution.\nBivariate random variable\nWhen \\(d=2\\), Equation 1 reads:\n\\[\n(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq2F_{0.95,2,n-2}\\frac{(n-1)(n+1)}{n(n-2)}\n\\tag{7}\\]\nSince \\(2F_{0.95,2,\\infty}=\\chi^2(2)\\), for large \\(n\\) we can simplity Equation 7 as follows:\n\\[\n\\boxed{(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_{\\text{new}}-\\bar{\\mathbf{x}})\\leq\\chi^2(0.95,2)}\n\\tag{8}\\]"
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#concentration-ellipse",
    "href": "posts/multivariate_probability_regions/index.html#concentration-ellipse",
    "title": "Multivariate probability regions",
    "section": "Concentration ellipse",
    "text": "Concentration ellipse\nEquation 8 is the equation of the \\(95\\%\\) prediction ellipse for a bivariate normal distribution, whose sample mean value and sample covariance matrix are calculated from a large-size dataset. The ellipse is also called the concentration ellipse. Using Equation 5 the area \\(A=V_2\\) of the concentration ellipse can be computed from Equation 2-Equation 3 when \\(n\\) is large as follows:\n\\[\n\\boxed{A=\\pi\\sqrt{\\det(\\Sigma)}\\,\\chi^2_{0.95,2}}\n\\tag{9}\\]\n\nExample 1 (Simulation) The dataset is composed of 1000 points simulated from a bivariate normal distribution with mean value and covariance matrix:\n\\[\n\\mathbf{\\mu}=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\quad\\mathbf{C}=\\begin{bmatrix}5&2\\\\2&4\\end{bmatrix}\n\\]\nI used the function rmvnorm() from the package rvnorm for the simulation of the dataset, and the function stat_ellipse() of ggplot2 for drawing the ellipse, Figure 1. Using stat_ellipse() dispensed me from computing eigenvalues and eigenvectors of the sample covariance matrix, which are needed to evaluate the orientation and the eccentricity of the ellipse in the Cartesian plane. This computation can be based on a simple principal component analysis (PCA).\n\n\nCode\nlibrary(tidyverse)\nlibrary(mvtnorm)\n\nset.seed(1492)\nsigma &lt;- matrix(c(5, 2, 2, 4), ncol = 2)\nn &lt;- 1000\nx &lt;- rmvnorm(n = n, mean = c(1, 2), sigma = sigma)\n\ndist &lt;- qchisq(0.95, 2)                # to define the 95% probability contour\nm &lt;- matrix(data = c(0, 0), ncol = 1) \nfor (i in 1:2) m[i] &lt;- mean(x[, i])    # sample mean value\nC &lt;- cov(x)                            # sample covariance matrix\nS &lt;- solve(C)                          # inverse of C\nQ &lt;- numeric(n)                        \nfor (i in 1:n) Q[i] &lt;- t(x[i, ] - m) %*% S %*% (x[i, ] - m) # computation of the quadratic form\n\ndf &lt;- data.frame(x, Q)\ndf &lt;- df %&gt;%\n  mutate(inside = Q &lt;= dist)\naest &lt;- round(100*(1 - sum(!df$inside)/n), 2) # estimated coverage\narea &lt;- pi*dist*sqrt(det(C)) # area of the concentration ellipse \n\nmy_theme = theme(\n    axis.title.x = element_text(size = 16),\n    axis.text.x = element_text(size = 14),\n    axis.title.y = element_text(size = 16),\n    axis.text.y = element_text(size = 14),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 14),\n    panel.border = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    axis.line = element_line(colour = \"black\"),\n    aspect.ratio = 1,\n    legend.position = \"none\")\n\nggplot(df, aes(x = X1, y = X2, color = inside)) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  geom_point(size = 1) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", level = 0.95, \n               color = \"blue\", fill = \"blue\", alpha = 0.2) +\n  scale_x_continuous(limits = c(-10, 10)) + \n  scale_y_continuous(limits = c(-10, 10)) + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 1: The curve corresponding to the value \\(\\chi^2(0.95,2)=5.99\\) contains \\(95\\%\\) of the variable pairs; the points falling outside the concentration ellipse are plotted in red.\n\n\n\n\n\nThe coverage of the concentration ellipse is 94.9%. Moreover, the code calculates the area enclosed by the concentration ellipse, Equation 9: 74 (arbitrary units)."
  },
  {
    "objectID": "posts/multivariate_probability_regions/index.html#an-application-to-posturography",
    "href": "posts/multivariate_probability_regions/index.html#an-application-to-posturography",
    "title": "Multivariate probability regions",
    "section": "An application to posturography",
    "text": "An application to posturography\nPostural control is often quantified by recording the trajectory of the center of pressure (CoP) during human quiet standing. This quantification has many important applications, including the assessment of balance disorders and the early detection of balance degradation to prevent falls in geriatric populations. The CoP trajectory in time, reported as the anteroposterior (AP) component (forward/backward) and the mediolateral (ML) component (left/right) paired together to form the so-called stabilogram, is typically acquired by means of a force platform, on top of which tested subjects are asked to stand in their upright posture for a while, typically 60 s, as still as possible. Manipulation of testing conditions, e.g., presence or absence of vision, different kind of surfaces for the feet to stay on are possible, so as to gain further insights about the postural control system performance.\nAmong the many CoP features that have been introduced in the literature, few of them have to do with the concentration ellipse of the CoP. E.g., the principal sway direction, to represent the relative contribution of the ML and AP components to the CoP fluctuations, in terms of the angle between the AP axis and the direction of the main eigenvector produced by the PCA. Another feature of interest, the one considered in this post, is the area of the concentration ellipse, also called the sway area (Schubert and Kirchner 2014). For instance, an increase in this feature value among elderly people has been associated with a higher risk of fall. I want to show here an example of calculation of the sway area using the code above, by taking an exemplary stabilogram from a recently published dataset (Santos et al. 2017), Figure 2.\n\n\nCode\nlibrary(ggpubr)\n\ndata &lt;- read.table(file = 'PDS01OR1grf.txt', header = TRUE, sep = \"\\t\")\nt &lt;- data$Time                  # 6000 samples acquired at a rate of 100 Hz\nCoP_AP &lt;- data$COPNET_X*1e2     # anteroposterior component (AP), cm\nCoP_ML &lt;- data$COPNET_Z*1e2     # mediolateral component (ML), cm\nCoP_AP &lt;- CoP_AP - mean(CoP_AP) # detrending\nCoP_ML &lt;- CoP_ML - mean(CoP_ML) # detrending\n\ndf &lt;- data.frame(time = t, x_ap = CoP_AP, x_ml = CoP_ML)\n\np_ap &lt;- ggplot(df, aes(x = time, y = x_ap)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"time, s\",\n       y = \"AP direction, cm\") + \n  scale_y_continuous(limits = c(-2, 2)) +\n  my_theme\n\np_ml &lt;- ggplot(df, aes(x = time, y = x_ml)) +\n  geom_line(color = \"red\") +\n  labs(x = \"time, s\",\n       y = \"ML direction, cm\") + \n  scale_y_continuous(limits = c(-2, 2)) +\n  my_theme\n\nggarrange(p_ap, p_ml)\n\n\n\n\n\n\n\n\nFigure 2: Anteroposterior and mediolateral components of the CoP for the subject with ID 1 from the Duarte’s dataset, tested Open-Eyes on a Rigid surface (first trial out of three).\n\n\n\n\n\nThe stabilogram with the concentration ellipse superimposed on it are shown in Figure 3.\n\n\nCode\nx &lt;- cbind(df$x_ml, df$x_ap) # detrending already done\nd &lt;- qchisq(0.95, 2)         # to define the 95% probability contour\nn &lt;- nrow(x)\nC &lt;- cov(x)                  # sample covariance matrix\nS &lt;- solve(C)                # inverse of C\nQ &lt;- numeric(n)                        \nfor (i in 1:n) Q[i] &lt;- t(x[i, ]) %*% S %*% x[i, ] # computation of the quadratic form\n\ndf_Q &lt;- data.frame(x, Q)\ndf_Q &lt;- df_Q %&gt;%\n  mutate(inside = Q &lt;= dist)\naest &lt;- round(100*(1 - sum(!df_Q$inside)/n), 2) # estimated coverage\narea &lt;- pi*dist*sqrt(det(C)) # area of the concentration ellipse \n\nggplot(df_Q, aes(x = X1, y = X2, color = inside)) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  geom_point(size = 0.5) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", level = 0.95, \n               color = \"blue\", fill = \"blue\", alpha = 0.2) +\n  labs(x = \"ML component, cm\",\n       y = \"AP component, cm\") +\n  scale_x_continuous(limits = c(-2, 2)) + \n  scale_y_continuous(limits = c(-2, 2)) + \n  my_theme\n\n\n\n\n\n\n\n\nFigure 3: Exemplary stabilogram with superimposed the concentration ellipse - parts of the CoP trajectory outside the concentration ellipse are displayed in red.\n\n\n\n\n\nThe coverage of the concentration ellipse is 97%. The sway area is 3.09 cm\\(^2\\)."
  },
  {
    "objectID": "posts/combinatorics/index.html",
    "href": "posts/combinatorics/index.html",
    "title": "Pills of combinatorics",
    "section": "",
    "text": "At least in the elementary case of finite sample spaces (i.e., a finite number of outcomes can be generated by our experiment), we often find useful to consider the intuitive notion that the outcomes are equally likely; this allows to introduce what is known as the classical definition of probability. For any set \\(A\\) made of some collection of outcomes from the sample space, we define the probability of \\(A\\) as:\n\\[\n\\text{Pr}(A)=\\frac{\\text{number of cases in}\\;A}{\\text{number of cases in the sample space}}\n\\]\nAfter the sample space has been defined and its size determined, we calculate probabilities of sets by counting the number of possible cases.\nThe art of counting is a relevant part of a field in mathematics known as combinatorics. In this post, I present the basic principle of counting and apply it to a number of situations that are often encountered in probabilistic models."
  },
  {
    "objectID": "posts/combinatorics/index.html#the-counting-principle",
    "href": "posts/combinatorics/index.html#the-counting-principle",
    "title": "Pills of combinatorics",
    "section": "The counting principle",
    "text": "The counting principle\nConsider an experiment that consists of two consecutive stages. The possible outcomes of the first stage are \\(a_1,\\cdots,a_n\\) and, for each outcome from the first stage, \\(b_1,\\cdots,b_m\\) outcomes are possible for the second stage. The outcomes of the two-stage experiment are thus the ordered pairs \\((a_i,b_j),i=1,...,m;j=1,\\cdots,m\\), whose number is \\(nm\\). An obvious generalization is for an \\(r\\)-stage experiment, with \\(n_i,i=1,2,\\cdots,r\\) outcomes each. The total number of outcomes is\n\\[\nN=\\prod_{i=1}^rn_i\n\\]\n\nExample 1 (Number of subsets of an \\(n\\)-element set.) Consider an \\(n\\)-element set \\(\\{a_1,a_2,\\cdots,a_n\\}\\). The construction of a subset can be seen as an \\(n\\)-stage process, where, at the \\(i\\)th stage, the \\(i\\)th element is offered the opportunity to be a member of the subset or not (binary choice). Therefore the number of subsets is \\(N=2^n\\), inclusive of either the empty set (no elements are in one subset) or the sample space (all the elements are in another subset).\n\nConsider now the situation where we have \\(n\\) distinct objects in a box and we want to form groups by sequentially selecting \\(k\\) objects from it, without repetitions being allowed (sampling without replacement) or with repetitions being allowed (sampling with replacement). In the former case, of course, we cannot select more objects than they are in the box, namely \\(k\\leq n\\). In the latter case, the restriction \\(k\\leq n\\) does not apply and, although unlikely, a group can even consist of \\(k\\) replicas of the same object.\nMoreover, the selection process can differ in regard to wthether we are interested in the order of selection or not. If the order of selection matters, two groups that are made by the same objects, each one with its own number of occurrences, are considered distinct, whereas, if the order of selection does not matter, they should count as one case only in the probability calculation.\nBased on these properties, four different arrangements of \\(k\\) out of a collection of \\(n\\) objects have to be considered, namely (without/with) repetition-order of selection (does/does not) matter. Each arrangement has its own name and related formula of counting, as outlined in the following."
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-without-replacement",
    "href": "posts/combinatorics/index.html#sampling-without-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling without replacement",
    "text": "Sampling without replacement\n\nOrder of selection matters\nThe number of arrangements, called permutations, can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at the second stage, the choices are reduced by one unit, i.e., \\(n-1\\); when we reach the \\(k\\)-stage, we are left with \\(n-k+1\\) choices. Applying the principle of counting, we have:\n\\[\nD(n,k)=n(n-1)\\cdots (n-k+1)=\\frac{n!}{(n-k)!}\n\\tag{1}\\]\nwhere the factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\), i.e., \\(n!=1\\cdot2\\cdots n\\). Recall that, when \\(k=n\\), \\(D(n,n)=n!\\) (\\(0!=1!=1\\)).\n\n\nOrder of selection does not matter\nThe number of arrangements, called combinations, can be calculated by noting that there exist \\(P(k,k)=k!\\) sequences of length \\(k\\) that differ from one another just in terms of the order of the presentation of their elements. They contribute only one arrangement to the total number of them:\n\\[\nC(n,k)=\\frac{D(n,k)}{D(k,k)}=\\frac{n!}{(n-k)!k!}={n\\choose k}\n\\tag{2}\\]\nwhere the binomial coefficient \\({n\\choose k}\\), indexed by the pair of integers \\(n\\geq k\\geq0\\), is considered.\n\nExample 2 (Newton’s binomial theorem) For any real number \\(n\\) that is not a non-negative integer, the theorem states that:\n\\[\n\\sum_{k=0}^n{n\\choose k}a^kb^{n-k}=(a+b)^n\n\\]\nwhen \\(a,b\\in\\mathbb{R}\\).\nSince \\({n\\choose k}\\) is the number of \\(k\\)-element sequences of a given \\(n\\)-element set, the sum over \\(k\\) of \\({n\\choose k}\\) counts the number of subsets of all possible sizes. Using the result of Example 1:\n\\[\n\\sum_{k=0}^{n}{n\\choose k}=2^n\n\\]\nThis result is also a simple application of the Newton’s binomial theorem for \\(a=b=1\\).\n\nRecall that a combination is a choice of \\(k\\) elements out of an \\(n\\)-element set without regard to order. This is the same as partitioning the set in two: one part contains \\(k\\) elements and the other contains the remaining \\(n−k\\). We can generalize by considering partitions in more than two subsets. We have \\(n\\) distinct objects and we are given non-negative integers \\(n_1,n_2,\\cdots,n_r\\), whose sum is equal to \\(n\\). The \\(n\\) items are to be divided into \\(r\\) disjoint groups, with the \\(i\\)th group containing exactly \\(n_i\\) items. The total number of groups is given by the multinomial coefficient:\n\\[\nN_r={n\\choose n_1n_2\\cdots n_r}=\\frac{n!}{n_1!n_2!\\cdots n_r!}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#sampling-with-replacement",
    "href": "posts/combinatorics/index.html#sampling-with-replacement",
    "title": "Pills of combinatorics",
    "section": "Sampling with replacement",
    "text": "Sampling with replacement\n\nOrder of selection matters\nThe number of arrangements, called dispositions can be calculated as the result of a \\(k\\)-stage process. At the first stage, we have \\(n\\) possible choices, at all the other stages, the choices are still \\(n\\). Applying the principle of counting, we have:\n\\[\nD^*(n,k)=n^k\n\\tag{3}\\]\n\n\nOrder of selection does not matter\nThe number of arrangements, called partitions is given by:\n\\[\nC^*(n,k)=\\frac{(n+k-1)!}{k!(n-1)!}={n+k-1\\choose k}\n\\tag{4}\\]\nTo understand the formula of Equation 4, think of writing \\(C^*(n,k)\\) when, for instance, \\(n=6,k=4\\) as, say, \\(a_1a_2a_3a_2\\) or equivalently \\(a_1*a_2**a_3*a_4\\), where any element \\(a_i,i=1,2,\\cdots,n\\) is followed by a number of asterisks equal to the number of its occurrences in the sequence (the total number of asterisks in the equivalent representation needs to be equal to \\(k\\)). Another example: \\(a_2a_3a_3a_3\\) and \\(a_1a_2*a_3***a_4\\). It is observed that a one-to-one correspondence exists between the original arrangements and all possible permutations in the alignment of elements and asterisks in the equivalent representation. Since each alignment starts with \\(a_1\\), we need to permute \\(n+k-1\\) elements, among which \\(k\\) (the asterisks) and \\(n-1\\) (the elements \\(a_i,i=2,\\cdots,n\\)) are equal."
  },
  {
    "objectID": "posts/combinatorics/index.html#formulae",
    "href": "posts/combinatorics/index.html#formulae",
    "title": "Pills of combinatorics",
    "section": "Formulae",
    "text": "Formulae\nIn conclusion, the four different arrangements of \\(k\\) out of a collection of \\(n\\) objects can be computed as follows:\n\\[\n\\begin{split}\n&\\quad\\quad\\textbf{without replacement}&\\quad\\quad\\textbf{with replacement}\\\\\n\\\\\n\\textbf{order matters}&\\quad\\quad\\text{permutations}&\\quad\\quad\\text{dispositions}\\\\\n&\\quad\\quad D(n,k)=\\dfrac{n!}{(n-k)!}&\\quad\\quad D^*(n,k)=n^k\\\\\n\\textbf{order does not matter}&\\quad\\quad\\text{combinations}&\\quad\\quad\\text{partitions}\\\\\n&\\quad\\quad C(n,k)={n\\choose k}&\\quad\\quad C^*(n,k)={n+k-1\\choose k}\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/combinatorics/index.html#exercises",
    "href": "posts/combinatorics/index.html#exercises",
    "title": "Pills of combinatorics",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1 Given a group of \\(n\\) individuals, count how many subgroups can be formed having one particular person as the leader, and a number (from 0 to \\(n-1\\)) of additional members.\nAnswer The counting principle can be applied as follows. First, we have \\(n\\) possible choices for the leader, and once the leader is chosen, \\(2^{n-1}\\) subsets can be considered, which may include from none to all the remaining individuals of the group. We have therefore:\n\\[\nN=n2^{n-1}\n\\]\n\n\nExercise 2 How many words that consists of four distinct letters can be formed?\nAnswer The sample space is composed of \\(n=26\\) elements. The sequences we are interested are formed by \\(k=4\\) elements, repetitions are not allowed. Two words are considered distinct based on the order of appearance of the four letters into them. The number of words is then given by Equation 1:\n\\[\nN=D(n,k)=\\frac{n!}{(n-k)!}=\\frac{26!}{22!}=26\\cdot25\\cdot24\\cdot23=358,800\n\\]\n\n\nExercise 3 How many combinations exist using two letters out of four letters?\nAnswer The sample is composed of \\(n=4\\) elements. The sequences we are interested are formed by \\(k=2\\) distinct elements, repetitions are not allowed. Two sequences are considered the same if they differ just by the order of the elements in them, namely \\(a_1a_2\\) is the same as \\(a_2a_1\\). The number of combinations is then given by Equation 2:\n\\[\nN=C(n,k)={n\\choose k}=\\frac{4!}{2!2!}=6\n\\]\n\n\nExercise 4 How many distinct combinations can be formed from the letters ATALANTA?\nAnswer There are \\(n=8\\) letters which may be arranged in \\(n!\\) ways, but this leads to double counting. If the \\(k_1=4\\) “A”s are permuted, then nothing is changed, and similarly for the \\(k_2=2\\) “T”s. The total number of distinct combinations is then given by the multinomial coefficient:\n\\[\nN=\\frac{n!}{k_1!k_2!}=\\frac{8!}{4!2!}=7\\cdot6\\cdot5\\cdot4=840\n\\]"
  },
  {
    "objectID": "posts/random_incidence/index.html",
    "href": "posts/random_incidence/index.html",
    "title": "Random incidence",
    "section": "",
    "text": "Consider the situation when the continuous time axis of our observations is partitioned into a sequence of interarrival intervals. With the term arrival, we designate the occurrence of everything we are interested to observe, for instance, a particle emitted from radioactive material and captured by a radiation counter, a message reaching its destination queue, maybe the bus that we are waiting for at the bus stop. Usually, probabilistic models that attempt to describe these different type of arrivals share the same assumption: namely the interarrival times (i.e., the times between successive arrivals) are modeled as independent random variables. For instance, the continuous-time Poisson process (hopefully, this is not the right model to predict the next bus arrival!) is the case where the interarrival times are modeled as independent identically exponentially distributed random variables.\n\n\n\n\n\n\nExponential distribution\n\n\n\nA continuous random variable \\(X\\) is said to be exponential, or exponentially distributed with parameter \\(\\lambda\\), when its cumulative distribution function (CDF) is written\n\\[\nF_X(x)=\\text{Pr}(X\\leq x)=1-e^{-\\lambda x},\\;x\\geq0\n\\]\nThe probability density function (PDF) is then given by:\n\\[\np_X(x)=\\frac{d}{dx}F_X(x)=\\lambda\\,e^{-\\lambda x},\\;x\\geq0\n\\]\nThe mean value, the mean square value and the variance of \\(X\\) are:\n\\[\n\\left\\{\n\\begin{split}\nE[X]&=\\int_0^{\\infty}xp_X(x)\\,dx=\\frac{1}{\\lambda}\\\\\nE[X^2]&=\\int_0^{\\infty}x^2p_X(x)\\,dx=\\frac{2}{\\lambda^2}\\\\\n\\text{Var}(X)&=E[X^2]-E[X]^2=\\frac{1}{\\lambda^2}\n\\end{split}\n\\right.\n\\]\nIntegration by parts can be used to calculate the two expectations \\(E[X]\\) and \\(E[X^2]\\).\n\n\n\n\n\n\n\n\nPoisson process\n\n\n\nConsider a sequence of independent exponential random variables \\(T_1,T_2,T_3\\cdots,\\) with common parameter \\(\\lambda\\), and let these stand for the interarrival times. The arrivals are then recorded at times \\(T_1,T_1+T_2,T_1+T_2+T_3,\\cdots\\) and so forth, to define a continuous-time Poisson process.\nA Poisson process is endowed with the following important properties:\n\nIndependence of non-overlapping sets of times. This is a direct consequence of the assumed independence of the interarrival times.\nFresh-start property. The part of the Poisson process that starts at any particular time \\(t&gt;0\\) is a probabilistic replica of the Poisson process starting at time 0, and is independent of the part of the process prior to time \\(t\\). This can be seen as a special case of point 1.\nMemoryless interarrival time distribution. If \\(T\\) is the time of the first arrival and if we know that \\(T&gt;t\\), then the remaining time \\(T−t\\) is exponentially distributed, with the same parameter \\(\\lambda\\):\n\n\\[\n\\begin{split}\n\\text{Pr}(T&gt;t+s\\,\\vert\\,T&gt;t)&=\\frac{\\text{Pr}(T&gt;t+s,T&gt;t)}{\\text{Pr}(T&gt;t)}\\\\\n&=\\frac{\\text{Pr}(T&gt;t+s)}{\\text{Pr}(T&gt;t)}=\\frac{1-F_T(t+s)}{1-F_T(t)}\\\\\n&=\\frac{e^{-\\lambda(t+s)}}{e^{-\\lambda t}}=e^{-\\lambda s}=\\text{Pr}(T&gt;s)\n\\end{split}\n\\]\nProperties 2.-3. can be rephrased saying that the ones whose life is modeled by an exponential distribution, well they should remain forever young: no matter how long they have lived so far, the remaining time to their death is predicted as if they are just born!\n\n\nThe term random incidence denotes the arrival of an observer at an arbitrary time \\(t^*\\) into a gap between two consecutive arrivals in an arrival-type process that is not necessarily described by a Poisson model, Figure 1.\n\n\n\n\n\n\nFigure 1: Illustration of the random incidence phenomenon.\n\n\n\nWe are not saying that \\(t^*\\) is random; in this regard, perhaps, using the term random incidence may appear misleading. However, the interval between the time of the previous arrival \\(t_p\\) and \\(t^*\\), and the interval between \\(t^*\\) and the time of the next arrival \\(t_n\\) are random. All we need to state to start our discussion is to assume that the observer enters the arrival-type process in a situation when a previous arrival has surely occurred: probabilistically, the gap \\(t_n-t_p\\) is then a well-defined quantity.\nSuppose that we know the probability law of the interarrival times \\(Y\\). Let us denote by \\(W\\) the random variable that describes the duration of the gap entered by random incidence, \\(W=T_n-T_p\\). Finally, we denote by \\(T\\) the random variable that describes the waiting time for the next arrival from when the gap is entered by random incidence, \\(T=T_n-T^*\\).\nIt is argued that the probability that \\(W\\) assumes a value between \\(w\\) and \\(w+dw\\) is proportional to both the the duration of the gap \\(w\\) and the relative frequency of occurrence of such gaps \\(p_Y(w)dw\\):\n\\[\n\\text{Pr}(w\\leq W\\leq w+dw)=p_W(w)dw\\propto w\\,p_Y(w)dw\n\\]\nTherefore:\n\\[\np_W(w)=\\frac{w\\,p_Y(w)}{E[Y]}\n\\tag{1}\\]\nwhere the constant of proportionality is calculated to enforce the constraint of normalization for the PDF \\(p_W(w)\\) of \\(W\\) (its integral from 0 to infinity must be 1).\nNow, given that a gap of length \\(w\\) is entered by random incidence, we are equally likely to be anywhere within the gap. More precisely, given \\(w\\), the time until the next arrival \\(T\\) has a uniform PDF:\n\\[\np_{T\\vert W}(t\\,\\vert\\,w)=\\frac{1}{w},\\;0\\leq t\\leq w\n\\tag{2}\\]\nwhere \\(p_{T\\vert W}(t\\,\\vert\\,w)\\) is the conditional PDF of \\(T\\) given \\(W\\). Using Equation 1, Equation 2 the joint PDF \\(p_{TW}(t,w)\\) can be written:\n\\[\np_{TW}(t,w)=p_{T\\vert W}(t\\,\\vert\\,w)\\,p_W(w)=\\frac{p_Y(w)}{E[Y]},\\;0\\leq t\\leq w&lt;\\infty\n\\]\nFinally, the marginal PDF \\(p_T(t)\\) of the waiting time for the next arrival from when the gap is entered by random incidence can be formed simply by integrating out \\(W\\):\n\\[\n\\boxed{p_T(t)=\\int_{t}^{\\infty}\\frac{p_Y(w)}{E[Y]}dw=\\frac{1-F_Y(t)}{E[Y]},\\;t\\geq0}\n\\tag{3}\\]\n\nExample 1 Consider a bus passenger arriving at a bus stop. The probabilistic law of the bus headways \\(F_Y(y)\\) will determine the probability law for the waiting time until the next bus arrives, \\(p_T(t)\\) via Equation 3, if we ignore interactions between successive buses and assume that the arrivals are identically distributed and independent.\nSuppose that buses maintain perfect headways, being always spaced \\(T_0\\) minutes apart:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad y&lt;T_0\\\\1&\\quad y\\geq T_0\\end{split}\\right.\\rightarrow p_Y(y)=\\delta(y-T_0)\\rightarrow E[Y]=T_0\n\\]\nwhere the PDF is expressed in terms of a delta function located at time \\(T_0\\). The PDF of \\(T\\) can be written:\n\\[\np_T(t)=\\left\\{\\begin{split}&\\frac{1}{T_0}&\\quad 0\\leq t\\leq T_0\\\\&0&\\quad t&gt;T_0\\end{split}\\right.\\rightarrow E[T]=\\frac{T_0}{2}\n\\]\nAs expected intuitively, the time until the next arrival, given random incidence, is uniformly distributed between \\(0\\) and \\(T_0\\), with mean value \\(E[T]=T_0/2\\): if \\(T_0=60\\) min the average waiting time is 30 min.\nNow, suppose that the bus headways are on the hour, and fifteen minutes after the hour. Thus, the interarrival times alternate between 15 and 45 minutes. If the bus passenger shows up at the bus stop at any time uniformly distributed within a hour, she/he has to wait for an average time of 15/2 min (with probability 1/4) and 45/2 min (with probability 3/4):\n\\[\nE[T]=\\frac{15}{2}\\cdot\\frac{1}{4}+\\frac{45}{2}\\cdot\\frac{3}{4}=18.75\\;\\text{min}\n\\]\nMore formally:\n\\[\nF_Y(y)=\\left\\{\\begin{split}0&\\quad0\\leq y&lt;15\\\\1/2&\\quad15\\leq y&lt;45\\\\1&\\quad y\\geq45\\end{split}\\right.\\rightarrow E[Y]=30\\;\\text{min}\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\left\\{\\begin{split}1/30&\\quad0\\leq t&lt;15\\\\1/60&\\quad15\\leq t&lt;45\\\\0&\\quad t\\geq45\\end{split}\\right.\\rightarrow E[T]=18.75\\,\\text{min}\n\\]\nOn average, the bus passenger has to wait longer than it might be expected taking into account \\(E[Y]\\) only, namely \\(E[Y]/2=15\\) min.\n\nThis is because an observer who arrives at an arbitrary time, the bus passenger in this example, is more likely to fall in a large rather than a small interarrival interval: large interarrival intervals tend therefore to determine longer waiting times!\n\n\n\nExample 2 Consider the (unrealistic) case that the bus headways are Poissonian. What does this mean? Basically, we state that the interarrival times are modeled by independent exponentially distributed random variables with rate \\(\\lambda\\) (the rate denotes the number of arrivals per unit time):\n\\[\nF_Y(y)=1-e^{-\\lambda t},\\;t\\geq 0\\rightarrow p_Y(y)=\\frac{F_Y(y)}{dy}=\\lambda e^{-\\lambda t},\\;t\\geq0\\rightarrow E[Y]=1/\\lambda\n\\]\nUsing Equation 3:\n\\[\np_T(t)=\\lambda e^{-\\lambda t},\\;t\\geq0\n\\]\nHence \\(T\\) is exponentially distributed with rate \\(\\lambda\\). The average time of waiting is \\(E[T]=1/\\lambda\\). No matter when the event occurred, the observer who arrives at an arbitrary time \\(t^*\\) (the bus passenger in this example) sees the Poisson process to start fresh at time \\(t^*\\), Figure 1. It is worth noting that, according to the properties of independence, start-fresh and memorylessness stated above, we can run a Poisson process either forwards or backwards in time, without any modification in its properties. Hence, not only \\(T_n-T^*\\), but also \\(T^*-T_p\\) is exponentially distributed with parameter \\(\\lambda\\). Moreover, \\(T_n-T^*\\) and \\(T_n-T^*\\) are independent. We have therefore established that the gap \\(W\\) entered by random incidence is the sum of two independent exponential random variables with parameter \\(\\lambda\\) and mean value \\(2/\\lambda\\). More formally, using Equation 1 we get:\n\\[\np_W(w)=\\lambda w\\,e^{-\\lambda w},\\;w\\geq0\n\\]\nWe recall that the random variable \\(X\\) Erlang of order \\(k\\) has PDF:\n\\[\np(x)=\\frac{\\lambda^kx^{k-1}e^{-\\lambda x}}{(k-1)!}\n\\]\nIn conclusion, the gap duration is a random variable Erlang of order 2.\n\nIn a similar fashion to Example 1, when landing into a Poisson process at an arbitrary time, we are more likely to fall in a large interarrival interval; therefore, the length of what we perceive as a typical interarrival interval is greater than it is in reality."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "I am an associate professor at the BioRobotics Institute of the Scuola Superiore Sant’Anna, in Pisa, Italy. In my posts, I attempt to explain various concepts that I studied, in my double role of teacher and researcher.\nThere have been two main reasons for me to start this blog: first, learning - since writing is a very effective tool against shallow understanding (mine!); second, I wish to help others who may be struggling with similar problems.\nMy teaching is in the general area of instrumentation & measurement, statistics, signal processing and data analysis. My research focuses on developing algorithms for wearable inertial-sensor-based systems applied to motion analysis and assessment of human performance.\nRecently, I started studying the theory of diffusive processes for the modeling of time series of human motion.\nFor a record of my publications, please visit this site. Feel free to reach out to me via mail.\n\n\nThis blog’s content is under the Creative Commons Attribution-ShareAlike 4.0 International License:\n\nAngelo Maria Sabatini’s blog by Angelo Maria Sabatini is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBased on a work at https://amsabatini.netlify.app"
  },
  {
    "objectID": "index.html#copyrightpermissions",
    "href": "index.html#copyrightpermissions",
    "title": "Angelo Maria Sabatini",
    "section": "",
    "text": "This blog’s content is under the Creative Commons Attribution-ShareAlike 4.0 International License:\n\nAngelo Maria Sabatini’s blog by Angelo Maria Sabatini is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nBased on a work at https://amsabatini.netlify.app"
  }
]