<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Angelo Maria Sabatini">
<meta name="dcterms.date" content="2025-05-05">
<meta name="description" content="What does it mean to shift from statistical inference to prediction? This post uses a minimal but complete example to explore the shared terrain between statistics and machine learning ‚Äî from how models are trained, to how decisions are made and evaluated in real-world contexts.">

<title>Statistics and Machine Learning: A shared landscape ‚Äì Angelo Maria Sabatini</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-79108a0fc1995748cbd19a5b0e3e3e7c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-G7T7XXSXKG"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-G7T7XXSXKG', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<script src="../../site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="../../site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link rel="alternate" type="application/rss+xml" title="Angelo Maria Sabatini ‚Äì Blog RSS" href="../../blog.xml">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Angelo Maria Sabatini</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html"> 
<span class="menu-text">Archive</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/angelosabatini"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img" aria-label="RSS feed">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-a-statistical-model" id="toc-what-is-a-statistical-model" class="nav-link active" data-scroll-target="#what-is-a-statistical-model">What is a statistical model?</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a></li>
  <li><a href="#from-hypothesis-testing-to-predictive-modeling" id="toc-from-hypothesis-testing-to-predictive-modeling" class="nav-link" data-scroll-target="#from-hypothesis-testing-to-predictive-modeling">From Hypothesis Testing to Predictive Modeling</a>
  <ul class="collapse">
  <li><a href="#simulating-data-for-classical-inference" id="toc-simulating-data-for-classical-inference" class="nav-link" data-scroll-target="#simulating-data-for-classical-inference">Simulating Data for Classical Inference</a></li>
  <li><a href="#performing-a-t-test-and-fitting-a-regression-model" id="toc-performing-a-t-test-and-fitting-a-regression-model" class="nav-link" data-scroll-target="#performing-a-t-test-and-fitting-a-regression-model">Performing a t-test and Fitting a Regression Model</a></li>
  <li><a href="#display-results" id="toc-display-results" class="nav-link" data-scroll-target="#display-results">Display results</a></li>
  </ul></li>
  <li><a href="#from-inference-to-prediction-a-first-machine-learning-example" id="toc-from-inference-to-prediction-a-first-machine-learning-example" class="nav-link" data-scroll-target="#from-inference-to-prediction-a-first-machine-learning-example">From Inference to Prediction: A First Machine Learning Example</a>
  <ul class="collapse">
  <li><a href="#simulating-data-for-binary-classification" id="toc-simulating-data-for-binary-classification" class="nav-link" data-scroll-target="#simulating-data-for-binary-classification">Simulating Data for Binary Classification</a></li>
  <li><a href="#fitting-a-logistic-regression-model" id="toc-fitting-a-logistic-regression-model" class="nav-link" data-scroll-target="#fitting-a-logistic-regression-model">Fitting a Logistic Regression Model</a></li>
  <li><a href="#visualizing-the-decision-boundary" id="toc-visualizing-the-decision-boundary" class="nav-link" data-scroll-target="#visualizing-the-decision-boundary">Visualizing the Decision Boundary</a></li>
  <li><a href="#evaluating-performance-roc-curve-and-auc" id="toc-evaluating-performance-roc-curve-and-auc" class="nav-link" data-scroll-target="#evaluating-performance-roc-curve-and-auc">Evaluating Performance: ROC Curve and AUC</a></li>
  </ul></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks">Concluding remarks</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistics and Machine Learning: A shared landscape</h1>
  <div class="quarto-categories">
    <div class="quarto-category">statistics</div>
    <div class="quarto-category">machine learning</div>
  </div>
  </div>

<div>
  <div class="description">
    What does it mean to shift from statistical inference to prediction? This post uses a minimal but complete example to explore the shared terrain between statistics and machine learning ‚Äî from how models are trained, to how decisions are made and evaluated in real-world contexts.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Angelo Maria Sabatini </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 5, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>One of the most important similarities between statistics and machine learning (ML) is that both rely heavily on models. While this connection is sometimes overlooked, it forms the backbone of how we organize and analyze data. Regardless of the number and type of variables involved‚Äîwhether explanatory or response variables, continuous or categorical‚Äîstatistical modeling remains at the core of the process. Whether our goal is to predict outcomes, uncover relationships, or assess differences, modeling gives structure and meaning to the patterns we observe. ML, too, builds on this idea of modeling, but with a slightly different emphasis. Here, the focus is often more explicitly on defining a mathematical mapping between inputs and outputs‚Äîa modeling aspect that tends to be more immediately visible than in traditional statistical practice.</p>
<p>Several practical factors make a full overlap between the two approaches less straightforward. One is the sheer difference in the size of datasets: classical statistical analyses often rely on relatively small samples, especially in fields such as medicine or social sciences. This distinction, however, is beginning to blur: modern applications in fields like digital health, education technology, or online behavioral studies increasingly generate larger datasets, enabling the use of ML-inspired methods even in traditionally small-sample domains. ML, by contrast, tends to operate in contexts where large datasets are available‚Äîor at least assumed. Another difference lies in the typical workflow: in ML, it is standard practice to split data into training, validation, and testing sets, managing the trade-off between fitting well and generalizing. This concern, central to ML, is less explicitly present in traditional statistical workflows.</p>
<p>Despite these differences, statistics and ML increasingly share common objectives and approaches: building models that capture meaningful patterns while balancing complexity against generalization.(For readers unfamiliar with some of the technical terms, a brief summary is provided in the Key Concepts sections below.)</p>
<section id="what-is-a-statistical-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-statistical-model">What is a statistical model?</h2>
<p>In classical statistics ‚Äî for example, for methods introduced early in the training of sophomore students, such as simple linear regression and ANOVA (ANalysis Of VAriance) ‚Äî the model is typically written in a very structured form. Let us suppose that <span class="math inline">\(Y\)</span> is the response variable (outcome) and <span class="math inline">\(X_1,X_2,\cdots,X_m\)</span> are explanatory variables (predictors), collected into the vector <span class="math inline">\(\mathbf{X}\)</span>; the model is the equation that we suppose relate together the predictors to the outcome, for any of a number <span class="math inline">\(n\)</span> of observational units, with an important element noticed in the formulation of the model-the random error term, <span class="math inline">\(\varepsilon\)</span>:</p>
<p><span id="eq-model-general"><span class="math display">\[
Y_{i}=f(\mathbf{X}_i;\theta)+\varepsilon_i,\quad i=1,...,n
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(\theta\)</span> are the parameters to be estimated. This decomposition highlights two essential ideas:</p>
<ul>
<li>There is an underlying systematic component (<span class="math inline">\(f(\mathbf{X}_i;\theta)\)</span>),</li>
<li>Plus an error component (<span class="math inline">\(\varepsilon_i\)</span>), which captures variability not explained by the model.</li>
</ul>
<p>The classical statistical inferential procedures‚Äîwhether hypothesis testing or interval estimation‚Äîare fundamentally designed around the behavior of this error term. It is also important to realize that we can never observe the error term, but we can infer its behavior from the analysis of the model residual ‚Äî the difference between observed and predicted values.</p>
<p>In classical parametric inference, crucial assumptions are made about the behavior of the error terms <span class="math inline">\(\varepsilon_i\)</span> in the general model <a href="#eq-model-general" class="quarto-xref">Equation&nbsp;1</a>.</p>
<p>To ensure analytical tractability and valid inference, it is common to summarize these assumptions using the acronym LINE ‚Äî Linearity, Independence, Normality, and Equality of variance:</p>
<ol type="1">
<li><p><strong>Linearity</strong>: The model is linear in the parameters <span class="math inline">\(\theta\)</span>; that is, the expected value of the outcome variable <span class="math inline">\(Y\)</span> can be expressed as a linear combination of the parameters, possibly through transformations of the predictors <span class="math inline">\(\mathbf{X}\)</span>.</p></li>
<li><p><strong>Independence</strong>: The error terms are independent across observations. In particular, for time series data, consecutive errors are assumed to be uncorrelated. In cross-sectional data, failure of independence often reflects issues in experimental design, such as inadequate random assignment or hidden confounding. Detecting such problems can be challenging and typically requires careful planning at the data collection stage.</p></li>
<li><p><strong>Normality</strong>: The error terms are normally distributed with mean zero.</p></li>
<li><p><strong>Equality of variance (Homoscedasticity)</strong>: The error terms have constant variance across the range of the predictors.</p></li>
</ol>
<p>These assumptions provide the theoretical foundation for estimating parameters, constructing confidence intervals, and performing hypothesis tests in a coherent analytical framework. Understanding how residuals behave is not only crucial in classical inference, but also serves as a bridge toward modern predictive approaches, as discussed in the note below.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Residuals and ML">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Residuals and ML
</div>
</div>
<div class="callout-body-container callout-body">
<p>The concept of residuals ‚Äî as discrepancies between observed outcomes and model predictions ‚Äî remains central in ML as well. Although the term ‚Äúresidual‚Äù is less frequently emphasized explicitly, the analysis of prediction errors underpins critical tasks such as model evaluation, hyperparameter tuning, and post-hoc diagnostics across supervised learning workflows.</p>
<p>Understanding residuals provides a natural bridge between classical inferential thinking and modern predictive modeling practices.</p>
</div>
</div>
<p>This careful structure is what allows classical statistics to calibrate confidence intervals, compute p-values, and estimate statistical power, all based on theoretically derived sampling distributions. However, when these assumptions are violated, the inferential reliability may break down ‚Äî often in unpredictable ways. Modern approaches like <strong>bootstrap resampling</strong> offer alternatives that reduce dependency on strong theoretical assumptions, and interestingly, align closely with resampling strategies widely used in ML (such as cross-validation for hyperparameter tuning). In a sense, bootstrapping can be seen as a bridge: a classical method moving closer to the data-driven, flexible spirit that characterizes much of modern ML practice.</p>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Statistics-Based Key Concepts
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Estimator</strong>: A function of sample data used to infer the value of an unknown population parameter. Estimators form the basis for constructing confidence intervals and conducting hypothesis tests.</p></li>
<li><p><strong>Sampling Distribution</strong>: The probability distribution of an estimator over many hypothetical repeated samples from the same population.</p></li>
<li><p><strong>Confidence Interval</strong>: An estimated range, derived from an estimator, that likely contains the true value of the population parameter with a given level of confidence.</p></li>
<li><p><strong>p-value</strong>: The probability, under the null hypothesis, of obtaining a test statistic at least as extreme as the one observed.</p></li>
<li><p><strong>Statistical Power</strong>: The probability that a statistical test correctly rejects the null hypothesis when the alternative hypothesis is true.</p></li>
<li><p><strong>Type I Error</strong>: Incorrectly rejecting a true null hypothesis (false positive).</p></li>
<li><p><strong>Type II Error</strong>: Failing to reject a false null hypothesis (false negative).</p></li>
<li><p><strong>Residual</strong>: The difference between the observed value and the corresponding predicted value from a statistical model. Residuals are empirical proxies for the unobservable error terms, enabling diagnostic analyses of model fit and assumptions.</p></li>
<li><p><strong>Degrees of Freedom</strong>: The number of independent values in a calculation that are free to vary. In statistical inference, degrees of freedom determine the shape of sampling distributions (e.g., t or F) and reflect the amount of information available to estimate variability.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
ML-based key concepts
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Confusion Matrix</strong>: a table showing true positives, false positives, true negatives, and false negatives.</p></li>
<li><p><strong>ROC Curve</strong>: a plot of true positive rate vs false positive rate across different thresholds.</p></li>
<li><p><strong>ROC AUC</strong>: a single-number summary of a model‚Äôs discrimination ability across thresholds.</p></li>
<li><p><strong>Operating Point (Threshold Selection)</strong>: the decision threshold balancing false positives and false negatives, based on context.</p></li>
<li><p><strong>Class Imbalance</strong>: a condition where the categories in the outcome variable are unequally represented. It can lead to biased models, misleading performance metrics, and poor generalization ‚Äî especially when one class dominates. Common remedies include resampling, weighting, and using metrics like ROC AUC instead of raw accuracy. Although widely addressed in ML, class imbalance also affects classical inference, where low-prevalence categories can distort p-values and test power.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advanced key concepts
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Cross-Entropy Loss</strong>: measures the dissimilarity between predicted probabilities and true labels.</p></li>
<li><p><strong>Regularization</strong>: techniques (e.g., L1, L2) to prevent overfitting by penalizing model complexity.</p></li>
<li><p><strong>Gradient Descent</strong>: an optimization method for minimizing loss functions.</p></li>
<li><p><strong>Overfitting and Underfitting</strong>: learning too much noise (overfitting) or missing the signal (underfitting).</p></li>
</ul>
</div>
</div>
</section>
<section id="from-hypothesis-testing-to-predictive-modeling" class="level2">
<h2 class="anchored" data-anchor-id="from-hypothesis-testing-to-predictive-modeling">From Hypothesis Testing to Predictive Modeling</h2>
<p>Let us now illustrate the modeling perspective starting with a classical example: the independent-samples t-test. First, the neded libraries are uploaded.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="simulating-data-for-classical-inference" class="level3">
<h3 class="anchored" data-anchor-id="simulating-data-for-classical-inference">Simulating Data for Classical Inference</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate two groups with slightly different means</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>group1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">64</span>, <span class="at">mean =</span> <span class="fl">5.0</span>, <span class="at">sd =</span> <span class="fl">1.0</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>group2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">64</span>, <span class="at">mean =</span> <span class="fl">5.5</span>, <span class="at">sd =</span> <span class="fl">1.0</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine data into a single data frame</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"A"</span>, <span class="dv">64</span>), <span class="fu">rep</span>(<span class="st">"B"</span>, <span class="dv">64</span>))),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">outcome =</span> <span class="fu">c</span>(group1, group2)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We simulate two groups, randomly drawn from two normally distributed populations, with means <span class="math inline">\(\mu_A = 5.0\)</span> and <span class="math inline">\(\mu_B = 5.5\)</span>, and standard deviations <span class="math inline">\(\sigma_A = \sigma_B = 1.0\)</span>. Each group contains <span class="math inline">\(n = 64\)</span> observations. In the spirit of a real experiment, the population parameters are unknown ‚Äî the only information we have comes from the observed data. Our inferential task is to test whether a difference exists between the group means, under the standard null and alternative hypotheses:</p>
<p><span class="math display">\[
H_0: \mu_A = \mu_B \quad\quad H_1: \mu_A \ne \mu_B
\]</span></p>
<p>In this example, we simulate 64 observations per group ‚Äî a total of 128 ‚Äî to achieve 80% statistical power for detecting a medium effect size (<span class="math inline">\(d = 0.5\)</span>) with a two-sided t-test at the conventional 5% significance level. This decision is not arbitrary: in a well-planned experiment, the sample size should be defined <em>before</em> data collection, based on acceptable error rates, either of Type I or Type II. Surprisingly, many ‚Äî even published ‚Äî experimental studies skip this crucial planning step, treating sample size as an afterthought rather than a design parameter.</p>
<p>This consideration can be visualized by examining how statistical power varies with the sample size and the effect size. The plot in <a href="#fig-power-curve" class="quarto-xref">Figure&nbsp;1</a> shows the power curves for three typical values of Cohen‚Äôs effect size (<span class="math inline">\(d = 0.2\)</span>, <span class="math inline">\(0.5\)</span>, <span class="math inline">\(0.8\)</span>), representing small, medium, and large effects. The dashed line marks the conventional threshold of 80% power.</p>
<p>As the plot shows, detecting a medium effect with 80% reliability requires at least 64 observations <em>per group</em>. Informal rules of thumb such as ‚Äú30 is enough‚Äù may drastically underestimate the sample size needed for meaningful results.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-power-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-power-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="power_curve_ttest.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-power-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Power curves for two-sample t-tests at significance level (= 0.05), across three common effect sizes ((d = 0.2), (0.5), (0.8)). The dashed horizontal line marks the conventional 80% power threshold, often adopted as a design goal. The plot shows how detecting even a moderate effect requires at least 64 observations <em>per group</em> ‚Äî far more than suggested by popular heuristics.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="performing-a-t-test-and-fitting-a-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="performing-a-t-test-and-fitting-a-regression-model">Performing a t-test and Fitting a Regression Model</h3>
<p>In the snippet below, we perform a t-test and fit a regression model on the <em>same</em> simulated dataset. The outputs are produced by different functions but convey equivalent statistical information ‚Äî all centered on testing whether the two group means differ. This equivalence is not accidental: it arises from how categorical variables are treated in regression models. Specifically, dummy (or indicator) coding when doing ANOVA translates group membership into a numeric predictor, allowing the regression coefficient to represent the mean difference between groups ‚Äî exactly as tested by the t-test.</p>
<div class="cell">
<details class="code-fold">
<summary>Analysis code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform an independent-samples t-test</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>t_test_res <span class="ot">&lt;-</span> <span class="fu">t.test</span>(outcome <span class="sc">~</span> group, <span class="at">data =</span> df, <span class="at">var.equal =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear model with group as a predictor</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>lm_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(outcome <span class="sc">~</span> group, <span class="at">data =</span> df)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply ANOVA to the linear model</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>anova_res <span class="ot">&lt;-</span> <span class="fu">anova</span>(lm_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="display-results" class="level3">
<h3 class="anchored" data-anchor-id="display-results">Display results</h3>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
‚Üí t-test
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Two Sample t-test

data:  outcome by group
t = -2.8597, df = 126, p-value = 0.004964
alternative hypothesis: true difference in means between group A and group B is not equal to 0
95 percent confidence interval:
 -0.7629029 -0.1388665
sample estimates:
mean in group A mean in group B 
       5.038478        5.489362 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
‚Üí ANOVA
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Variance Table

Response: outcome
           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
group       1   6.506  6.5055  8.1781 0.004964 **
Residuals 126 100.231  0.7955                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
‚Üí Linear Model (summary)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = outcome ~ group, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.29853 -0.59548 -0.05597  0.53407  2.19797 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   5.0385     0.1115   45.19  &lt; 2e-16 ***
groupB        0.4509     0.1577    2.86  0.00496 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.8919 on 126 degrees of freedom
Multiple R-squared:  0.06095,   Adjusted R-squared:  0.0535 
F-statistic: 8.178 on 1 and 126 DF,  p-value: 0.004964</code></pre>
</div>
</div>
<p>How should we interpret these numbers? The goal of our analysis was to assess whether the two groups differ in their mean outcome. Both the t-test and the regression model provide consistent answers to this question.</p>
<p>In the regression output, the coefficient <code>groupB = 0.4509</code> represents the estimated difference in means between the two groups. The associated p-value (0.00496) indicates that this difference is statistically significant at the conventional 5% level ‚Äî strong evidence against the null hypothesis <span class="math inline">\((H_0:\mu_A=\mu_B)\)</span>.</p>
<p>The F-statistic from the ANOVA table (8.178) confirms this result, and it is numerically equal to the square of the t-statistic from the regression output <span class="math inline">\((F=t^2)\)</span>, as expected when comparing two groups using a linear model.</p>
<p>The residual standard error (0.8919) provides an estimate of the variability within groups, and the degrees of freedom (126) reflect that all available data were used for estimation. This is typical in classical inference, where no data is held out for validation ‚Äî in contrast with ML practice.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="üéØ Interpretation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
üéØ Interpretation
</div>
</div>
<div class="callout-body-container callout-body">
<p>As expected, the t-statistic and the p-value from the t-test correspond exactly to the t-statistic and p-value for the group coefficient in the regression model.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="t, F and the language of models">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
t, F and the language of models
</div>
</div>
<div class="callout-body-container callout-body">
<p>Although <code>t.test()</code> and <code>lm()</code> appear to be different tools, they both test the same hypothesis ‚Äî that the group means differ. The t-statistic for the group coefficient in the regression is identical to the one returned by the t-test. Furthermore, the <code>anova()</code> function applied to the linear model produces an F-statistic equal to the square of the t-statistic: <span class="math inline">\((F = t^2)\)</span>. Different outputs, same logic.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Normality and sample size">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Normality and sample size
</div>
</div>
<div class="callout-body-container callout-body">
<p>The values of the t and F statistics shown in the tables are derived from the Student‚Äôs t and Fisher‚Äôs F distributions, respectively. These distributions ‚Äî and the reliability of the associated inference ‚Äî rely on the assumption that residuals follow a normal distribution.</p>
<p>This assumption is especially critical when sample sizes are small. As sample size increases, the central limit theorem ensures that the sampling distribution of the test statistic becomes approximately normal, even if the underlying data are not. With 64 observations per group, the inference is considered robust to moderate departures from normality.</p>
</div>
</div>
<p>The p-value expresses the degree of statistical surprise associated with the observed value of the test statistic ‚Äî under the assumption that the null hypothesis is true. In other words, it quantifies how incompatible the data are with what would be expected from the sampling distribution under the null. Paradoxically, in statistical practice we often look for surprise: the lower the p-value, the more tempted we are to conclude that <strong>the null hypothesis may not hold</strong>.</p>
<p>The degrees of freedom associated with a statistical model offer a practical indication of its vulnerability to overfitting. Since they increase with the amount of data and decrease with the number of parameters estimated, a low number of degrees of freedom is a warning signal: the model may be too complex relative to the information available. In traditional statistical practice, this aspect is often underappreciated ‚Äî yet it plays a critical role in determining the stability and generalizability of inferential results.</p>
<p>This example shows how seemingly different statistical tools ‚Äî a t-test, a linear model, and an ANOVA ‚Äî all converge toward the same conclusion, using distinct but mathematically connected representations. What may appear at first as separate techniques are, in fact, different expressions of the same modeling logic.</p>
<p>It is also worth noting that, in this classical setting, all available observations ‚Äî 64 per group, for a total of 128 ‚Äî are used entirely for model estimation. No data is reserved for model validation. All degrees of freedom are ‚Äúspent‚Äù in fitting and inference, with no external check on how well the model might generalize to new data. This design choice ‚Äî to use all data for estimation ‚Äî is rooted in the assumptions and goals of classical inference. In contrast, ML workflows routinely reserve part of the data for validation, tuning, and performance assessment.</p>
</section>
</section>
<section id="from-inference-to-prediction-a-first-machine-learning-example" class="level2">
<h2 class="anchored" data-anchor-id="from-inference-to-prediction-a-first-machine-learning-example">From Inference to Prediction: A First Machine Learning Example</h2>
<p>We now shift from inference to prediction. In supervised ML, model validation is built into the workflow ‚Äî often using separate training and testing sets ‚Äî to evaluate how well the model generalizes.</p>
<p>Before diving deeper, it‚Äôs worth highlighting that although ML is often associated with ‚Äúalgorithms,‚Äù the core idea remains modeling. Algorithms are tools to find models ‚Äî representations of patterns in the data ‚Äî much as in statistics. The shift lies in the greater flexibility and adaptability expected of models built via ML.</p>
<p>Let‚Äôs now construct a simple classification example using simulated data. To keep the comparison with the previous example meaningful, we simulate two equally sized groups ‚Äî 64 observations each ‚Äî and generate two continuous predictors for classification. As with the t-test setting, the balanced design is not strictly necessary, but it remains highly advisable. Severe class imbalance can distort statistical inference and degrade the performance of classifiers, especially when probabilistic calibration is involved.</p>
<p>Wherever possible, experimental design should strive for balanced or near-balanced group sizes. In statistical inference, imbalance may reduce the precision of estimates or distort p-values. In ML, it can lead to biased classifiers, especially when the outcome classes are highly skewed. While techniques exist to handle imbalance ‚Äî such as weighting, oversampling, or downsampling, it is good experimental practice to minimize it at the design stage whenever possible.</p>
<section id="simulating-data-for-binary-classification" class="level3">
<h3 class="anchored" data-anchor-id="simulating-data-for-binary-classification">Simulating Data for Binary Classification</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n, <span class="fl">2.5</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n, <span class="fl">3.75</span>, <span class="dv">1</span>))</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n, <span class="fl">2.5</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n, <span class="fl">3.75</span>, <span class="dv">1</span>))</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>class <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>, n), <span class="fu">rep</span>(<span class="dv">1</span>, n)), <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"1"</span>))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>df_ml <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x1 =</span> x1, <span class="at">x2 =</span> x2, <span class="at">class =</span> class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The two classes are linearly separable, by design ‚Äî each group was generated from a distinct bivariate Gaussian distribution, <a href="#fig-bivariate-dataset" class="quarto-xref">Figure&nbsp;2</a>. This separation facilitates learning and allows us to explore classification behavior in a clean, controlled setting.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bivariate-dataset" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bivariate-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-bivariate-dataset-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bivariate-dataset-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Simulated dataset with two linearly separable classes, used to illustrate binary classification.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="fitting-a-logistic-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="fitting-a-logistic-regression-model">Fitting a Logistic Regression Model</h3>
<p>Although the name might suggest otherwise, logistic regression is not a method for modeling continuous outcomes. It is, in fact, one of the simplest and most widely used algorithms for <strong>binary classification</strong>. Given its solid statistical foundations, efficiency, and interpretability, logistic regression often serves as a reliable first choice in many applied ML workflows.</p>
<p>Rather than predicting a numeric value, the model estimates the <strong>probability that each observation belongs to a given class</strong> ‚Äî typically class 1. The model learns a relationship between the features and this probability, and a threshold (usually 0.5) is then applied to assign class labels.</p>
<p>Here we use the <a href="https://www.tidymodels.org/"><code>tidymodels</code></a> framework to fit the model. Tidymodels provides a unified and expressive syntax for specifying, training, and evaluating models in R, following the tidyverse principles.</p>
<p>To keep the example simple but realistic, we perform a basic train/test split before fitting the model. This reflects a fundamental aspect of the ML workflow: models are expected to generalize to new, unseen data. In contrast to traditional statistical modeling ‚Äî where inference is often based on the full dataset, with uncertainty captured via confidence intervals or p-values ‚Äî ML emphasizes predictive performance on held-out data.</p>
<p>Since logistic regression has no tunable hyperparameters, and our example involves just two features, we do not perform cross-validation or model tuning. A single split suffices to illustrate the predictive logic.</p>
<div class="cell">
<details class="code-fold">
<summary>Analysis code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split and annotate</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(df_ml, <span class="at">prop =</span> <span class="fl">0.75</span>, <span class="at">strata =</span> class)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span> <span class="fu">training</span>(split) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">"Train"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>test_df  <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)  <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">set =</span> <span class="st">"Test"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>df_all   <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(train_df, test_df)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit logistic regression on training set</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>model_ml <span class="ot">&lt;-</span> <span class="fu">logistic_reg</span>() <span class="sc">%&gt;%</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">"glm"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(class <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> train_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="visualizing-the-decision-boundary" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-decision-boundary">Visualizing the Decision Boundary</h3>
<p>In classical statistical testing, the choice of an operational point is often driven by conventions: a significance level of 0.05 and a power of 0.80 are typically adopted without extensive reflection. In ML, however, this decision is more explicitly tied to the ROC curve. The model provides estimated probabilities, and it is up to the practitioner ‚Äî in consultation with stakeholders ‚Äî to choose a classification threshold that reflects real-world priorities: minimizing false positives, false negatives, or some balanced trade-off. In the example discussed here, the boundary decision is plotted as a line over the point distributions of the two classes, as shown in <a href="#fig-bivariate-decision" class="quarto-xref">Figure&nbsp;3</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-bivariate-decision" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bivariate-decision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-bivariate-decision-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bivariate-decision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Decision boundary produced by a logistic regression classifier fitted on the training set (circles). Points in the test set are shown as triangles. Class 0 and Class 1 are represented in blue and red, respectively. The model learns to separate the two classes by estimating a probability of class membership and applying a threshold (here, 0.5). This visualization helps assess how well the model generalizes, especially when overlaid on unseen test data.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="evaluating-performance-roc-curve-and-auc" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-performance-roc-curve-and-auc">Evaluating Performance: ROC Curve and AUC</h3>
<p>We now evaluate model performance using the ROC curve, introduced earlier in the <em>ML-based key concepts</em> callout.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>test_df <span class="ot">&lt;-</span> test_df <span class="sc">%&gt;%</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(model_ml, <span class="at">new_data =</span> test_df, <span class="at">type =</span> <span class="st">"prob"</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>roc_data <span class="ot">&lt;-</span> <span class="fu">roc_curve</span>(test_df, <span class="at">truth =</span> class, .pred_0)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>auc_val  <span class="ot">&lt;-</span> <span class="fu">roc_auc</span>(test_df, <span class="at">truth =</span> class, .pred_0)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on training set (add probabilities + class prediction)</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span> train_df <span class="sc">%&gt;%</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">predict</span>(model_ml, <span class="at">new_data =</span> ., <span class="at">type =</span> <span class="st">"prob"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pred_class =</span> <span class="fu">if_else</span>(.pred_1 <span class="sc">&gt;=</span> <span class="fl">0.5</span>, <span class="st">"1"</span>, <span class="st">"0"</span>) <span class="sc">%&gt;%</span> <span class="fu">factor</span>(<span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"1"</span>)))</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure predicted class exists on test set</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>test_df <span class="ot">&lt;-</span> test_df <span class="sc">%&gt;%</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pred_class =</span> <span class="fu">if_else</span>(.pred_1 <span class="sc">&gt;=</span> <span class="fl">0.5</span>, <span class="st">"1"</span>, <span class="st">"0"</span>) <span class="sc">%&gt;%</span> <span class="fu">factor</span>(<span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"1"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This curve, shown in <a href="#fig-roc" class="quarto-xref">Figure&nbsp;4</a>, summarizes the trade-off between sensitivity and specificity across all possible thresholds, and provides a basis for threshold-independent comparison between classifiers.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-roc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-roc-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: ROC curve for the logistic regression model, evaluated on the test set. The curve shows how sensitivity (true positive rate) and specificity (1 - false positive rate) evolve as the decision threshold varies. This version reproduces the staircase structure typical of ROC curves, using a step plot with cleaned and ordered data. The dashed line represents chance-level performance. The area under the curve (AUC) summarizes overall discriminative ability.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The ROC curve provides a global view of model discrimination across all possible thresholds, but model decisions are ultimately made at specific thresholds. This is where the confusion matrix (<a href="#fig-confmat" class="quarto-xref">Figure&nbsp;5</a>) and the classification metrics table come into play: they reveal how a fixed cutoff ‚Äî here 0.5 ‚Äî translates into concrete outcomes in terms of correct and incorrect predictions. Such threshold-dependent evaluations are essential when aligning models with real-world decision-making contexts.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-confmat" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confmat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-confmat-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confmat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Confusion matrix for the logistic regression model on the test set (threshold = 0.5). The matrix summarizes the number of correct and incorrect predictions, broken down by actual and predicted class. It highlights how model decisions play out at a specific threshold, complementing the global view offered by the ROC curve.
</figcaption>
</figure>
</div>
</div>
</div>
<p>To complement the confusion matrix, we report key performance metrics in <a href="#tbl-performance-metrics" class="quarto-xref">Table&nbsp;1</a>. These include accuracy, sensitivity (recall for the positive class), and specificity (true negative rate), computed separately on the training and test sets.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute each metric separately, then combine</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>metrics_train <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">set =</span> <span class="st">"Train"</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">accuracy =</span> yardstick<span class="sc">::</span><span class="fu">accuracy</span>(train_df, <span class="at">truth =</span> class, <span class="at">estimate =</span> pred_class) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">sensitivity =</span> yardstick<span class="sc">::</span><span class="fu">sens</span>(train_df, <span class="at">truth =</span> class, <span class="at">estimate =</span> pred_class) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">specificity =</span> yardstick<span class="sc">::</span><span class="fu">spec</span>(train_df, <span class="at">truth =</span> class, <span class="at">estimate =</span> pred_class) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>metrics_test <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">set =</span> <span class="st">"Test"</span>,</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">accuracy =</span> yardstick<span class="sc">::</span><span class="fu">accuracy</span>(test_df, <span class="at">truth =</span> class, <span class="at">estimate =</span> pred_class) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate),</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">sensitivity =</span> yardstick<span class="sc">::</span><span class="fu">sens</span>(test_df, <span class="at">truth =</span> class, <span class="at">estimate =</span> pred_class) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate),</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">specificity =</span> yardstick<span class="sc">::</span><span class="fu">spec</span>(test_df, <span class="at">truth =</span> class, <span class="at">estimate =</span> pred_class) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.estimate)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This summary highlights how well the model generalizes and reinforces the idea that threshold selection ‚Äî while often fixed by convention in statistical testing ‚Äî plays a pivotal role in applied ML.</p>
<div class="cell">
<div id="tbl-performance-metrics" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-performance-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Summary of classification metrics at threshold 0.5 for training and test sets. Reported metrics include accuracy, sensitivity (recall for class 1), and specificity (true negative rate). These help interpret model performance beyond the AUC, and illustrate how different metrics depend on the choice of decision threshold.
</figcaption>
<div aria-describedby="tbl-performance-metrics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table quarto-disable-processing="true" class="table table-condensed table-bordered" style="font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;"></caption>
 <thead>
  <tr>
   <th style="text-align:left;"> set </th>
   <th style="text-align:right;"> accuracy </th>
   <th style="text-align:right;"> sensitivity </th>
   <th style="text-align:right;"> specificity </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Train </td>
   <td style="text-align:right;"> 0.76 </td>
   <td style="text-align:right;"> 0.787 </td>
   <td style="text-align:right;"> 0.733 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Test </td>
   <td style="text-align:right;"> 0.74 </td>
   <td style="text-align:right;"> 0.680 </td>
   <td style="text-align:right;"> 0.800 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="concluding-remarks" class="level2">
<h2 class="anchored" data-anchor-id="concluding-remarks">Concluding remarks</h2>
<p>The performance table above reveals not just how well the model generalizes from training to test data, but also highlights a deeper contrast between traditional <em>statistical inference</em> and <em>machine learning</em> practice. In classical statistics, the choice of a decision threshold ‚Äî say, a significance level <span class="math inline">\(\alpha=0.05\)</span> ‚Äî is often fixed by convention, serving as a gatekeeper for hypothesis testing. In ML, by contrast, the threshold (<span class="math inline">\(0.5\)</span> in our case) is not sacred: it can and should be <em>adapted</em> depending on context, cost, and stakeholder priorities.</p>
<p>The ROC curve offers a <em>global view</em> of the model‚Äôs discriminative ability across all thresholds, while the confusion matrix and the table of metrics make concrete the implications of a <em>specific threshold</em>. Together, they illustrate that prediction is not just about <em>accuracy</em>. It‚Äôs about <em>decisions</em>, <em>trade-offs</em>, and the <em>flexibility to adapt</em> models to real-world needs. And that‚Äôs precisely where statistical reasoning and ML begin to meet ‚Äî not in opposition, but in <em>dialogue</em>.</p>
<p>The tension between theoretical rigor and empirical adaptability will continue to shape the evolving relationship between statistics and ML. In future explorations, we will examine how models behave when ideal conditions break down, and how different strategies ‚Äî from regularization to robust modeling ‚Äî attempt to meet the timeless challenge of separating signal from noise.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<p>If you‚Äôd like to dig deeper into the ideas behind inference, prediction, and modern data modeling, here are some recommended readings ‚Äî ranging from foundational texts to more advanced perspectives:</p>
<p><strong><em>All of Statistics</em></strong> ‚Äî Larry Wasserman (2004)<br> A concise and mathematically grounded guide to statistical reasoning, ideal for those transitioning into data science.</p>
<p><strong><em>Statistics</em></strong> ‚Äî Freedman, Pisani, Purves (2007)<br> A clear and accessible introduction to the logic and language of statistics.</p>
<p><strong><em>Applied Predictive Modeling</em></strong> ‚Äî Kuhn &amp; Johnson (2013)<br> A hands-on resource focused on practical machine learning with a statistical backbone.</p>
<p><strong><em>Deep Learning</em></strong> ‚Äî Goodfellow, Bengio, Courville (2016)<br> The reference text for deep learning, covering theory and implementation.</p>
<p><strong><em>Computer Age Statistical Inference</em></strong> ‚Äî Efron &amp; Hastie (2016)<br> A beautifully written journey through the convergence of statistics and algorithmic thinking.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/amsabatini\.netlify\.app");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="angelosabatini/blog.comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>